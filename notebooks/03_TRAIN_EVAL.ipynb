{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 03 \u00b7 Entrenamiento y evaluaci\u00f3n (EWC baseline, preset FAST)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path; import json, sys, torch\nROOT=Path.cwd().parent; sys.path.append(str(ROOT))\nfrom src.utils import set_seeds, load_preset, make_loaders_from_csvs, ImageTransform\nfrom src.models import SNNVisionRegressor\nfrom src.training import TrainConfig, train_supervised\nfrom src.methods.ewc import EWC, EWCConfig\n\nset_seeds(42); preset='fast'; cfg=load_preset(ROOT/'configs'/'presets.yaml', preset); print('Preset:', cfg)\nPROC=ROOT/'data'/'processed'; RAW=ROOT/'data'/'raw'/'udacity'; tfm=ImageTransform(160,80,True,None)\n\n# SUPERVISED\nrun='circuito1'\ntrain_csv=PROC/run/'train.csv'; val_csv=PROC/run/'val.csv'; test_csv=PROC/run/'test.csv'\nassert train_csv.exists(), 'Ejecuta 01_DATA_QC_PREP.ipynb'\ntrain_loader,val_loader,test_loader=make_loaders_from_csvs(RAW/run, train_csv,val_csv,test_csv,\n    batch_size=cfg['batch_size'], encoder=cfg['encoder'], T=cfg['T'], gain=cfg['gain'], tfm=tfm)\n\nmodel=SNNVisionRegressor(in_channels=1,lif_beta=0.95); loss_fn=torch.nn.MSELoss()\ntcfg=TrainConfig(epochs=cfg['epochs'], batch_size=cfg['batch_size'], lr=cfg['lr'], amp=cfg['amp'])\nout_dir=ROOT/'outputs'/f'supervised_{preset}_ewc0'; print('Entrenando SUPERVISED...')\n_ = train_supervised(model, train_loader, val_loader, loss_fn, tcfg, out_dir, method=None); print('OK:', out_dir)\n\n# CONTINUAL (c1->c2) con EWC\nwith open(PROC/'tasks.json','r',encoding='utf-8') as f: tasks=json.load(f)\ndef make_loader_fn(task,batch):\n    name=task['name']; base=RAW/name; paths=task['paths']\n    return make_loaders_from_csvs(base, Path(paths['train']), Path(paths['val']), Path(paths['test']),\n        batch_size=batch, encoder=cfg['encoder'], T=cfg['T'], gain=cfg['gain'], tfm=tfm)\ntask_list=[{'name':n,'paths':tasks['splits'][n]} for n in tasks['tasks_order']]\nmodel2=SNNVisionRegressor(in_channels=1,lif_beta=0.95); ewc=EWC(model2,EWCConfig(lambd=1e10,fisher_batches=25))\nfrom src.training import _device; device=_device()\ntcfg2=TrainConfig(epochs=cfg['epochs'], batch_size=cfg['batch_size'], lr=cfg['lr'], amp=cfg['amp'])\noutc=ROOT/'outputs'/f'continual_{preset}_ewc'; outc.mkdir(parents=True, exist_ok=True)\nresults={}; seen=[]\nfor i,t in enumerate(task_list):\n    name=t['name']; tr,va,te=make_loader_fn(t, tcfg2.batch_size); print(f'Tarea {i+1}:', name)\n    _ = train_supervised(model2, tr, va, torch.nn.MSELoss(), tcfg2, outc/f'task_{i+1}_{name}', method=ewc)\n    print('Estimando Fisher...'); ewc.estimate_fisher(va, torch.nn.MSELoss(), device=device)\n    def eval_loader(loader):\n        mae_sum=mse_sum=n=0.0\n        for x,y in loader:\n            x=x.to(device); y=y.to(device)\n            with torch.no_grad(): y_hat=model2(x)\n            mae_sum += torch.mean(torch.abs(y_hat-y)).item()*len(y)\n            mse_sum += torch.mean((y_hat-y)**2).item()*len(y); n += len(y)\n        return mae_sum/n, mse_sum/n\n    te_mae,te_mse=eval_loader(te); results[name]={'test_mae':te_mae,'test_mse':te_mse}; seen.append((name,te))\n    for pname,p_loader in seen[:-1]:\n        p_mae,p_mse=eval_loader(p_loader)\n        results[pname][f'after_{name}_mae']=p_mae; results[pname][f'after_{name}_mse']=p_mse\nwith open(outc/'continual_results.json','w',encoding='utf-8') as f: json.dump(results,f,indent=2); print('OK:', outc/'continual_results.json')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}