{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7f4298",
   "metadata": {},
   "source": [
    "Celda 0 — Bootstrap / paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4448c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 0 — Rutas base y utilidades simples\n",
    "from pathlib import Path\n",
    "import sys, os, re, json, math, datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Raíz del repo\n",
    "ROOT = Path.cwd().parents[0] if (Path.cwd().name == \"notebooks\") else Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "OUT = ROOT / \"outputs\"\n",
    "SUMMARY = OUT / \"summary\"\n",
    "SUMMARY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT   :\", ROOT)\n",
    "print(\"OUT    :\", OUT)\n",
    "print(\"SUMMARY:\", SUMMARY)\n",
    "\n",
    "# -------------------- Helpers de E/S --------------------\n",
    "def _abs_run_dir(run_dir: str | Path) -> Path:\n",
    "    p = Path(run_dir)\n",
    "    return p if p.is_absolute() else (OUT / p)\n",
    "\n",
    "def _read_json(path: Path):\n",
    "    try:\n",
    "        if path.exists():\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _read_csv_df(path: Path):\n",
    "    try:\n",
    "        if path.exists():\n",
    "            return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def run_mtime(run_dir: str | Path) -> float:\n",
    "    \"\"\"Máximo mtime de carpeta y su primer nivel (robusto).\"\"\"\n",
    "    p = _abs_run_dir(run_dir)\n",
    "    try:\n",
    "        mt = [p.stat().st_mtime]\n",
    "        for c in p.iterdir():\n",
    "            try:\n",
    "                mt.append(c.stat().st_mtime)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return max(mt)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def canonical_method(s: str) -> str:\n",
    "    \"\"\"Normaliza etiquetas de método para agrupar.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"unknown\"\n",
    "    t = s.lower()\n",
    "    if (\"rehearsal\" in t) and (\"+ewc\" in t or \"_ewc\" in t):\n",
    "        return \"rehearsal+ewc\"\n",
    "    if \"sca-snn\" in t: return \"sca-snn\"\n",
    "    if re.search(r\"\\bsa[-_]snn\\b\", t): return \"sa-snn\"\n",
    "    if re.search(r\"\\bas[-_]snn\\b\", t): return \"as-snn\"\n",
    "    if \"colanet\" in t: return \"colanet\"\n",
    "    if re.search(r\"\\bewc\\b\", t) or \"ewc_lam\" in t: return \"ewc\"\n",
    "    if \"rehearsal\" in t: return \"rehearsal\"\n",
    "    if \"naive\" in t or \"finetune\" in t or \"fine-tune\" in t: return \"naive\"\n",
    "    return t.split(\"_\")[0]\n",
    "\n",
    "def _safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if x is None: return default\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758f181",
   "metadata": {},
   "source": [
    "Celda 1 — Config de selección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1 — Config de selección y salida\n",
    "\n",
    "# Etiqueta de este corte (todo lo generado irá aquí)\n",
    "SUMMARY_LABEL = \"paper_set_accurate_2025-11-03\"\n",
    "THIS_SUMMARY = SUMMARY / SUMMARY_LABEL\n",
    "THIS_SUMMARY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Filtros duros (ajusta a tu foto “buena”)\n",
    "PRESET_FILTER    = \"accurate\"         # None para no filtrar\n",
    "ENCODER_FILTER   = \"rate\"             # None para no filtrar\n",
    "SEED_FILTER      = 42                 # None para no filtrar\n",
    "METHODS_KEEP     = {\"sa-snn\",\"as-snn\",\"sca-snn\",\"ewc\",\"rehearsal\",\"naive\"}  # set() para no filtrar\n",
    "\n",
    "# Comparabilidad (mismo modelo/T/amp) y batch_size “tolerante”\n",
    "MODEL_MATCH_SUBSTR = \"PilotNetSNN_66x200_gray\"  # substring\n",
    "T_TARGET           = 30\n",
    "AMP_REQUIRED       = True\n",
    "BATCH_SIZE_TARGET  = 160        # tolerante: igual o NaN (cuando no conste)\n",
    "STRICT_CFG         = True       # si False, no se exige comparabilidad estricta\n",
    "\n",
    "# Corte temporal\n",
    "MTIME_FROM = dt.datetime(2025, 10, 31, 0, 0, 0)  # None para no filtrar por fecha\n",
    "\n",
    "# Restringir a runs del runner nuevo (run_row.*)\n",
    "ONLY_NEW_RUNNER = True\n",
    "\n",
    "# Filtros opcionales extra\n",
    "TAG_INCLUDE_SUBSTR = []         # p.ej., [\"best_\", \"grid05_\"]\n",
    "RUN_DIR_WHITELIST  = []         # p.ej., [\"continual_accurate_ewc_...\"]\n",
    "\n",
    "# Si faltan métricas, intentar leer eval_matrix.(csv|json)\n",
    "REQUIRE_EVALMATRIX = False      # si True, descartamos runs que no la tengan\n",
    "\n",
    "# Salidas “de ranking”\n",
    "TOPN = 6\n",
    "ALPHA_COMPOSITE = 0.5           # peso mae_norm vs forget_norm en score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10242379",
   "metadata": {},
   "source": [
    "Celda 2 — Construcción de la tabla base (una sola vez, fuente única)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2 — Reconstrucción 100% desde ficheros\n",
    "# (parser robusto: per_task_perf como dict o lista, fallback a CSV y run_row.json; amp normalizado)\n",
    "\n",
    "import re, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Utilidades de lectura seguras (usamos tus helpers si existen; si no, definimos mínimos) ---\n",
    "def _read_json(p: Path):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _read_csv_df(p: Path):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if isinstance(x, bool) or x is None:\n",
    "            return default\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _as_bool(x):\n",
    "    if isinstance(x, bool):\n",
    "        return x\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return None\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"true\",\"1\",\"yes\",\"y\"}:\n",
    "        return True\n",
    "    if s in {\"false\",\"0\",\"no\",\"n\"}:\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "# --- 1) Metadatos básicos (solo desde ficheros) ---\n",
    "def _parse_basic_meta(run_dir: Path):\n",
    "    \"\"\"\n",
    "    Extrae preset, method, encoder, model, seed, T, amp, batch_size desde ficheros del run.\n",
    "    Prioridad: run_row.json → manifests → heurística de nombre de carpeta.\n",
    "    \"\"\"\n",
    "    jrow = _read_json(run_dir / \"run_row.json\")\n",
    "\n",
    "    def jget(*keys, default=None):\n",
    "        obj = jrow or {}\n",
    "        for k in keys:\n",
    "            if not isinstance(obj, dict):\n",
    "                return default\n",
    "            obj = obj.get(k, None)\n",
    "        return obj if obj is not None else default\n",
    "\n",
    "    preset   = jget(\"preset\")\n",
    "    method   = jget(\"method\")\n",
    "    encoder  = jget(\"encoder\")\n",
    "    model    = jget(\"model\")\n",
    "    seed     = jget(\"seed\")\n",
    "    T        = jget(\"T\", default=jget(\"meta\",\"T\"))\n",
    "    amp      = jget(\"amp\", default=None)\n",
    "    batch_sz = jget(\"batch_size\", default=jget(\"meta\",\"batch_size\"))\n",
    "\n",
    "    # Fallbacks desde manifest del primer task\n",
    "    man1 = _read_json(run_dir / \"task_1_circuito1\" / \"manifest.json\")\n",
    "    if model is None:\n",
    "        model = (man1 or {}).get(\"model_name\") or (man1 or {}).get(\"model\")\n",
    "    if batch_sz is None:\n",
    "        meta1 = (man1 or {}).get(\"meta\", {}) if isinstance(man1, dict) else {}\n",
    "        batch_sz = meta1.get(\"batch_size\")\n",
    "\n",
    "    # Heurística de nombre si faltan preset/method/encoder\n",
    "    if preset is None or method is None or encoder is None:\n",
    "        # Nombre tipo: continual_<preset>_<method>_..._(rate|latency|raw|image)_model-..._seed_42\n",
    "        name = run_dir.name\n",
    "        m = re.match(r\"continual_([^_]+)_([^_].*?)_(rate|latency|raw|image)\\b\", name)\n",
    "        if m:\n",
    "            preset  = preset  or m.group(1)\n",
    "            method  = method  or m.group(2)\n",
    "            encoder = encoder or m.group(3)\n",
    "\n",
    "    # Tipados / normalizaciones\n",
    "    seed    = _safe_float(seed)\n",
    "    T       = _safe_float(T)\n",
    "    amp_v   = _as_bool(amp)\n",
    "    batch_sz = _safe_float(batch_sz)\n",
    "\n",
    "    return dict(\n",
    "        preset=preset, method=method, encoder=encoder, model=model,\n",
    "        seed=seed, T=T, amp=amp_v, batch_size=batch_sz\n",
    "    )\n",
    "\n",
    "# --- 2) Lectura robusta de per_task (dict o lista; fallback CSV; último recurso run_row.json) ---\n",
    "def _read_per_task_perf(run_dir: Path):\n",
    "    \"\"\"\n",
    "    Devuelve un dict con entradas por tarea:\n",
    "      {\"task_1_circuito1\": {\"best_mae\":..., \"final_mae\":...}, ...}\n",
    "    Acepta per_task_perf.json como dict o lista; fallback a per_task_perf.csv.\n",
    "    Último recurso: intenta poblar desde run_row.json si trae los MAE por tarea.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    # JSON principal\n",
    "    js = _read_json(run_dir / \"per_task_perf.json\")\n",
    "    if isinstance(js, dict):\n",
    "        # Estructura ya buena\n",
    "        out = {str(k): (v if isinstance(v, dict) else {}) for k, v in js.items()}\n",
    "    elif isinstance(js, list):\n",
    "        # Convertir lista a dict canónico\n",
    "        for i, it in enumerate(js):\n",
    "            if not isinstance(it, dict):\n",
    "                continue\n",
    "            name  = it.get(\"task\") or it.get(\"name\") or it.get(\"task_name\") or it.get(\"task_id\") or f\"task_{i+1}\"\n",
    "            best  = it.get(\"best_mae\")\n",
    "            final = it.get(\"final_mae\") or it.get(\"val_final_mae\") or it.get(\"val_last_mae\")\n",
    "            out[str(name)] = {\"best_mae\": _safe_float(best), \"final_mae\": _safe_float(final)}\n",
    "\n",
    "    # Fallback CSV si faltan datos clave\n",
    "    if (not out) or all((\"circuito1\" not in k and \"circuito2\" not in k) for k in out.keys()):\n",
    "        df = _read_csv_df(run_dir / \"per_task_perf.csv\")\n",
    "        if df is not None and len(df):\n",
    "            # Detecta columnas: nombre tarea\n",
    "            tcol = None\n",
    "            for c in [\"task\",\"name\",\"task_name\",\"task_id\"]:\n",
    "                if c in df.columns:\n",
    "                    tcol = c\n",
    "                    break\n",
    "            # Detecta columnas best/final\n",
    "            best_col  = \"best_mae\" if \"best_mae\" in df.columns else (\"val_best_mae\" if \"val_best_mae\" in df.columns else None)\n",
    "            final_col = \"final_mae\" if \"final_mae\" in df.columns else (\n",
    "                        \"val_final_mae\" if \"val_final_mae\" in df.columns else (\n",
    "                        \"val_last_mae\" if \"val_last_mae\" in df.columns else None))\n",
    "            if tcol and best_col and final_col:\n",
    "                for _, r in df.iterrows():\n",
    "                    name  = str(r[tcol])\n",
    "                    best  = _safe_float(r.get(best_col))\n",
    "                    final = _safe_float(r.get(final_col))\n",
    "                    out[name] = {\"best_mae\": best, \"final_mae\": final}\n",
    "\n",
    "    # Último recurso: run_row.json con maes por tarea\n",
    "    if (\"task_1_circuito1\" not in out) or (\"task_2_circuito2\" not in out):\n",
    "        rr = _read_json(run_dir / \"run_row.json\") or {}\n",
    "        c1b = _safe_float(rr.get(\"circuito1_best_mae\"))\n",
    "        c1f = _safe_float(rr.get(\"circuito1_final_mae\"))\n",
    "        c2b = _safe_float(rr.get(\"circuito2_best_mae\"))\n",
    "        c2f = _safe_float(rr.get(\"circuito2_final_mae\"))\n",
    "        if not np.isnan(c1b) or not np.isnan(c1f):\n",
    "            out.setdefault(\"task_1_circuito1\", {})[\"best_mae\"]  = c1b\n",
    "            out.setdefault(\"task_1_circuito1\", {})[\"final_mae\"] = c1f\n",
    "        if not np.isnan(c2b) or not np.isnan(c2f):\n",
    "            out.setdefault(\"task_2_circuito2\", {})[\"best_mae\"]  = c2b\n",
    "            out.setdefault(\"task_2_circuito2\", {})[\"final_mae\"] = c2f\n",
    "\n",
    "    return out\n",
    "\n",
    "# --- 3) Otros lectores (forgetting, eficiencia, eval_matrix) ---\n",
    "def _read_forgetting(run_dir: Path):\n",
    "    \"\"\"Lee forgetting.json si está disponible.\"\"\"\n",
    "    js = _read_json(run_dir / \"forgetting.json\")\n",
    "    return js or {}\n",
    "\n",
    "def _read_efficiency(run_dir: Path):\n",
    "    \"\"\"Lee emisiones/tiempo desde efficiency_summary.json o emissions.csv o continual_results.json.\"\"\"\n",
    "    j = _read_json(run_dir / \"efficiency_summary.json\") or {}\n",
    "    emissions = _safe_float(j.get(\"emissions_kg\"), default=np.nan)\n",
    "    elapsed   = _safe_float(j.get(\"elapsed_sec\"), default=np.nan)\n",
    "\n",
    "    if math.isnan(emissions):\n",
    "        df = _read_csv_df(run_dir / \"emissions.csv\")\n",
    "        if df is not None:\n",
    "            col = \"co2e_kg\" if \"co2e_kg\" in df.columns else (\"emissions_kg\" if \"emissions_kg\" in df.columns else None)\n",
    "            if col:\n",
    "                emissions = float(pd.to_numeric(df[col], errors=\"coerce\").fillna(0).sum())\n",
    "\n",
    "    if math.isnan(elapsed):\n",
    "        j2 = _read_json(run_dir / \"continual_results.json\") or {}\n",
    "        elapsed = _safe_float(j2.get(\"elapsed_sec\"), default=np.nan)\n",
    "\n",
    "    return emissions, elapsed\n",
    "\n",
    "def _read_eval_matrix(run_dir: Path):\n",
    "    \"\"\"Carga eval_matrix si existe (csv o json). Devuelve DataFrame si posible.\"\"\"\n",
    "    p_csv = run_dir / \"eval_matrix.csv\"\n",
    "    p_json = run_dir / \"eval_matrix.json\"\n",
    "    if p_csv.exists():\n",
    "        try:\n",
    "            return pd.read_csv(p_csv)\n",
    "        except Exception:\n",
    "            return None\n",
    "    js = _read_json(p_json)\n",
    "    if js is not None:\n",
    "        try:\n",
    "            return pd.DataFrame(js)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# --- 4) Olvido a partir de eval_matrix si no hay forgetting.json ---\n",
    "def _compute_forgetting_from_eval_matrix(eval_df: pd.DataFrame, per_task: dict):\n",
    "    \"\"\"\n",
    "    Para 2 tareas (circuito1, circuito2), calcula olvido de la tarea 1 tras aprender la 2.\n",
    "      best@t1: best_mae en task_1_circuito1 (per_task)\n",
    "      final@t2 sobre t1: eval_matrix (última columna relacionada con circuito1)\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    if not isinstance(per_task, dict):\n",
    "        return out\n",
    "\n",
    "    # best en circuito1 (tarea 1)\n",
    "    t1_key = None\n",
    "    for k in per_task.keys():\n",
    "        if \"circuito1\" in str(k).lower():\n",
    "            t1_key = k\n",
    "            break\n",
    "    best_t1 = None\n",
    "    if t1_key:\n",
    "        best_t1 = _safe_float(per_task.get(t1_key, {}).get(\"best_mae\"))\n",
    "\n",
    "    # final sobre circuito1 después de aprender la última tarea\n",
    "    final_t1_after_last = None\n",
    "    if isinstance(eval_df, pd.DataFrame) and len(eval_df) and eval_df.shape[1] > 0:\n",
    "        cols = [c for c in eval_df.columns if \"circuito1\" in str(c).lower()]\n",
    "        if cols:\n",
    "            # Heurística: tomar la última columna de circuito1\n",
    "            try:\n",
    "                final_t1_after_last = _safe_float(eval_df[cols[-1]].values[-1])\n",
    "            except Exception:\n",
    "                final_t1_after_last = np.nan\n",
    "\n",
    "    if best_t1 is None or math.isnan(best_t1) or final_t1_after_last is None or math.isnan(final_t1_after_last):\n",
    "        return out  # no podemos computar\n",
    "\n",
    "    forget_abs = max(0.0, final_t1_after_last - best_t1)\n",
    "    forget_rel = forget_abs / max(1e-9, best_t1)\n",
    "    out[\"circuito1_forget_abs\"] = forget_abs\n",
    "    out[\"circuito1_forget_rel\"] = forget_rel\n",
    "    out[\"circuito2_forget_abs\"] = 0.0\n",
    "    out[\"circuito2_forget_rel\"] = 0.0\n",
    "    out[\"avg_forget_rel\"] = forget_rel\n",
    "    return out\n",
    "\n",
    "# --- 5) Tabla completa reconstruida desde disco ---\n",
    "def build_results_table_from_disk(base_out: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    run_dirs = [p for p in base_out.glob(\"continual_*\") if p.is_dir()]\n",
    "    print(f\"[INFO] Escaneando {len(run_dirs)} runs en {base_out}\")\n",
    "\n",
    "    for rd in run_dirs:\n",
    "        meta     = _parse_basic_meta(rd)\n",
    "        per_task = _read_per_task_perf(rd)\n",
    "        eff_kg, elapsed = _read_efficiency(rd)\n",
    "        forget_js = _read_forgetting(rd) or {}\n",
    "        eval_df   = _read_eval_matrix(rd)\n",
    "\n",
    "        # MAEs por tarea (per_task puede ser dict o list -> ya normalizado a dict)\n",
    "        c1_best = c1_final = c2_best = c2_final = np.nan\n",
    "        if isinstance(per_task, dict):\n",
    "            for k, v in per_task.items():\n",
    "                k_low = str(k).lower()\n",
    "                if \"circuito1\" in k_low and isinstance(v, dict):\n",
    "                    c1_best  = _safe_float(v.get(\"best_mae\"), default=c1_best)\n",
    "                    c1_final = _safe_float(v.get(\"final_mae\"), default=c1_final)\n",
    "                elif \"circuito2\" in k_low and isinstance(v, dict):\n",
    "                    c2_best  = _safe_float(v.get(\"best_mae\"), default=c2_best)\n",
    "                    c2_final = _safe_float(v.get(\"final_mae\"), default=c2_final)\n",
    "\n",
    "        # Olvido: 1) forgetting.json si existe; 2) si no, eval_matrix\n",
    "        f_c1_abs = _safe_float(forget_js.get(\"circuito1_forget_abs\"))\n",
    "        f_c1_rel = _safe_float(forget_js.get(\"circuito1_forget_rel\"))\n",
    "        f_c2_abs = _safe_float(forget_js.get(\"circuito2_forget_abs\"))\n",
    "        f_c2_rel = _safe_float(forget_js.get(\"circuito2_forget_rel\"))\n",
    "        avg_f_rel = _safe_float(forget_js.get(\"avg_forget_rel\"))\n",
    "\n",
    "        if all(math.isnan(x) for x in [f_c1_abs, f_c1_rel, f_c2_abs, f_c2_rel, avg_f_rel]):\n",
    "            comp = _compute_forgetting_from_eval_matrix(eval_df, per_task)\n",
    "            if comp:\n",
    "                f_c1_abs = comp.get(\"circuito1_forget_abs\", f_c1_abs)\n",
    "                f_c1_rel = comp.get(\"circuito1_forget_rel\", f_c1_rel)\n",
    "                f_c2_abs = comp.get(\"circuito2_forget_abs\", f_c2_abs)\n",
    "                f_c2_rel = comp.get(\"circuito2_forget_rel\", f_c2_rel)\n",
    "                avg_f_rel = comp.get(\"avg_forget_rel\", avg_f_rel)\n",
    "\n",
    "        row = dict(\n",
    "            run_dir=str(rd.relative_to(base_out)),\n",
    "            preset=meta[\"preset\"],\n",
    "            method=meta[\"method\"],\n",
    "            encoder=meta[\"encoder\"],\n",
    "            model=meta[\"model\"],\n",
    "            seed=meta[\"seed\"],\n",
    "            T=meta[\"T\"],\n",
    "            batch_size=meta[\"batch_size\"],\n",
    "            amp=meta[\"amp\"],\n",
    "\n",
    "            emissions_kg=eff_kg,\n",
    "            elapsed_sec=elapsed,\n",
    "\n",
    "            circuito1_best_mae=c1_best,\n",
    "            circuito1_final_mae=c1_final,\n",
    "            circuito2_best_mae=c2_best,\n",
    "            circuito2_final_mae=c2_final,\n",
    "\n",
    "            circuito1_forget_abs=f_c1_abs,\n",
    "            circuito1_forget_rel=f_c1_rel,\n",
    "            circuito2_forget_abs=f_c2_abs,\n",
    "            circuito2_forget_rel=f_c2_rel,\n",
    "            avg_forget_rel=avg_f_rel,\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Flags extra\n",
    "    df[\"is_new_runner\"] = df[\"run_dir\"].apply(\n",
    "        lambda rd: (_abs_run_dir(rd) / \"run_row.json\").exists() or (_abs_run_dir(rd) / \"run_row.csv\").exists()\n",
    "    )\n",
    "    df[\"mtime\"] = df[\"run_dir\"].apply(run_mtime)\n",
    "    df[\"mtime_dt\"] = pd.to_datetime(df[\"mtime\"], unit=\"s\")\n",
    "    df[\"method_base\"] = df[\"method\"].astype(str).apply(canonical_method)\n",
    "\n",
    "    # Tipos numéricos\n",
    "    for c in [\n",
    "        \"seed\",\"T\",\"batch_size\",\"emissions_kg\",\"elapsed_sec\",\n",
    "        \"circuito1_best_mae\",\"circuito1_final_mae\",\"circuito2_best_mae\",\"circuito2_final_mae\",\n",
    "        \"circuito1_forget_abs\",\"circuito1_forget_rel\",\"circuito2_forget_abs\",\"circuito2_forget_rel\",\"avg_forget_rel\"\n",
    "    ]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    out_csv = SUMMARY / \"results_table_fromdisk.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] results_table_fromdisk → {out_csv} | filas:\", len(df))\n",
    "\n",
    "    # Diagnóstico de NaNs globales (para tu tranquilidad)\n",
    "    nan_cols = [\"circuito1_best_mae\",\"circuito1_final_mae\",\"circuito2_best_mae\",\"circuito2_final_mae\",\"avg_forget_rel\"]\n",
    "    print(\"[DEBUG] NaNs globales:\", {c:int(df[c].isna().sum()) for c in nan_cols if c in df})\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Construye tabla desde disco y muestra una vista rápida ---\n",
    "df_all = build_results_table_from_disk(OUT)\n",
    "display(df_all.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6dbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3 — Preselección y (opcional) exigir eval_matrix\n",
    "\n",
    "def _preselect(df):\n",
    "    dd = df.copy()\n",
    "    if PRESET_FILTER:\n",
    "        dd = dd[dd[\"preset\"] == PRESET_FILTER]\n",
    "    if ENCODER_FILTER:\n",
    "        dd = dd[dd[\"encoder\"] == ENCODER_FILTER]\n",
    "    if SEED_FILTER is not None:\n",
    "        dd = dd[dd[\"seed\"] == SEED_FILTER]\n",
    "    if METHODS_KEEP:\n",
    "        dd = dd[dd[\"method_base\"].isin(METHODS_KEEP)]\n",
    "    if ONLY_NEW_RUNNER and \"is_new_runner\" in dd.columns:\n",
    "        dd = dd[dd[\"is_new_runner\"] == True]\n",
    "    if MTIME_FROM is not None:\n",
    "        dd = dd[dd[\"mtime\"] >= MTIME_FROM.timestamp()]\n",
    "    if TAG_INCLUDE_SUBSTR:\n",
    "        mask = dd[\"run_dir\"].astype(str).apply(lambda s: any(t in s for t in TAG_INCLUDE_SUBSTR))\n",
    "        dd = dd[mask]\n",
    "    if RUN_DIR_WHITELIST:\n",
    "        dd = dd[dd[\"run_dir\"].isin(RUN_DIR_WHITELIST)]\n",
    "    return dd\n",
    "\n",
    "df_pre = _preselect(df_all)\n",
    "print(f\"[INFO] Preselección: {len(df_pre)} runs.\")\n",
    "\n",
    "if REQUIRE_EVALMATRIX:\n",
    "    def _has_eval(rd):\n",
    "        p = _abs_run_dir(rd)\n",
    "        return (p / \"eval_matrix.csv\").exists() or (p / \"eval_matrix.json\").exists()\n",
    "    df_pre = df_pre[df_pre[\"run_dir\"].apply(_has_eval)]\n",
    "    print(f\"[INFO] Con eval_matrix: {len(df_pre)} runs.\")\n",
    "\n",
    "display(df_pre.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4 — Filtro final (preset/encoder/seed/fecha/métodos) + comparabilidad robusta\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Helper silencioso\n",
    "def _read_json_silent(p: Path):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            import json\n",
    "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "def _to_bool(x):\n",
    "    if isinstance(x, bool): return x\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)): return None\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"true\",\"1\",\"yes\",\"y\"}: return True\n",
    "    if s in {\"false\",\"0\",\"no\",\"n\"}: return False\n",
    "    return None\n",
    "\n",
    "def _batch_size_filled(row: pd.Series) -> float:\n",
    "    \"\"\"Backfill de batch_size mirando run_row.json o task_1_circuito1/manifest.json.\"\"\"\n",
    "    bs = pd.to_numeric(row.get(\"batch_size\"), errors=\"coerce\")\n",
    "    if pd.notna(bs):\n",
    "        return float(bs)\n",
    "    rd = _abs_run_dir(row[\"run_dir\"])\n",
    "    # run_row.json\n",
    "    j = _read_json_silent(rd / \"run_row.json\") or {}\n",
    "    cand = j.get(\"batch_size\", None)\n",
    "    if cand is None:\n",
    "        cand = (j.get(\"meta\", {}) if isinstance(j, dict) else {}).get(\"batch_size\")\n",
    "    if cand is not None:\n",
    "        try:\n",
    "            return float(cand)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # manifest del primer task\n",
    "    man = _read_json_silent(rd / \"task_1_circuito1\" / \"manifest.json\")\n",
    "    meta = man.get(\"meta\", {}) if isinstance(man, dict) else {}\n",
    "    cand = meta.get(\"batch_size\")\n",
    "    if cand is not None:\n",
    "        try:\n",
    "            return float(cand)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return float(\"nan\")  # equivalente a np.nan\n",
    "\n",
    "# 1) Filtros “duros”\n",
    "df_sel = df_all.copy()\n",
    "\n",
    "dbg_counts = {\"inicio\": len(df_sel)}\n",
    "\n",
    "if PRESET_FILTER:\n",
    "    df_sel = df_sel[df_sel[\"preset\"] == PRESET_FILTER]\n",
    "dbg_counts[\"preset\"] = len(df_sel)\n",
    "\n",
    "if ENCODER_FILTER:\n",
    "    df_sel = df_sel[df_sel[\"encoder\"] == ENCODER_FILTER]\n",
    "dbg_counts[\"encoder\"] = len(df_sel)\n",
    "\n",
    "if SEED_FILTER is not None:\n",
    "    df_sel = df_sel[df_sel[\"seed\"] == SEED_FILTER]\n",
    "dbg_counts[\"seed\"] = len(df_sel)\n",
    "\n",
    "if METHODS_KEEP:\n",
    "    df_sel = df_sel[df_sel[\"method_base\"].isin(METHODS_KEEP)]\n",
    "dbg_counts[\"methods_keep\"] = len(df_sel)\n",
    "\n",
    "if ONLY_NEW_RUNNER and \"is_new_runner\" in df_sel.columns:\n",
    "    df_sel = df_sel[df_sel[\"is_new_runner\"] == True]\n",
    "dbg_counts[\"only_new_runner\"] = len(df_sel)\n",
    "\n",
    "if MTIME_FROM is not None:\n",
    "    df_sel = df_sel[df_sel[\"mtime\"] >= MTIME_FROM.timestamp()]\n",
    "dbg_counts[\"mtime_from\"] = len(df_sel)\n",
    "\n",
    "if TAG_INCLUDE_SUBSTR:\n",
    "    mask = df_sel[\"run_dir\"].astype(str).apply(lambda s: any(t in s for t in TAG_INCLUDE_SUBSTR))\n",
    "    df_sel = df_sel[mask]\n",
    "    dbg_counts[\"tag_include\"] = len(df_sel)\n",
    "\n",
    "if RUN_DIR_WHITELIST:\n",
    "    df_sel = df_sel[df_sel[\"run_dir\"].isin(RUN_DIR_WHITELIST)]\n",
    "    dbg_counts[\"whitelist\"] = len(df_sel)\n",
    "\n",
    "print(\"[DEBUG] filtros duros →\", \" | \".join(f\"{k} → {v} runs\" for k,v in dbg_counts.items()))\n",
    "\n",
    "if df_sel.empty:\n",
    "    print(\"[WARN] Tras filtros duros no quedan runs. Usando df_all sin filtros para no cortar el flujo.\")\n",
    "    df_sel = df_all.copy()\n",
    "\n",
    "# 2) Comparabilidad (mismo modelo/T/amp/batch_size), con relajaciones por fases\n",
    "df_sel = df_sel.copy()\n",
    "df_sel[\"batch_size_filled\"] = df_sel.apply(_batch_size_filled, axis=1)\n",
    "\n",
    "model_ok = df_sel[\"model\"].astype(str).str.contains(MODEL_MATCH_SUBSTR, na=False) if MODEL_MATCH_SUBSTR else pd.Series(True, index=df_sel.index)\n",
    "T_ok     = (pd.to_numeric(df_sel[\"T\"], errors=\"coerce\") == T_TARGET) if (T_TARGET is not None) else pd.Series(True, index=df_sel.index)\n",
    "\n",
    "if AMP_REQUIRED:\n",
    "    amp_s = df_sel[\"amp\"].apply(_to_bool)\n",
    "    amp_ok = amp_s.fillna(False)\n",
    "else:\n",
    "    amp_ok = pd.Series(True, index=df_sel.index)\n",
    "\n",
    "bs = pd.to_numeric(df_sel[\"batch_size_filled\"], errors=\"coerce\")\n",
    "bs_ok = bs.eq(BATCH_SIZE_TARGET) if (BATCH_SIZE_TARGET is not None) else pd.Series(True, index=df_sel.index)\n",
    "\n",
    "print(\"[DEBUG] Comparabilidad — resúmenes\")\n",
    "print(\" - modelos presentes:\", sorted(df_sel[\"model\"].astype(str).unique().tolist()))\n",
    "print(\" - T únicos:\", sorted(pd.to_numeric(df_sel[\"T\"], errors=\"coerce\").dropna().unique().tolist()))\n",
    "print(\" - AMP valores:\", dict(pd.Series(amp_s if AMP_REQUIRED else [True]*len(df_sel)).value_counts(dropna=False)))\n",
    "print(f\" - batch_size_filled (≠ {BATCH_SIZE_TARGET}):\", sorted(pd.to_numeric(df_sel.loc[~bs_ok, \"batch_size_filled\"], errors='coerce').dropna().unique().tolist()))\n",
    "\n",
    "candidates = [\n",
    "    (model_ok & T_ok & amp_ok & bs_ok,               \"strict: model+T+amp+batch\"),\n",
    "    (model_ok & T_ok & amp_ok,                       \"relaxed: sin batch\"),\n",
    "    (model_ok & T_ok,                                \"relaxed: sin amp & batch\"),\n",
    "    (model_ok,                                       \"relaxed: sólo model\"),\n",
    "    (pd.Series(True, index=df_sel.index),            \"relaxed: sin comparabilidad\")\n",
    "]\n",
    "\n",
    "df_kept = None\n",
    "chosen_reason = None\n",
    "for mask, reason in candidates:\n",
    "    tmp = df_sel[mask].copy()\n",
    "    if len(tmp) > 0:\n",
    "        df_kept = tmp\n",
    "        chosen_reason = reason\n",
    "        break\n",
    "\n",
    "if df_kept is None or df_kept.empty:\n",
    "    print(\"[WARN] Comparabilidad dejó 0 runs incluso tras relajar. Continuamos con df_sel sin comparabilidad para no cortar el flujo.\")\n",
    "    df_kept = df_sel.copy()\n",
    "    chosen_reason = \"fallback total\"\n",
    "\n",
    "print(f\"[DEBUG] comparabilidad → kept={len(df_kept)}, dropped={len(df_sel)-len(df_kept)} | estrategia: {chosen_reason}\")\n",
    "\n",
    "# 3) Asegura columnas métricas clave\n",
    "# Identifica columnas *_final_mae y escoge la de la última tarea (heurística)\n",
    "task_cols = [c for c in df_kept.columns if c.endswith(\"_final_mae\")]\n",
    "assert task_cols, \"No se encuentran columnas *_final_mae.\"\n",
    "\n",
    "def _key(col):\n",
    "    base = col.replace(\"_final_mae\",\"\")\n",
    "    m = re.search(r\"(\\d+)$\", base)\n",
    "    idx = int(m.group(1)) if m else 0\n",
    "    base = re.sub(r\"\\d+$\",\"\", base)\n",
    "    return (base, idx)\n",
    "\n",
    "task_cols_sorted = sorted(task_cols, key=_key)\n",
    "MAE_COL = task_cols_sorted[-1]\n",
    "MAE_TASK_NAME = MAE_COL.replace(\"_final_mae\",\"\")\n",
    "\n",
    "# Asegura tipos numéricos\n",
    "for c in [MAE_COL, \"avg_forget_rel\", \"emissions_kg\", \"elapsed_sec\"]:\n",
    "    if c not in df_kept.columns:\n",
    "        df_kept[c] = np.nan\n",
    "    df_kept[c] = pd.to_numeric(df_kept[c], errors=\"coerce\")\n",
    "\n",
    "# Rellenos prudentes\n",
    "if df_kept[\"emissions_kg\"].isna().all():\n",
    "    df_kept[\"emissions_kg\"] = 0.0\n",
    "else:\n",
    "    df_kept[\"emissions_kg\"] = df_kept[\"emissions_kg\"].fillna(df_kept[\"emissions_kg\"].median())\n",
    "\n",
    "if df_kept[\"avg_forget_rel\"].isna().all():\n",
    "    df_kept[\"avg_forget_rel\"] = 0.0\n",
    "else:\n",
    "    df_kept[\"avg_forget_rel\"] = df_kept[\"avg_forget_rel\"].fillna(df_kept[\"avg_forget_rel\"].max())\n",
    "\n",
    "# Guarda la tabla de selección (foto del corte)\n",
    "sel_csv = THIS_SUMMARY / \"selection_table.csv\"\n",
    "df_kept.to_csv(sel_csv, index=False)\n",
    "print(f\"[OK] Selección final → {sel_csv} | filas:\", len(df_kept))\n",
    "display(df_kept.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c450288",
   "metadata": {},
   "source": [
    "Celda 3 — (Opcional) re-evaluar eval_matrix donde falte, solo sobre pre-selección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b43aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5 — Winners, TopN, Pareto y gráficos (tolerante a NaNs)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def winners_per_method(dfin: pd.DataFrame, mae_col: str) -> pd.DataFrame:\n",
    "    x = dfin.dropna(subset=[mae_col]).copy()\n",
    "    if x.empty:\n",
    "        return dfin.copy().head(0)\n",
    "    order = [mae_col, \"avg_forget_rel\", \"emissions_kg\"]\n",
    "    x = x.sort_values(order, ascending=[True, True, True])\n",
    "    x = x.drop_duplicates(subset=[\"method_base\"], keep=\"first\")\n",
    "    return x.sort_values(order, ascending=[True, True, True])\n",
    "\n",
    "def composite_score(dfin: pd.DataFrame, mae_col: str, alpha=0.5) -> pd.Series:\n",
    "    x = dfin.copy()\n",
    "    # Normalizamos con recorte robusto a percentiles y tolerando NaNs\n",
    "    def _norm(col):\n",
    "        v = pd.to_numeric(x[col], errors=\"coerce\")\n",
    "        if v.notna().sum() == 0:\n",
    "            return pd.Series(np.nan, index=x.index)\n",
    "        lo, hi = np.nanpercentile(v, 5), np.nanpercentile(v, 95)\n",
    "        rng = max(1e-9, (hi - lo))\n",
    "        y = (v - lo) / rng\n",
    "        return y.clip(0, 1)\n",
    "    x[mae_col+\"_norm\"] = _norm(mae_col)\n",
    "    x[\"avg_forget_rel_norm\"] = _norm(\"avg_forget_rel\")\n",
    "    return alpha * x[mae_col+\"_norm\"] + (1.0 - alpha) * x[\"avg_forget_rel_norm\"]\n",
    "\n",
    "def pareto_front(dfin: pd.DataFrame, xcol: str, ycol: str) -> pd.DataFrame:\n",
    "    x = dfin.dropna(subset=[xcol, ycol]).copy()\n",
    "    if x.empty:\n",
    "        return x\n",
    "    pts = x[[xcol, ycol]].values.astype(float)\n",
    "    n = len(pts)\n",
    "    dom = np.zeros(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if dom[i]:\n",
    "            continue\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if np.all(pts[j] <= pts[i]) and np.any(pts[j] < pts[i]):\n",
    "                dom[i] = True\n",
    "                break\n",
    "    return x[~dom].sort_values([xcol, ycol], ascending=[True, True])\n",
    "\n",
    "# 1) Winners por método\n",
    "win_df = winners_per_method(df_kept, MAE_COL)\n",
    "winners_csv = THIS_SUMMARY / \"winners_per_method.csv\"\n",
    "win_df.to_csv(winners_csv, index=False)\n",
    "print(\"[OK] Winners →\", winners_csv)\n",
    "display(win_df)\n",
    "\n",
    "# 2) Top-N por score compuesto\n",
    "df_rank = df_kept.copy()\n",
    "df_rank[\"score\"] = composite_score(df_rank, MAE_COL, alpha=ALPHA_COMPOSITE)\n",
    "df_rank2 = df_rank.dropna(subset=[\"score\", MAE_COL]).copy()\n",
    "topn_df = df_rank2.sort_values([\"score\", MAE_COL, \"avg_forget_rel\"]).head(TOPN)\n",
    "topn_csv = THIS_SUMMARY / f\"top{TOPN}_composite.csv\"\n",
    "topn_df.to_csv(topn_csv, index=False)\n",
    "print(f\"[OK] Top-{TOPN} →\", topn_csv)\n",
    "display(topn_df[[\"run_dir\",\"method_base\",\"seed\",MAE_COL,\"avg_forget_rel\",\"emissions_kg\",\"score\"]])\n",
    "\n",
    "# 3) Frente de Pareto (MAE vs Olvido)\n",
    "pareto_df = pareto_front(df_kept, MAE_COL, \"avg_forget_rel\")\n",
    "pareto_csv = THIS_SUMMARY / \"pareto.csv\"\n",
    "pareto_df.to_csv(pareto_csv, index=False)\n",
    "print(\"[OK] Pareto →\", pareto_csv)\n",
    "display(pareto_df)\n",
    "\n",
    "# 4) Gráficos (solo si hay datos válidos)\n",
    "def scatter_mae_forget(dfin: pd.DataFrame, title: str, outfile: Path):\n",
    "    dd = dfin.dropna(subset=[MAE_COL, \"avg_forget_rel\"]).copy()\n",
    "    if dd.empty:\n",
    "        print(f\"[INFO] {title}: sin datos suficientes para scatter (NaNs).\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    for mb in sorted(dd[\"method_base\"].dropna().unique()):\n",
    "        ss = dd[dd[\"method_base\"] == mb]\n",
    "        ax.scatter(ss[MAE_COL], ss[\"avg_forget_rel\"], label=mb, s=48)\n",
    "    ax.set_xlabel(f\"MAE final ({MAE_TASK_NAME}) ↓\")\n",
    "    ax.set_ylabel(\"Olvido relativo medio ↓\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outfile, dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(\"[OK] Scatter →\", outfile)\n",
    "\n",
    "scatter_all_png = THIS_SUMMARY / \"scatter_all.png\"\n",
    "scatter_mae_forget(df_kept, f\"{SUMMARY_LABEL} — todos\", scatter_all_png)\n",
    "\n",
    "scatter_win_png = THIS_SUMMARY / \"scatter_winners.png\"\n",
    "scatter_mae_forget(win_df if not win_df.empty else df_kept, f\"{SUMMARY_LABEL} — winners\", scatter_win_png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a9a80b",
   "metadata": {},
   "source": [
    "Celda 4 — Filtros finales y comparabilidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6 — Curvas y barras comparativas (opcional si tienes src/plots)\n",
    "try:\n",
    "    from src.plots import plot_across_runs, plot_mae_curves_for_run as plot_loss_curves_for_run\n",
    "except Exception:\n",
    "    plot_across_runs = None\n",
    "    plot_loss_curves_for_run = None\n",
    "\n",
    "# Curvas por run (val_loss/val_mae), restringido a la selección\n",
    "if plot_loss_curves_for_run is not None:\n",
    "    base_plots = THIS_SUMMARY / \"plots_val_metrics\"\n",
    "    base_plots.mkdir(parents=True, exist_ok=True)\n",
    "    for rd in sorted(set(df_sel[\"run_dir\"].astype(str))):\n",
    "        try:\n",
    "            plot_loss_curves_for_run(_abs_run_dir(rd), base_plots, smooth_window=3)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falló curvas en {Path(rd).name}: {e}\")\n",
    "    print(\"[OK] Curvas por run →\", base_plots)\n",
    "else:\n",
    "    print(\"[INFO] plot_mae_curves_for_run no disponible; saltando.\")\n",
    "\n",
    "# Barras y trade-offs “across runs”, restringido a la selección\n",
    "if plot_across_runs is not None:\n",
    "    dest_acc = THIS_SUMMARY / \"plots_across_runs\"\n",
    "    dest_acc.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        plot_across_runs(df_sel, dest_acc)\n",
    "        print(\"[OK] plot_across_runs →\", dest_acc)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] plot_across_runs falló:\", e)\n",
    "else:\n",
    "    print(\"[INFO] plot_across_runs no disponible; saltando.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ed370",
   "metadata": {},
   "source": [
    "Celda 5 — Ganadores por método, Top-N y Pareto (+ gráficos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cea50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5 — Winners, TopN, Pareto y gráficos (tolerante a NaNs)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def winners_per_method(dfin: pd.DataFrame, mae_col: str) -> pd.DataFrame:\n",
    "    x = dfin.dropna(subset=[mae_col]).copy()\n",
    "    if x.empty:\n",
    "        return dfin.copy().head(0)\n",
    "    order = [mae_col, \"avg_forget_rel\", \"emissions_kg\"]\n",
    "    x = x.sort_values(order, ascending=[True, True, True])\n",
    "    x = x.drop_duplicates(subset=[\"method_base\"], keep=\"first\")\n",
    "    return x.sort_values(order, ascending=[True, True, True])\n",
    "\n",
    "def composite_score(dfin: pd.DataFrame, mae_col: str, alpha=0.5) -> pd.Series:\n",
    "    x = dfin.copy()\n",
    "    # Normalizamos con recorte robusto a percentiles y tolerando NaNs\n",
    "    def _norm(col):\n",
    "        v = pd.to_numeric(x[col], errors=\"coerce\")\n",
    "        if v.notna().sum() == 0:\n",
    "            return pd.Series(np.nan, index=x.index)\n",
    "        lo, hi = np.nanpercentile(v, 5), np.nanpercentile(v, 95)\n",
    "        rng = max(1e-9, (hi - lo))\n",
    "        y = (v - lo) / rng\n",
    "        return y.clip(0, 1)\n",
    "    x[mae_col+\"_norm\"] = _norm(mae_col)\n",
    "    x[\"avg_forget_rel_norm\"] = _norm(\"avg_forget_rel\")\n",
    "    return alpha * x[mae_col+\"_norm\"] + (1.0 - alpha) * x[\"avg_forget_rel_norm\"]\n",
    "\n",
    "def pareto_front(dfin: pd.DataFrame, xcol: str, ycol: str) -> pd.DataFrame:\n",
    "    x = dfin.dropna(subset=[xcol, ycol]).copy()\n",
    "    if x.empty:\n",
    "        return x\n",
    "    pts = x[[xcol, ycol]].values.astype(float)\n",
    "    n = len(pts)\n",
    "    dom = np.zeros(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if dom[i]:\n",
    "            continue\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if np.all(pts[j] <= pts[i]) and np.any(pts[j] < pts[i]):\n",
    "                dom[i] = True\n",
    "                break\n",
    "    return x[~dom].sort_values([xcol, ycol], ascending=[True, True])\n",
    "\n",
    "# 1) Winners por método\n",
    "win_df = winners_per_method(df_kept, MAE_COL)\n",
    "winners_csv = THIS_SUMMARY / \"winners_per_method.csv\"\n",
    "win_df.to_csv(winners_csv, index=False)\n",
    "print(\"[OK] Winners →\", winners_csv)\n",
    "display(win_df)\n",
    "\n",
    "# 2) Top-N por score compuesto\n",
    "df_rank = df_kept.copy()\n",
    "df_rank[\"score\"] = composite_score(df_rank, MAE_COL, alpha=ALPHA_COMPOSITE)\n",
    "df_rank2 = df_rank.dropna(subset=[\"score\", MAE_COL]).copy()\n",
    "topn_df = df_rank2.sort_values([\"score\", MAE_COL, \"avg_forget_rel\"]).head(TOPN)\n",
    "topn_csv = THIS_SUMMARY / f\"top{TOPN}_composite.csv\"\n",
    "topn_df.to_csv(topn_csv, index=False)\n",
    "print(f\"[OK] Top-{TOPN} →\", topn_csv)\n",
    "display(topn_df[[\"run_dir\",\"method_base\",\"seed\",MAE_COL,\"avg_forget_rel\",\"emissions_kg\",\"score\"]])\n",
    "\n",
    "# 3) Frente de Pareto (MAE vs Olvido)\n",
    "pareto_df = pareto_front(df_kept, MAE_COL, \"avg_forget_rel\")\n",
    "pareto_csv = THIS_SUMMARY / \"pareto.csv\"\n",
    "pareto_df.to_csv(pareto_csv, index=False)\n",
    "print(\"[OK] Pareto →\", pareto_csv)\n",
    "display(pareto_df)\n",
    "\n",
    "# 4) Gráficos (solo si hay datos válidos)\n",
    "def scatter_mae_forget(dfin: pd.DataFrame, title: str, outfile: Path):\n",
    "    dd = dfin.dropna(subset=[MAE_COL, \"avg_forget_rel\"]).copy()\n",
    "    if dd.empty:\n",
    "        print(f\"[INFO] {title}: sin datos suficientes para scatter (NaNs).\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    for mb in sorted(dd[\"method_base\"].dropna().unique()):\n",
    "        ss = dd[dd[\"method_base\"] == mb]\n",
    "        ax.scatter(ss[MAE_COL], ss[\"avg_forget_rel\"], label=mb, s=48)\n",
    "    ax.set_xlabel(f\"MAE final ({MAE_TASK_NAME}) ↓\")\n",
    "    ax.set_ylabel(\"Olvido relativo medio ↓\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outfile, dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(\"[OK] Scatter →\", outfile)\n",
    "\n",
    "scatter_all_png = THIS_SUMMARY / \"scatter_all.png\"\n",
    "scatter_mae_forget(df_kept, f\"{SUMMARY_LABEL} — todos\", scatter_all_png)\n",
    "\n",
    "scatter_win_png = THIS_SUMMARY / \"scatter_winners.png\"\n",
    "scatter_mae_forget(win_df if not win_df.empty else df_kept, f\"{SUMMARY_LABEL} — winners\", scatter_win_png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c739f5b",
   "metadata": {},
   "source": [
    "Celda 6 — Plots por-run y “across runs” (sólo la selección)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a731c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6 — Curvas y barras comparativas (opcional si tienes src/plots)\n",
    "try:\n",
    "    from src.plots import plot_across_runs, plot_mae_curves_for_run as plot_loss_curves_for_run\n",
    "except Exception:\n",
    "    plot_across_runs = None\n",
    "    plot_loss_curves_for_run = None\n",
    "\n",
    "# Curvas por run (val_loss/val_mae), restringido a la selección\n",
    "if plot_loss_curves_for_run is not None:\n",
    "    base_plots = THIS_SUMMARY / \"plots_val_metrics\"\n",
    "    base_plots.mkdir(parents=True, exist_ok=True)\n",
    "    for rd in sorted(set(df_sel[\"run_dir\"].astype(str))):\n",
    "        try:\n",
    "            plot_loss_curves_for_run(_abs_run_dir(rd), base_plots, smooth_window=3)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falló curvas en {Path(rd).name}: {e}\")\n",
    "    print(\"[OK] Curvas por run →\", base_plots)\n",
    "else:\n",
    "    print(\"[INFO] plot_mae_curves_for_run no disponible; saltando.\")\n",
    "\n",
    "# Barras y trade-offs “across runs”, restringido a la selección\n",
    "if plot_across_runs is not None:\n",
    "    dest_acc = THIS_SUMMARY / \"plots_across_runs\"\n",
    "    dest_acc.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        plot_across_runs(df_sel, dest_acc)\n",
    "        print(\"[OK] plot_across_runs →\", dest_acc)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] plot_across_runs falló:\", e)\n",
    "else:\n",
    "    print(\"[INFO] plot_across_runs no disponible; saltando.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA A — Diagnóstico de NaN (qué falta y por qué)\n",
    "def diag_nans(df, cols):\n",
    "    miss = df[df[cols].isna().any(axis=1)].copy()\n",
    "    if miss.empty:\n",
    "        print(\"[OK] No hay NaN en columnas:\", cols)\n",
    "        return miss\n",
    "    print(f\"[INFO] {len(miss)} runs con NaN en {cols}:\")\n",
    "    display(miss[[\"run_dir\",\"method_base\",\"preset\",\"encoder\",\"seed\",\"T\",\"amp\",\"batch_size\"] + cols].head(50))\n",
    "    # pistas de ficheros clave\n",
    "    for rd in miss[\"run_dir\"].head(10):\n",
    "        p = _abs_run_dir(rd)\n",
    "        print(\"—\", rd)\n",
    "        for f in [\"per_task_perf.json\",\"per_task_perf.csv\",\"forgetting.json\",\"eval_matrix.csv\",\"eval_matrix.json\",\n",
    "                  \"efficiency_summary.json\",\"emissions.csv\",\"run_row.json\",\"task_1_circuito1/manifest.json\"]:\n",
    "            print(\"   \", f, \"→\", \"OK\" if (p / f).exists() else \"NO\")\n",
    "    return miss\n",
    "\n",
    "cols_clave = [\"circuito1_best_mae\",\"circuito1_final_mae\",\"circuito2_final_mae\",\"avg_forget_rel\"]\n",
    "print(\"## NaN en selección ##\")\n",
    "_ = diag_nans(df_sel, cols_clave)\n",
    "\n",
    "print(\"\\n## NaN en df_all (global) ##\")\n",
    "_ = diag_nans(df_all, cols_clave)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570da81d",
   "metadata": {},
   "source": [
    "Celda 7 — Auditoría rápida (por qué se excluye algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA B — Sanidad de CL (naive debería olvidar más que métodos CL)\n",
    "chk = (df_sel.groupby(\"method_base\", as_index=False)\n",
    "       .agg(forget_mean=(\"avg_forget_rel\",\"mean\"),\n",
    "            mae_mean=(MAE_COL,\"mean\"),\n",
    "            runs=(\"run_dir\",\"count\"))\n",
    "       .sort_values(\"forget_mean\"))\n",
    "display(chk)\n",
    "\n",
    "naive_forget = chk.loc[chk[\"method_base\"]==\"naive\",\"forget_mean\"]\n",
    "if not naive_forget.empty:\n",
    "    naive_val = naive_forget.values[0]\n",
    "    cl_methods = chk[~chk[\"method_base\"].isin([\"naive\"])]\n",
    "    if not cl_methods.empty:\n",
    "        cl_median = float(cl_methods[\"forget_mean\"].median())\n",
    "        print(f\"[CHECK] naive_forget={naive_val:.4f} vs CL_median={cl_median:.4f}\")\n",
    "        if naive_val + 1e-6 >= cl_median:\n",
    "            print(\"[OK] Naive olvida ≥ mediana CL (coherente).\")\n",
    "        else:\n",
    "            print(\"[WARN] Naive NO olvida más que CL. Revisa eval_matrix/forgetting.json o implementación.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
