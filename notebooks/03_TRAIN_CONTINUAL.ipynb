{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c448d5",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# 03_TRAIN_CONTINUAL ‚Äî Entrenamiento Continual con *presets*\n",
    "\n",
    "**Qu√© hace este notebook**  \n",
    "Entrena y eval√∫a modelos en **aprendizaje continual** (secuencia de tareas) usando una **configuraci√≥n unificada** desde `configs/presets.yaml`. Permite:  \n",
    "1) lanzar un *run* base con el m√©todo del preset,  \n",
    "2) **comparar m√©todos** manteniendo fijos datos/modelo, y  \n",
    "3) generar un **resumen agregado** de resultados en `outputs/summary/`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Centralizar la configuraci√≥n de **modelo**, **datos/codificaci√≥n temporal**, **optimizador** y **m√©todo continual** v√≠a `presets.yaml`.\n",
    "- Soportar **H5 offline** (si `use_offline_spikes: true`) o **CSV + codificaci√≥n en runtime** (si `encode_runtime: true`), seleccion√°ndolo de forma coherente con el preset.\n",
    "- Comparar m√©todos (`naive`, `ewc`, `rehearsal`, `rehearsal+ewc`, y los bio-inspirados previstos: `as-snn`, `sa-snn`, `sca-snn`, `colanet`) con **id√©ntica preparaci√≥n de datos**.\n",
    "- Exportar un **CSV de agregados** con m√©tricas clave (MAE/MSE por tarea, olvido absoluto/relativo, etc.).\n",
    "\n",
    "## ‚úÖ Prerrequisitos\n",
    "- Haber generado `data/processed/tasks.json` (y opcionalmente `tasks_balanced.json`) con **01_DATA_QC_PREP** o **01A_PREP_BALANCED**.\n",
    "- Si el preset usa **offline** (`use_offline_spikes: true`), haber creado los H5 compatibles con **02_ENCODE_OFFLINE** (mismo `encoder/T/gain/size/to_gray` que el preset).\n",
    "- Revisar `configs/presets.yaml` (secciones `model`, `data`, `optim`, `continual`, `prep`).\n",
    "\n",
    "## ‚ö†Ô∏è Notas importantes\n",
    "- **No combines** `use_offline_spikes: true` y `encode_runtime: true`. El notebook lo detecta y lanza error.\n",
    "- La **semilla** global se toma de `CFG[\"data\"][\"seed\"]` para reproducibilidad.\n",
    "- La carpeta de salida incluye en el nombre preset, m√©todo, *encoder*, modelo, *seed*, etc., para facilitar trazabilidad.\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "## üß≠ √çndice\n",
    "\n",
    "- [1) Setup del entorno y paths](#sec-01)  \n",
    "- [2) Carga del preset unificado (`configs/presets.yaml`)](#sec-02)  \n",
    "- [3) Verificaci√≥n de datos y selecci√≥n de `tasks.json`](#sec-03)  \n",
    "- [4) Factories DataLoaders + Modelo (+ tasks)](#sec-04)  \n",
    "- [5) Ejecuci√≥n base con el preset (eco de config + run)](#sec-06)  \n",
    "- [6) Comparativa de m√©todos (mismo preset/semilla/datos)](#sec-07)  \n",
    "- [7) Barrido de combinaciones (opcional)](#sec-08)  \n",
    "- [8) Resumen completo: inventario ‚Üí parseo ‚Üí agregados ‚Üí tabla](#sec-09)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668cb21",
   "metadata": {},
   "source": [
    "<a id=\"sec-01\"></a>\n",
    "## 1) Setup del entorno y paths\n",
    "\n",
    "**Objetivo**  \n",
    "Preparar el entorno: limitar hilos BLAS (evitar *oversubscription*), detectar `ROOT` (ra√≠z del repo) y a√±adirlo a `sys.path`, importar utilidades del proyecto y seleccionar dispositivo (`cuda` si est√° disponible). Se activan optimizaciones de PyTorch en GPU (TF32/cuDNN) para acelerar.\n",
    "\n",
    "> Aqu√≠ **no** se leen a√∫n los presets; solo se configura el runtime global. \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9084ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Imports y setup de entorno (threads, paths, dispositivo)\n",
    "# =============================================================================\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, json, torch\n",
    "\n",
    "# Ra√≠z del repo y sys.path\n",
    "ROOT = Path.cwd().parents[0] if (Path.cwd().name == \"notebooks\") else Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "# Librer√≠as del proyecto\n",
    "from src.datasets import ImageTransform, AugmentConfig\n",
    "from src.models import build_model\n",
    "from src.utils import load_preset\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ajustes de rendimiento (opcional)\n",
    "torch.set_num_threads(4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851cb55",
   "metadata": {},
   "source": [
    "<a id=\"sec-02\"></a>\n",
    "\n",
    "## 2) Carga del preset unificado (`configs/presets.yaml`)\n",
    "\n",
    "**Objetivo**  \n",
    "Cargar un **preset** (`fast` | `std` | `accurate`) y derivar toda la configuraci√≥n operativa:\n",
    "\n",
    "- **Modelo/transform**: tama√±o de imagen, escala de grises, etc.\n",
    "- **Datos/codificaci√≥n**: `encoder` (`rate|latency|raw`), `T`, `gain`, `seed`.\n",
    "- **DataLoader**: `num_workers`, `prefetch_factor`, `pin_memory`, `persistent_workers`.\n",
    "- **Augment** opcional y **balanceo online** si procede.\n",
    "\n",
    "Incluye un **guardarra√≠l**: si `use_offline_spikes: true` y `encode_runtime: true` est√°n ambos activos, se aborta con un error claro (config inv√°lida).  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232708c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRESET=accurate] model=pilotnet_snn 200x66 gray=True\n",
      "[DATA] encoder=rate T=30 gain=0.5 seed=42\n",
      "[LOADER] workers=8 prefetch=2 pin=True persistent=True\n",
      "[BALANCE] online=False bins=50\n",
      "[RUNTIME_ENCODE] False | [OFFLINE_SPIKES] True\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Config global: presets.yaml\n",
    "# =============================================================================\n",
    "from pathlib import Path\n",
    "from src.datasets import ImageTransform, AugmentConfig\n",
    "from src.utils import load_preset\n",
    "\n",
    "PRESET = \"accurate\"  # fast | std | accurate\n",
    "CFG = load_preset(ROOT / \"configs\" / \"presets.yaml\", PRESET)\n",
    "\n",
    "# ---- Modelo / Transform ------------------------------------------------------\n",
    "MODEL_NAME = CFG[\"model\"][\"name\"]\n",
    "tfm = ImageTransform(\n",
    "    CFG[\"model\"][\"img_w\"],\n",
    "    CFG[\"model\"][\"img_h\"],\n",
    "    to_gray=bool(CFG[\"model\"][\"to_gray\"]),\n",
    "    crop_top=None\n",
    ")\n",
    "\n",
    "# ---- Datos / codificaci√≥n temporal ------------------------------------------\n",
    "ENCODER = CFG[\"data\"][\"encoder\"]\n",
    "T       = int(CFG[\"data\"][\"T\"])\n",
    "GAIN    = float(CFG[\"data\"][\"gain\"])\n",
    "SEED    = int(CFG[\"data\"][\"seed\"])\n",
    "\n",
    "USE_OFFLINE_SPIKES = bool(CFG[\"data\"].get(\"use_offline_spikes\", False))\n",
    "RUNTIME_ENCODE     = bool(CFG[\"data\"].get(\"encode_runtime\", False))\n",
    "\n",
    "# ---- DataLoader / augment / balanceo ----------------------------------------\n",
    "NUM_WORKERS = int(CFG[\"data\"].get(\"num_workers\") or 0)           # robusto ante None\n",
    "PREFETCH    = int(CFG[\"data\"].get(\"prefetch_factor\") or 2)       # <- casteo robusto\n",
    "PIN_MEMORY  = bool(CFG[\"data\"].get(\"pin_memory\", True))\n",
    "PERSISTENT  = bool(CFG[\"data\"].get(\"persistent_workers\", True))\n",
    "\n",
    "AUG_CFG = AugmentConfig(**(CFG[\"data\"].get(\"aug_train\") or {})) \\\n",
    "          if CFG[\"data\"].get(\"aug_train\") else None\n",
    "\n",
    "USE_ONLINE_BALANCING = bool(CFG[\"data\"].get(\"balance_online\", False))\n",
    "BAL_BINS = int(CFG[\"data\"].get(\"balance_bins\") or 50)\n",
    "BAL_EPS  = float(CFG[\"data\"].get(\"balance_smooth_eps\") or 1e-3)\n",
    "\n",
    "# Guardarra√≠les\n",
    "if USE_OFFLINE_SPIKES and RUNTIME_ENCODE:\n",
    "    raise RuntimeError(\"Config inv√°lida: use_offline_spikes=True y encode_runtime=True a la vez.\")\n",
    "\n",
    "print(f\"[PRESET={PRESET}] model={MODEL_NAME} {tfm.w}x{tfm.h} gray={tfm.to_gray}\")\n",
    "print(f\"[DATA] encoder={ENCODER} T={T} gain={GAIN} seed={SEED}\")\n",
    "print(f\"[LOADER] workers={NUM_WORKERS} prefetch={PREFETCH} pin={PIN_MEMORY} persistent={PERSISTENT}\")\n",
    "print(f\"[BALANCE] online={USE_ONLINE_BALANCING} bins={BAL_BINS}\")\n",
    "print(f\"[RUNTIME_ENCODE] {RUNTIME_ENCODE} | [OFFLINE_SPIKES] {USE_OFFLINE_SPIKES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5de0d",
   "metadata": {},
   "source": [
    "<a id=\"sec-03\"></a>\n",
    "\n",
    "## 3) Verificaci√≥n de datos y selecci√≥n de `tasks.json`\n",
    "\n",
    "**Objetivo**  \n",
    "Construir `task_list` y verificar que existen los *splits* por tarea:\n",
    "\n",
    "- Si el preset pide **balanced** (`prep.use_balanced_tasks: true`) y existe `tasks_balanced.json`, se usa; en caso contrario, se cae a `tasks.json` (se informa).\n",
    "- Se valida que `train/val/test.csv` existen para cada *run*.  \n",
    "- Si entrenas con **H5 offline**, se comprueba que est√°n presentes los H5 con **nomenclatura compatible** con el preset (`encoder/T/gain/size/to_gray`).\n",
    "\n",
    "> Si falta alg√∫n H5 requerido, genera primero con **02_ENCODE_OFFLINE**.  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c673a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: tasks_balanced.json\n",
      " - circuito1: train_balanced.csv\n",
      " - circuito2: train_balanced.csv\n",
      "OK: verificaci√≥n de splits.\n",
      "Preset en uso: accurate\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Verificaci√≥n de datos (splits y, si procede, H5)\n",
    "# =============================================================================\n",
    "from pathlib import Path as _P\n",
    "import json\n",
    "\n",
    "PROC = ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# --- Elegir tasks seg√∫n el preset ---\n",
    "USE_BALANCED = bool(CFG.get(\"prep\", {}).get(\"use_balanced_tasks\", False))\n",
    "tb_name = (CFG.get(\"prep\", {}).get(\"tasks_balanced_file_name\") or \"tasks_balanced.json\")\n",
    "t_name  = (CFG.get(\"prep\", {}).get(\"tasks_file_name\")           or \"tasks.json\")\n",
    "\n",
    "cand_bal = PROC / tb_name\n",
    "cand_std = PROC / t_name\n",
    "TASKS_FILE = cand_bal if (USE_BALANCED and cand_bal.exists()) else cand_std\n",
    "\n",
    "with open(TASKS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    tasks_json = json.load(f)\n",
    "\n",
    "task_list = [{\"name\": n, \"paths\": tasks_json[\"splits\"][n]} for n in tasks_json[\"tasks_order\"]]\n",
    "print(\"Usando:\", TASKS_FILE.name)\n",
    "for t in task_list:\n",
    "    print(f\" - {t['name']}: {_P(t['paths']['train']).name}\")\n",
    "\n",
    "# Guardarra√≠l: si se pidi√≥ balanced, exigir train_balanced.csv\n",
    "if USE_BALANCED:\n",
    "    for t in task_list:\n",
    "        train_path = _P(tasks_json[\"splits\"][t[\"name\"]][\"train\"])\n",
    "        if train_path.name != \"train_balanced.csv\":\n",
    "            raise RuntimeError(\n",
    "                f\"[{t['name']}] Esperaba 'train_balanced.csv' en modo balanced, pero encontr√© '{train_path.name}'.\"\n",
    "            )\n",
    "\n",
    "# Si entrenas con H5 offline, comprueba que existan\n",
    "if USE_OFFLINE_SPIKES:\n",
    "    mw, mh = CFG[\"model\"][\"img_w\"], CFG[\"model\"][\"img_h\"]\n",
    "    color = \"gray\" if CFG[\"model\"][\"to_gray\"] else \"rgb\"\n",
    "    gain_tag = (GAIN if ENCODER == \"rate\" else 0)\n",
    "    missing = []\n",
    "    for t in task_list:\n",
    "        run = t[\"name\"]\n",
    "        base = PROC / run\n",
    "        for split in (\"train\", \"val\", \"test\"):\n",
    "            expected = base / f\"{split}_{ENCODER}_T{T}_gain{gain_tag}_{color}_{mw}x{mh}.h5\"\n",
    "            if not expected.exists():\n",
    "                missing.append(str(expected))\n",
    "    if missing:\n",
    "        print(\"[WARN] Faltan H5 compatibles con el preset.\")\n",
    "        print(\"       Genera primero con 02_ENCODE_OFFLINE.ipynb (o tools/encode_tasks.py).\")\n",
    "\n",
    "print(\"OK: verificaci√≥n de splits.\")\n",
    "print(f\"Preset en uso: {PRESET}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926bb4b",
   "metadata": {},
   "source": [
    "<a id=\"sec-04\"></a>\n",
    "## 4) Factories unificados: DataLoaders + Modelo (+ tasks)\n",
    "\n",
    "**Objetivo**  \n",
    "Crear, en una sola llamada, los **componentes coherentes con el preset**:\n",
    "\n",
    "- `build_components_for(CFG, ROOT)` ‚Üí devuelve `tfm`, `make_loader_fn`, `make_model_fn`.\n",
    "  - El **loader** respeta autom√°ticamente el modo datos (H5 offline vs. CSV+encode runtime), *workers/prefetch/pin/persistent*, *augment*, y **balanceo online** si est√° activo.\n",
    "  - El **modelo** se instancia seg√∫n `model.name` y par√°metros asociados.\n",
    "- `build_task_list_for(CFG, ROOT)` ‚Üí devuelve `task_list` y el *tasks file* efectivamente usado.\n",
    "\n",
    "> Con esto evitas duplicar l√≥gica entre cuadernos y garantizas que **bench, entrenamiento y comparativa** usen la **misma** configuraci√≥n.  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a172cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks file: tasks_balanced.json\n",
      "make_loader_fn listo (H5 si use_offline_spikes=True; fallback CSV+runtime si no).\n"
     ]
    }
   ],
   "source": [
    "# === Factories y task list coherentes con el PRESET cargado ===\n",
    "from src.utils import build_task_list_for, build_components_for\n",
    "\n",
    "# Construye tfm, make_loader_fn y make_model_fn leyendo TODO de CFG (igual que hac√≠as a mano):\n",
    "tfm, make_loader_fn, make_model_fn = build_components_for(CFG, ROOT)\n",
    "\n",
    "# Elige autom√°ticamente tasks_balanced.json si el preset lo pide y existe; si no, tasks.json\n",
    "task_list, tasks_file = build_task_list_for(CFG, ROOT)\n",
    "\n",
    "print(\"Tasks file:\", tasks_file.name)\n",
    "print(\"make_loader_fn listo (H5 si use_offline_spikes=True; fallback CSV+runtime si no).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdd076",
   "metadata": {},
   "source": [
    "<a id=\"sec-05\"></a>\n",
    "## 5) Ejecuci√≥n base con el preset (eco de config + run)\n",
    "\n",
    "**Objetivo**  \n",
    "Lanzar **un experimento** con el m√©todo y par√°metros del preset (`CFG[\"continual\"]`). La celda:\n",
    "\n",
    "- Imprime un **resumen de configuraci√≥n** (modelo, datos, loader, m√©todo).\n",
    "- Ejecuta `run_continual(...)`.\n",
    "- Guarda resultados en `outputs/continual_*` (incluye `continual_results.json` y `manifest.json` por tarea).\n",
    "\n",
    "> Revisa la consola para confirmar dispositivo, *encoder/T/gain* y modo de datos (offline/ runtime).  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Ejecuci√≥n base con el preset (eco de config + run)\n",
    "# =============================================================================\n",
    "from src.runner import run_continual\n",
    "\n",
    "# Echo de configuraci√≥n ‚Äúresumido‚Äù (lo esencial para el run)\n",
    "print(f\"[RUN] preset={PRESET} | method={CFG['continual']['method']} \"\n",
    "      f\"| seed={CFG['data']['seed']} | enc={CFG['data']['encoder']} \"\n",
    "      f\"| kwargs={CFG['continual'].get('params', {})}\")\n",
    "print(f\"[MODEL] {MODEL_NAME} {tfm.w}x{tfm.h} gray={tfm.to_gray}\")\n",
    "print(f\"[DATA] T={CFG['data']['T']} gain={CFG['data']['gain']} \"\n",
    "      f\"| offline_spikes={CFG['data']['use_offline_spikes']} \"\n",
    "      f\"| runtime_encode={CFG['data']['encode_runtime']}\")\n",
    "print(f\"[LOADER] workers={CFG['data']['num_workers']} \"\n",
    "      f\"prefetch={CFG['data']['prefetch_factor']} pin={CFG['data']['pin_memory']} \"\n",
    "      f\"persistent={CFG['data']['persistent_workers']} \"\n",
    "      f\"| aug={bool(CFG['data']['aug_train'])} \"\n",
    "      f\"| balance_online={CFG['data']['balance_online']}\")\n",
    "\n",
    "out_path, _ = run_continual(\n",
    "    task_list=task_list,\n",
    "    make_loader_fn=make_loader_fn,   # wrapper (Celda 4)\n",
    "    make_model_fn=make_model_fn,     # factory (Celda 5)\n",
    "    tfm=tfm,\n",
    "    cfg=CFG,                         # preset completo\n",
    "    preset_name=PRESET,              # solo naming\n",
    "    out_root=ROOT / \"outputs\",\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"OK:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f15dc1",
   "metadata": {},
   "source": [
    "<a id=\"sec-06\"></a>\n",
    "## 6) Comparativa de m√©todos (mismo preset / misma semilla / mismos datos)\n",
    "\n",
    "**Objetivo**  \n",
    "Ejecutar una **bater√≠a de m√©todos** cambiando **solo** `continual.method` y sus `params`, manteniendo fijos: preset, semilla, *encoder/T/gain*, tama√±o de imagen, *augment*, etc.\n",
    "\n",
    "- Se clona `CFG` por m√©todo y se invoca `run_continual(...)` con las **factories** del propio `cfg_i`.\n",
    "- El diccionario `METHODS` puede ampliarse con nombres registrados en `src/methods/`:\n",
    "  - `naive`, `ewc`, `rehearsal`, `rehearsal+ewc`\n",
    "  - (bio-inspirados previstos) `as-snn`, `sa-snn`, `sca-snn`, `colanet`\n",
    "\n",
    "**Recomendaciones**\n",
    "- Si usas **offline H5**, aseg√∫rate de que existen para el preset (`02_ENCODE_OFFLINE`).\n",
    "- Si activas *replay* (rehearsal), puedes **reducir** `persistent_workers` para evitar atascos de DataLoader en algunos entornos; la celda ya lo ajusta como precauci√≥n.\n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f64d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPARATIVA DE M√âTODOS: mismo preset, misma semilla, mismos datos ===\n",
    "from copy import deepcopy\n",
    "from src.runner import run_continual\n",
    "from src.utils import build_task_list_for, build_components_for\n",
    "\n",
    "CFG_BASE = deepcopy(CFG)\n",
    "\n",
    "METHODS = {\n",
    "    #\"naive\": {},\n",
    "    #\"ewc\": {\"lam\": 7e8, \"fisher_batches\": 500},\n",
    "    #\"ewc\": {\"lam\": 1e9, \"fisher_batches\": 1000},\n",
    "    \"rehearsal\": {\"buffer_size\": 3000, \"replay_ratio\": 0.1},\n",
    "    \"rehearsal+ewc\": {\"buffer_size\": 3000, \"replay_ratio\": 0.1, \"lam\": 1e9, \"fisher_batches\": 1000},\n",
    "    \"as-snn\": {\"gamma_ratio\": 0.3, \"lambda_a\": 1.59168, \"ema\": 0.824},\n",
    "}\n",
    "\n",
    "METHODS = {\n",
    "    \"sa-snn\": {\"attach_to\": \"f6\", \"k\": 12, \"tau\": 20, \"th_min\": 1.0, \"th_max\": 2.0,\n",
    "                      \"vt_scale\": 1.25, \"p\": 3_000_000, \"flatten_spatial\": False,\n",
    "                      \"assume_binary_spikes\": False, \"reset_counters_each_task\": False},\n",
    "}\n",
    "\n",
    "METHODS = {\n",
    "  \"sa-snn\":  {\"attach_to\":\"f6\",\"k\":7,\"tau\":36,\"vt_scale\":1.7,\"p\":7_000_000,\n",
    "                     \"flatten_spatial\":False,\"assume_binary_spikes\":False,\"reset_counters_each_task\":False},\n",
    "  \"sa-snn\": {\"attach_to\":\"f6\",\"k\":10,\"tau\":24,\"vt_scale\":1.25,\"p\":3_000_000,\n",
    "                     \"flatten_spatial\":False,\"assume_binary_spikes\":False,\"reset_counters_each_task\":False},\n",
    "}\n",
    "\n",
    "runs_out = []\n",
    "for method_name, method_params in METHODS.items():\n",
    "    cfg_i = deepcopy(CFG_BASE)\n",
    "    cfg_i[\"continual\"][\"method\"] = method_name\n",
    "    cfg_i[\"continual\"][\"params\"] = method_params\n",
    "    if \"rehearsal\" in method_name:\n",
    "        cfg_i[\"data\"][\"persistent_workers\"] = False\n",
    "\n",
    "    # (Re)construye factories por si el cfg cambia\n",
    "    tfm_i, make_loader_fn_i, make_model_fn_i = build_components_for(cfg_i, ROOT)\n",
    "    task_list_i, tasks_file_i = build_task_list_for(cfg_i, ROOT)\n",
    "\n",
    "    print(f\"\\n=== RUN: preset={PRESET} | method={method_name} | seed={cfg_i['data']['seed']} \"\n",
    "          f\"| enc={cfg_i['data']['encoder']} | kwargs={method_params} ===\")\n",
    "    out_dir, _ = run_continual(\n",
    "        task_list=task_list_i,\n",
    "        make_loader_fn=make_loader_fn_i,\n",
    "        make_model_fn=make_model_fn_i,\n",
    "        tfm=tfm_i,\n",
    "        cfg=cfg_i,\n",
    "        preset_name=PRESET,\n",
    "        out_root=ROOT / \"outputs\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    runs_out.append(out_dir)\n",
    "\n",
    "print(\"\\nHecho:\", [str(p) for p in runs_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27426dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comparativa de variantes SA-SNN (preset=accurate) ===\n",
    "from copy import deepcopy\n",
    "from src.runner import run_continual\n",
    "from src.utils import build_task_list_for, build_components_for\n",
    "\n",
    "CFG_BASE = deepcopy(CFG)\n",
    "\n",
    "# Lista de (method_name, params) ‚Äî evita claves duplicadas en dict\n",
    "EXPS = [\n",
    "    (\"sa-snn\", {\n",
    "        \"attach_to\":\"f6\",\n",
    "        \"k\": 6,\n",
    "        \"tau\": 42,\n",
    "        \"vt_scale\": 2.0,\n",
    "        \"p\": 7_000_000,\n",
    "        \"flatten_spatial\": False,\n",
    "        \"assume_binary_spikes\": False,\n",
    "        \"reset_counters_each_task\": False,\n",
    "    }),\n",
    "    (\"sa-snn\", {\n",
    "        \"attach_to\":\"f6\",\n",
    "        \"k\": 9,\n",
    "        \"tau\": 28,\n",
    "        \"vt_scale\": 1.45,\n",
    "        \"p\": 5_000_000,\n",
    "        \"flatten_spatial\": False,\n",
    "        \"assume_binary_spikes\": False,\n",
    "        \"reset_counters_each_task\": False,\n",
    "    }),\n",
    "]\n",
    "\n",
    "runs_out = []\n",
    "for method_name, method_params in EXPS:\n",
    "    cfg_i = deepcopy(CFG_BASE)\n",
    "    cfg_i[\"continual\"][\"method\"] = method_name\n",
    "    cfg_i[\"continual\"][\"params\"] = method_params\n",
    "\n",
    "    # (Re)construye los componentes con este cfg\n",
    "    tfm_i, make_loader_fn_i, make_model_fn_i = build_components_for(cfg_i, ROOT)\n",
    "    task_list_i, tasks_file_i = build_task_list_for(cfg_i, ROOT)\n",
    "\n",
    "    print(f\"\\n=== RUN: preset={PRESET} | method={method_name} | seed={cfg_i['data']['seed']} \"\n",
    "          f\"| enc={cfg_i['data']['encoder']} | kwargs={method_params} ===\")\n",
    "\n",
    "    out_dir, _ = run_continual(\n",
    "        task_list=task_list_i,\n",
    "        make_loader_fn=make_loader_fn_i,\n",
    "        make_model_fn=make_model_fn_i,\n",
    "        tfm=tfm_i,\n",
    "        cfg=cfg_i,\n",
    "        preset_name=PRESET,      # mismo preset que vienes usando\n",
    "        out_root=ROOT / \"outputs\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    runs_out.append(out_dir)\n",
    "\n",
    "print(\"\\nHecho:\", [str(p) for p in runs_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c3c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN (sa-snn_balanced@accurate) ===\n",
      "preset=accurate | method=sa-snn | kwargs={'attach_to': 'f6', 'k': 8, 'tau': 32, 'vt_scale': 1.55, 'p': 5000000, 'flatten_spatial': False, 'assume_binary_spikes': False, 'reset_counters_each_task': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUN (sa-snn_plastic-lite@accurate) ===\n",
      "preset=accurate | method=sa-snn | kwargs={'attach_to': 'f6', 'k': 10, 'tau': 28, 'vt_scale': 1.35, 'p': 4000000, 'flatten_spatial': False, 'assume_binary_spikes': False, 'reset_counters_each_task': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "# === Variantes SA-SNN (preset accurate) y logs completos a fichero ===\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from src.runner import run_continual\n",
    "from src.utils import build_task_list_for, build_components_for\n",
    "\n",
    "CFG_BASE = deepcopy(CFG)\n",
    "OUT_ROOT = Path(ROOT) / \"outputs\"\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    {\n",
    "        \"name\":   \"sa-snn_balanced@accurate\",\n",
    "        \"preset\": \"accurate\",\n",
    "        \"method\": \"sa-snn\",\n",
    "        \"params\": {\n",
    "            \"attach_to\": \"f6\",\n",
    "            \"k\": 8,\n",
    "            \"tau\": 32,\n",
    "            \"vt_scale\": 1.55,\n",
    "            \"p\": 5_000_000,\n",
    "            \"flatten_spatial\": False,\n",
    "            \"assume_binary_spikes\": False,\n",
    "            \"reset_counters_each_task\": False,\n",
    "            # opcional si tu impl lo soporta:\n",
    "            # \"trace_every\": 8192\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\":   \"sa-snn_plastic-lite@accurate\",\n",
    "        \"preset\": \"accurate\",\n",
    "        \"method\": \"sa-snn\",\n",
    "        \"params\": {\n",
    "            \"attach_to\": \"f6\",\n",
    "            \"k\": 10,\n",
    "            \"tau\": 28,\n",
    "            \"vt_scale\": 1.35,\n",
    "            \"p\": 4_000_000,\n",
    "            \"flatten_spatial\": False,\n",
    "            \"assume_binary_spikes\": False,\n",
    "            \"reset_counters_each_task\": False,\n",
    "            # \"trace_every\": 8192\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    {\n",
    "        \"name\":   \"E1_k8_tau32_vt133_ampwarm\",\n",
    "        \"preset\": \"accurate\",\n",
    "        \"method\": \"sa-snn\",\n",
    "        \"params\": {\n",
    "            \"attach_to\": \"f6\",\n",
    "            \"k\": 8,\n",
    "            \"tau\": 32,\n",
    "            \"vt_scale\": 1.33,\n",
    "            \"th\": [0.95, 1.9],\n",
    "            \"flatten_spatial\": False,\n",
    "            \"assume_binary_spikes\": False,\n",
    "            \"reset_counters_each_task\": False,\n",
    "            \"amp_schedule\": [\n",
    "                {\"t_max\": 3, \"amp\": False},\n",
    "                {\"t_max\": 30, \"amp\": True},\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\":   \"E2_k8_tau32_vt133_ampoff\",\n",
    "        \"preset\": \"accurate\",\n",
    "        \"method\": \"sa-snn\",\n",
    "        \"params\": {\n",
    "            \"attach_to\": \"f6\",\n",
    "            \"k\": 8,\n",
    "            \"tau\": 32,\n",
    "            \"vt_scale\": 1.33,\n",
    "            \"th\": [0.95, 1.9],\n",
    "            \"flatten_spatial\": False,\n",
    "            \"assume_binary_spikes\": False,\n",
    "            \"reset_counters_each_task\": False,\n",
    "            \"AMP\": False,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\":   \"E3_k8_tau28_vt133_ampwarm\",\n",
    "        \"preset\": \"accurate\",\n",
    "        \"method\": \"sa-snn\",\n",
    "        \"params\": {\n",
    "            \"attach_to\": \"f6\",\n",
    "            \"k\": 8,\n",
    "            \"tau\": 28,\n",
    "            \"vt_scale\": 1.33,\n",
    "            \"th\": [0.95, 1.9],\n",
    "            \"flatten_spatial\": False,\n",
    "            \"assume_binary_spikes\": False,\n",
    "            \"reset_counters_each_task\": False,\n",
    "            \"amp_schedule\": [\n",
    "                {\"t_max\": 3, \"amp\": False},\n",
    "                {\"t_max\": 30, \"amp\": True},\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for exp in EXPERIMENTS:\n",
    "    cfg_run = deepcopy(CFG_BASE)\n",
    "    cfg_run[\"continual\"][\"method\"] = exp[\"method\"]\n",
    "    cfg_run[\"continual\"][\"params\"] = exp[\"params\"]\n",
    "\n",
    "    tfm_i, make_loader_fn_i, make_model_fn_i = build_components_for(cfg_run, ROOT)\n",
    "    task_list_i, tasks_file_i = build_task_list_for(cfg_run, ROOT)\n",
    "\n",
    "    log_file = OUT_ROOT / f\"{exp['name']}.log\"\n",
    "    print(f\"\\n=== RUN ({exp['name']}) ===\")\n",
    "    print(f\"preset={exp['preset']} | method={exp['method']} | kwargs={exp['params']}\")\n",
    "    with open(log_file, \"w\") as f, redirect_stdout(f):\n",
    "        out_dir, _ = run_continual(\n",
    "            task_list=task_list_i,\n",
    "            make_loader_fn=make_loader_fn_i,\n",
    "            make_model_fn=make_model_fn_i,\n",
    "            tfm=tfm_i,\n",
    "            cfg=cfg_run,\n",
    "            preset_name=exp[\"preset\"],\n",
    "            out_root=OUT_ROOT,\n",
    "            verbose=True,\n",
    "        )\n",
    "    results[exp[\"name\"]] = {\"out_dir\": str(out_dir), \"log\": str(log_file)}\n",
    "\n",
    "print(\"\\n[FIN] Experimentos:\")\n",
    "for k, v in results.items():\n",
    "    print(f\" - {k}: {v['out_dir']} (log: {v['log']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e604e8",
   "metadata": {},
   "source": [
    "<a id=\"sec-07\"></a>\n",
    "## 7) Barrido de combinaciones (opcional)\n",
    "\n",
    "**Objetivo**  \n",
    "Explorar matrices de configuraci√≥n (**presets √ó seeds √ó encoders √ó m√©todos**) para estudios amplios.\n",
    "\n",
    "- **Coste alto**: controla `batch_size`, `T` y *workers/prefetch* si la GPU va justa.\n",
    "- Si usas **offline**, genera los H5 para cada combinaci√≥n (`encoder/T/gain/size/to_gray`) antes del barrido.\n",
    "- Mant√©n **nomenclatura consistente** (el *runner* la parsea despu√©s para el resumen).\n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Driver de ejecuci√≥n: barrido de combinaciones (opcional)\n",
    "# =============================================================================\n",
    "from copy import deepcopy\n",
    "from src.runner import run_continual\n",
    "from src.utils import load_preset, build_task_list_for, build_components_for\n",
    "\n",
    "PRESETS   = [\"std\"]  # [PRESET] a√±ade \"std\", \"accurate\" si quieres\n",
    "SEEDS     = [CFG[\"data\"][\"seed\"]] # [CFG[\"data\"][\"seed\"], 43]\n",
    "ENCODERS  = [CFG[\"data\"][\"encoder\"]]\n",
    "METHODS   = [\n",
    "    # (\"naive\", {}),\n",
    "    # (\"ewc\", {\"lam\": 1e9, \"fisher_batches\": 600}),\n",
    "    # (\"rehearsal\", {\"buffer_size\": 5000, \"replay_ratio\": 0.2}),\n",
    "    # (\"rehearsal+ewc\", {\"buffer_size\": 5000, \"replay_ratio\": 0.2, \"lam\": 7e8, \"fisher_batches\": 600}),\n",
    "    (\"as-snn\", {\"gamma_ratio\": 0.3, \"lambda_a\": 1.59168, \"ema\": 0.824}),\n",
    "]\n",
    "\n",
    "METHODS   = [\n",
    "    # (\"naive\", {}),\n",
    "    # (\"ewc\", {\"lam\": 1e9, \"fisher_batches\": 600}),\n",
    "    # (\"rehearsal\", {\"buffer_size\": 5000, \"replay_ratio\": 0.2}),\n",
    "    # (\"rehearsal+ewc\", {\"buffer_size\": 5000, \"replay_ratio\": 0.2, \"lam\": 7e8, \"fisher_batches\": 600}),\n",
    "    (\"sa-snn\", {\n",
    "        \"k\": 10,\n",
    "        \"tau\": 10.0,\n",
    "        \"th_min\": 1.0,\n",
    "        \"th_max\": 2.0,\n",
    "        \"p\": 2000000,\n",
    "        \"vt_scale\": 1.0,\n",
    "        # \"attach_to\": \"classifier.0\",   # opcional: si sabes la capa densa\n",
    "        \"flatten_spatial\": False,\n",
    "        \"assume_binary_spikes\": False,\n",
    "        \"reset_counters_each_task\": False,\n",
    "    }),\n",
    "]\n",
    "\n",
    "for preset_i in PRESETS:\n",
    "    CFG_i = load_preset(ROOT / \"configs\" / \"presets.yaml\", preset_i)\n",
    "    for seed_i in SEEDS:\n",
    "        for enc_i in ENCODERS:\n",
    "            for method_name, method_params in METHODS:\n",
    "                cfg_i2 = deepcopy(CFG_i)\n",
    "                cfg_i2[\"data\"][\"seed\"] = seed_i\n",
    "                cfg_i2[\"data\"][\"encoder\"] = enc_i\n",
    "                cfg_i2[\"continual\"][\"method\"] = method_name\n",
    "                cfg_i2[\"continual\"][\"params\"] = method_params\n",
    "                if \"rehearsal\" in method_name:\n",
    "                    cfg_i2[\"data\"][\"persistent_workers\"] = False\n",
    "\n",
    "                tfm_i, make_loader_fn_i, make_model_fn_i = build_components_for(cfg_i2, ROOT)\n",
    "                task_list_i, tasks_file_i = build_task_list_for(cfg_i2, ROOT)\n",
    "\n",
    "                print(f\"\\n=== RUN: preset={preset_i} | method={method_name} | seed={seed_i} \"\n",
    "                      f\"| enc={enc_i} | kwargs={method_params} ===\")\n",
    "                out_path, _ = run_continual(\n",
    "                    task_list=task_list_i,\n",
    "                    make_loader_fn=make_loader_fn_i,\n",
    "                    make_model_fn=make_model_fn_i,\n",
    "                    tfm=tfm_i,\n",
    "                    cfg=cfg_i2,\n",
    "                    preset_name=preset_i,\n",
    "                    out_root=ROOT / \"outputs\",\n",
    "                    verbose=True,\n",
    "                )\n",
    "                print(\"OK:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0650365",
   "metadata": {},
   "source": [
    "<a id=\"sec-08\"></a>\n",
    "## 8) Resumen completo: inventario ‚Üí parseo ‚Üí agregados ‚Üí tabla\n",
    "\n",
    "**Objetivo**  \n",
    "Crear un **resumen reproducible** de todos los *runs*:\n",
    "\n",
    "- **Inventario** de carpetas `outputs/continual_*`.\n",
    "- **Parseo** de nombres para extraer `preset`, `m√©todo`, `encoder`, `seed`, `modelo`, y par√°metros relevantes.\n",
    "- C√°lculo de **olvido** (absoluto y relativo) y **agregados** por grupo (media, œÉ, n).\n",
    "- Export a `outputs/summary/continual_summary_agg.csv` y **tabla formateada** para la memoria.\n",
    "\n",
    "> Si no se detectan *runs*, verifica que exista `continual_results.json` dentro de cada carpeta.  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Resumen de runs (usa utilidades comunes) ===\n",
    "from pathlib import Path\n",
    "from src.utils_exp import build_runs_df, aggregate_and_show\n",
    "\n",
    "outputs_root = ROOT / \"outputs\"\n",
    "\n",
    "# (Opcional) inventario r√°pido en disco\n",
    "print(\"Inventario de runs en:\", outputs_root)\n",
    "for p in sorted(outputs_root.glob(\"continual_*\")):\n",
    "    print(\" -\", p.name, \"| results.json:\", (p / \"continual_results.json\").exists())\n",
    "\n",
    "df = build_runs_df(outputs_root)\n",
    "print(f\"runs en resumen: {len(df)}\")\n",
    "_ = aggregate_and_show(df, outputs_root)  # tambi√©n guarda CSV en outputs/summary/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
