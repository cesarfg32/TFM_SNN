{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c448d5",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# 03_TRAIN_CONTINUAL ‚Äî Entrenamiento Continual con *presets*\n",
    "\n",
    "**Qu√© hace este notebook**  \n",
    "Entrena y eval√∫a modelos en **aprendizaje continual** (secuencia de tareas) usando una **configuraci√≥n unificada** desde `configs/presets.yaml`. Permite:  \n",
    "1) lanzar un *run* base con el m√©todo del preset,  \n",
    "2) **comparar m√©todos** manteniendo fijos datos/modelo, y  \n",
    "3) generar un **resumen agregado** de resultados en `outputs/summary/`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Centralizar la configuraci√≥n de **modelo**, **datos/codificaci√≥n temporal**, **optimizador** y **m√©todo continual** v√≠a `presets.yaml`.\n",
    "- Soportar **H5 offline** (si `use_offline_spikes: true`) o **CSV + codificaci√≥n en runtime** (si `encode_runtime: true`), seleccion√°ndolo de forma coherente con el preset.\n",
    "- Comparar m√©todos (`naive`, `ewc`, `rehearsal`, `rehearsal+ewc`, y los bio-inspirados previstos: `as-snn`, `sa-snn`, `sca-snn`, `colanet`) con **id√©ntica preparaci√≥n de datos**.\n",
    "- Exportar un **CSV de agregados** con m√©tricas clave (MAE/MSE por tarea, olvido absoluto/relativo, etc.).\n",
    "\n",
    "## ‚úÖ Prerrequisitos\n",
    "- Haber generado `data/processed/tasks.json` (y opcionalmente `tasks_balanced.json`) con **01_DATA_QC_PREP** o **01A_PREP_BALANCED**.\n",
    "- Si el preset usa **offline** (`use_offline_spikes: true`), haber creado los H5 compatibles con **02_ENCODE_OFFLINE** (mismo `encoder/T/gain/size/to_gray` que el preset).\n",
    "- Revisar `configs/presets.yaml` (secciones `model`, `data`, `optim`, `continual`, `prep`).\n",
    "\n",
    "## ‚ö†Ô∏è Notas importantes\n",
    "- **No combines** `use_offline_spikes: true` y `encode_runtime: true`. El notebook lo detecta y lanza error.\n",
    "- La **semilla** global se toma de `CFG[\"data\"][\"seed\"]` para reproducibilidad.\n",
    "- La carpeta de salida incluye en el nombre preset, m√©todo, *encoder*, modelo, *seed*, etc., para facilitar trazabilidad.\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "## üß≠ √çndice\n",
    "\n",
    "- [1) Setup del entorno y paths](#sec-01)  \n",
    "- [2) Carga del preset unificado (`configs/presets.yaml`)](#sec-02)  \n",
    "- [3) Verificaci√≥n de datos y selecci√≥n de `tasks.json`](#sec-03)  \n",
    "- [4) Factories DataLoaders + Modelo (+ tasks)](#sec-04)  \n",
    "- [5) Ejecuci√≥n base con el preset (eco de config + run)](#sec-06)  \n",
    "- [6) Comparativa de m√©todos (mismo preset/semilla/datos)](#sec-07)  \n",
    "- [7) Barrido de combinaciones (opcional)](#sec-08)  \n",
    "- [8) Resumen completo: inventario ‚Üí parseo ‚Üí agregados ‚Üí tabla](#sec-09)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668cb21",
   "metadata": {},
   "source": [
    "<a id=\"sec-01\"></a>\n",
    "## 1) Setup del entorno y paths\n",
    "\n",
    "**Objetivo**  \n",
    "Preparar el entorno: limitar hilos BLAS (evitar *oversubscription*), detectar `ROOT` (ra√≠z del repo) y a√±adirlo a `sys.path`, importar utilidades del proyecto y seleccionar dispositivo (`cuda` si est√° disponible). Se activan optimizaciones de PyTorch en GPU (TF32/cuDNN) para acelerar.\n",
    "\n",
    "> Aqu√≠ **no** se leen a√∫n los presets; solo se configura el runtime global. \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1) Setup del entorno y paths\n",
    "# =============================================================================\n",
    "import os, torch\n",
    "\n",
    "# Robustez multiproceso/WSL\n",
    "# os.environ[\"PYTORCH_SHARING_STRATEGY\"] = \"file_system\"\n",
    "# os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "\n",
    "# >>> Memoria CUDA: allocator estable en runs largos <<<\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64,garbage_collection_threshold:0.6\"\n",
    "\n",
    "# Loggear it/s cada epoch desde training.py (opcional)\n",
    "os.environ[\"TRAIN_LOG_ITPS\"] = \"1\"\n",
    "\n",
    "try:\n",
    "    import torch.multiprocessing as mp\n",
    "    mp.set_sharing_strategy(\"file_system\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# (solo para depurar ca√≠das puntuales; descomenta si quieres localizar llamada)\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, torch\n",
    "\n",
    "ROOT = Path.cwd().parents[0] if (Path.cwd().name == \"notebooks\") else Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from src.datasets import ImageTransform, AugmentConfig\n",
    "from src.models   import build_model\n",
    "from src.utils    import load_preset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_num_threads(4)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "OUT = ROOT / \"outputs\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"OUT :\", OUT)\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851cb55",
   "metadata": {},
   "source": [
    "<a id=\"sec-02\"></a>\n",
    "\n",
    "## 2) Carga del preset unificado (`configs/presets.yaml`)\n",
    "\n",
    "**Objetivo**  \n",
    "Cargar un **preset** (`fast` | `std` | `accurate`) y derivar toda la configuraci√≥n operativa:\n",
    "\n",
    "- **Modelo/transform**: tama√±o de imagen, escala de grises, etc.\n",
    "- **Datos/codificaci√≥n**: `encoder` (`rate|latency|raw`), `T`, `gain`, `seed`.\n",
    "- **DataLoader**: `num_workers`, `prefetch_factor`, `pin_memory`, `persistent_workers`.\n",
    "- **Augment** opcional y **balanceo online** si procede.\n",
    "\n",
    "Incluye un **guardarra√≠l**: si `use_offline_spikes: true` y `encode_runtime: true` est√°n ambos activos, se aborta con un error claro (config inv√°lida).  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232708c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2) Carga del preset (configs/presets.yaml)\n",
    "# =============================================================================\n",
    "from src.utils import load_preset\n",
    "\n",
    "PRESET = \"accurate\"  # \"fast\" para pruebas r√°pidas, \"accurate\" para resultados finales\n",
    "CFG = load_preset(ROOT / \"configs\" / \"presets.yaml\", PRESET)\n",
    "\n",
    "# ---- Modelo / Transform ----\n",
    "MODEL_NAME = CFG[\"model\"][\"name\"]\n",
    "tfm = ImageTransform(\n",
    "    CFG[\"model\"][\"img_w\"],\n",
    "    CFG[\"model\"][\"img_h\"],\n",
    "    to_gray=bool(CFG[\"model\"][\"to_gray\"]),\n",
    "    crop_top=None,\n",
    ")\n",
    "\n",
    "# ---- Datos / codificaci√≥n ----\n",
    "ENCODER = CFG[\"data\"][\"encoder\"]\n",
    "T       = int(CFG[\"data\"][\"T\"])\n",
    "GAIN    = float(CFG[\"data\"][\"gain\"])\n",
    "SEED    = int(CFG[\"data\"][\"seed\"])\n",
    "\n",
    "USE_OFFLINE_SPIKES = bool(CFG[\"data\"].get(\"use_offline_spikes\", False))\n",
    "RUNTIME_ENCODE     = bool(CFG[\"data\"].get(\"encode_runtime\", False))\n",
    "\n",
    "# ---- Loader / augment / balanceo ----\n",
    "NUM_WORKERS = int(CFG[\"data\"].get(\"num_workers\") or 0)\n",
    "PREFETCH    = int(CFG[\"data\"].get(\"prefetch_factor\") or 2)\n",
    "PIN_MEMORY  = bool(CFG[\"data\"].get(\"pin_memory\", True))\n",
    "PERSISTENT  = bool(CFG[\"data\"].get(\"persistent_workers\", True))\n",
    "\n",
    "AUG_CFG = AugmentConfig(**(CFG[\"data\"].get(\"aug_train\") or {})) \\\n",
    "          if CFG[\"data\"].get(\"aug_train\") else None\n",
    "\n",
    "USE_ONLINE_BALANCING = bool(CFG[\"data\"].get(\"balance_online\", False))\n",
    "BAL_BINS = int(CFG[\"data\"].get(\"balance_bins\") or 50)\n",
    "BAL_EPS  = float(CFG[\"data\"].get(\"balance_smooth_eps\") or 1e-3)\n",
    "\n",
    "# Guardarra√≠l\n",
    "if USE_OFFLINE_SPIKES and RUNTIME_ENCODE:\n",
    "    raise RuntimeError(\"Config inv√°lida: use_offline_spikes=True y encode_runtime=True a la vez.\")\n",
    "\n",
    "print(f\"[PRESET={PRESET}] model={MODEL_NAME} {tfm.w}x{tfm.h} gray={tfm.to_gray}\")\n",
    "print(f\"[DATA] encoder={ENCODER} T={T} gain={GAIN} seed={SEED}\")\n",
    "print(f\"[LOADER] workers={NUM_WORKERS} prefetch={PREFETCH} pin={PIN_MEMORY} persistent={PERSISTENT}\")\n",
    "print(f\"[BALANCE] online={USE_ONLINE_BALANCING} bins={BAL_BINS} eps={BAL_EPS}\")\n",
    "print(f\"[RUNTIME_ENCODE] {RUNTIME_ENCODE} | [OFFLINE_SPIKES] {USE_OFFLINE_SPIKES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5de0d",
   "metadata": {},
   "source": [
    "<a id=\"sec-03\"></a>\n",
    "\n",
    "## 3) Verificaci√≥n de datos y selecci√≥n de `tasks.json`\n",
    "\n",
    "**Objetivo**  \n",
    "Construir `task_list` y verificar que existen los *splits* por tarea:\n",
    "\n",
    "- Si el preset pide **balanced** (`prep.use_balanced_tasks: true`) y existe `tasks_balanced.json`, se usa; en caso contrario, se cae a `tasks.json` (se informa).\n",
    "- Se valida que `train/val/test.csv` existen para cada *run*.  \n",
    "- Si entrenas con **H5 offline**, se comprueba que est√°n presentes los H5 con **nomenclatura compatible** con el preset (`encoder/T/gain/size/to_gray`).\n",
    "\n",
    "> Si falta alg√∫n H5 requerido, genera primero con **02_ENCODE_OFFLINE**.  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c673a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3) Verificaci√≥n de datos (splits y, si procede, H5)\n",
    "# =============================================================================\n",
    "import json\n",
    "from pathlib import Path as _P\n",
    "\n",
    "PROC = ROOT / \"data\" / \"processed\"\n",
    "\n",
    "USE_BALANCED = bool(CFG.get(\"prep\", {}).get(\"use_balanced_tasks\", False))\n",
    "tb_name = (CFG.get(\"prep\", {}).get(\"tasks_balanced_file_name\") or \"tasks_balanced.json\")\n",
    "t_name  = (CFG.get(\"prep\", {}).get(\"tasks_file_name\") or \"tasks.json\")\n",
    "\n",
    "cand_bal = PROC / tb_name\n",
    "cand_std = PROC / t_name\n",
    "TASKS_FILE = cand_bal if (USE_BALANCED and cand_bal.exists()) else cand_std\n",
    "\n",
    "with open(TASKS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    tasks_json = json.load(f)\n",
    "\n",
    "task_list = [{\"name\": n, \"paths\": tasks_json[\"splits\"][n]} for n in tasks_json[\"tasks_order\"]]\n",
    "print(\"Usando:\", TASKS_FILE.name)\n",
    "for t in task_list:\n",
    "    print(f\" - {t['name']}: {_P(t['paths']['train']).name}\")\n",
    "\n",
    "if USE_BALANCED:\n",
    "    for t in task_list:\n",
    "        train_path = _P(tasks_json[\"splits\"][t[\"name\"]][\"train\"])\n",
    "        if train_path.name != \"train_balanced.csv\":\n",
    "            raise RuntimeError(\n",
    "                f\"[{t['name']}] Esperaba 'train_balanced.csv' en modo balanced, pero encontr√© '{train_path.name}'.\"\n",
    "            )\n",
    "\n",
    "if USE_OFFLINE_SPIKES:\n",
    "    mw, mh = CFG[\"model\"][\"img_w\"], CFG[\"model\"][\"img_h\"]\n",
    "    color = \"gray\" if CFG[\"model\"][\"to_gray\"] else \"rgb\"\n",
    "    gain_tag = (GAIN if ENCODER == \"rate\" else 0)\n",
    "    missing = []\n",
    "    for t in task_list:\n",
    "        base = PROC / t[\"name\"]\n",
    "        for split in (\"train\", \"val\", \"test\"):\n",
    "            expected = base / f\"{split}_{ENCODER}_T{T}_gain{gain_tag}_{color}_{mw}x{mh}.h5\"\n",
    "            if not expected.exists():\n",
    "                missing.append(str(expected))\n",
    "    if missing:\n",
    "        print(\"[WARN] Faltan H5 compatibles con el preset. Genera con 02_ENCODE_OFFLINE.\")\n",
    "print(\"OK: verificaci√≥n de splits.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926bb4b",
   "metadata": {},
   "source": [
    "<a id=\"sec-04\"></a>\n",
    "## 4) Factories unificados: DataLoaders + Modelo (+ tasks)\n",
    "\n",
    "**Objetivo**  \n",
    "Crear, en una sola llamada, los **componentes coherentes con el preset**:\n",
    "\n",
    "- `build_components_for(CFG, ROOT)` ‚Üí devuelve `tfm`, `make_loader_fn`, `make_model_fn`.\n",
    "  - El **loader** respeta autom√°ticamente el modo datos (H5 offline vs. CSV+encode runtime), *workers/prefetch/pin/persistent*, *augment*, y **balanceo online** si est√° activo.\n",
    "  - El **modelo** se instancia seg√∫n `model.name` y par√°metros asociados.\n",
    "- `build_task_list_for(CFG, ROOT)` ‚Üí devuelve `task_list` y el *tasks file* efectivamente usado.\n",
    "\n",
    "> Con esto evitas duplicar l√≥gica entre cuadernos y garantizas que **bench, entrenamiento y comparativa** usen la **misma** configuraci√≥n.  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4) Factories: DataLoaders + Modelo + task_list\n",
    "# =============================================================================\n",
    "from src.utils import build_task_list_for, build_components_for\n",
    "\n",
    "tfm, make_loader_fn, make_model_fn = build_components_for(CFG, ROOT)\n",
    "task_list, tasks_file = build_task_list_for(CFG, ROOT)\n",
    "\n",
    "print(\"Tasks file:\", tasks_file.name)\n",
    "print(\"make_loader_fn listo (usa H5 si offline; si no, CSV + encode runtime).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdd076",
   "metadata": {},
   "source": [
    "<a id=\"sec-05\"></a>\n",
    "## 5) Ejecuci√≥n base con el preset (eco de config + run)\n",
    "\n",
    "**Objetivo**  \n",
    "Lanzar **un experimento** con el m√©todo y par√°metros del preset (`CFG[\"continual\"]`). La celda:\n",
    "\n",
    "- Imprime un **resumen de configuraci√≥n** (modelo, datos, loader, m√©todo).\n",
    "- Ejecuta `run_continual(...)`.\n",
    "- Guarda resultados en `outputs/continual_*` (incluye `continual_results.json` y `manifest.json` por tarea).\n",
    "\n",
    "> Revisa la consola para confirmar dispositivo, *encoder/T/gain* y modo de datos (offline/ runtime).  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5) Ejecuci√≥n base con el preset\n",
    "# =============================================================================\n",
    "from src.runner import run_continual\n",
    "\n",
    "print(f\"[RUN] preset={PRESET} | method={CFG['continual']['method']} \"\n",
    "      f\"| seed={CFG['data']['seed']} | enc={CFG['data']['encoder']} \"\n",
    "      f\"| kwargs={CFG['continual'].get('params', {})}\")\n",
    "print(f\"[MODEL] {MODEL_NAME} {tfm.w}x{tfm.h} gray={tfm.to_gray}\")\n",
    "print(f\"[DATA] T={CFG['data']['T']} gain={CFG['data']['gain']} \"\n",
    "      f\"| offline_spikes={CFG['data']['use_offline_spikes']} \"\n",
    "      f\"| runtime_encode={CFG['data']['encode_runtime']}\")\n",
    "print(f\"[LOADER] workers={CFG['data']['num_workers']} \"\n",
    "      f\"prefetch={CFG['data']['prefetch_factor']} pin={CFG['data']['pin_memory']} \"\n",
    "      f\"persistent={CFG['data']['persistent_workers']} \"\n",
    "      f\"| aug={bool(CFG['data']['aug_train'])} \"\n",
    "      f\"| balance_online={CFG['data']['balance_online']}\")\n",
    "\n",
    "out_path, _ = run_continual(\n",
    "    task_list=task_list,\n",
    "    make_loader_fn=make_loader_fn,\n",
    "    make_model_fn=make_model_fn,\n",
    "    tfm=tfm,\n",
    "    cfg=CFG,\n",
    "    preset_name=PRESET,\n",
    "    out_root=OUT,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"OK:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f15dc1",
   "metadata": {},
   "source": [
    "<a id=\"sec-06\"></a>\n",
    "## 6) Comparativa de m√©todos (mismo preset / misma semilla / mismos datos)\n",
    "\n",
    "**Objetivo**  \n",
    "Ejecutar una **bater√≠a de m√©todos** cambiando **solo** `continual.method` y sus `params`, manteniendo fijos: preset, semilla, *encoder/T/gain*, tama√±o de imagen, *augment*, etc.\n",
    "\n",
    "- Se clona `CFG` por m√©todo y se invoca `run_continual(...)` con las **factories** del propio `cfg_i`.\n",
    "- El diccionario `METHODS` puede ampliarse con nombres registrados en `src/methods/`:\n",
    "  - `naive`, `ewc`, `rehearsal`, `rehearsal+ewc`\n",
    "  - (bio-inspirados previstos) `as-snn`, `sa-snn`, `sca-snn`, `colanet`\n",
    "\n",
    "**Recomendaciones**\n",
    "- Si usas **offline H5**, aseg√∫rate de que existen para el preset (`02_ENCODE_OFFLINE`).\n",
    "- Si activas *replay* (rehearsal), puedes **reducir** `persistent_workers` para evitar atascos de DataLoader en algunos entornos; la celda ya lo ajusta como precauci√≥n.\n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f64d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXPERIMENTS = [\n",
    "    # (\"naive\", {}),\n",
    "    # (\"ewc\", {\"lam\": 1e9, \"fisher_batches\": 1000}),\n",
    "    # (\"rehearsal\", {\"buffer_size\": 3000, \"replay_ratio\": 0.1}),\n",
    "    # (\"rehearsal+ewc\", {\"buffer_size\": 3000, \"replay_ratio\": 0.1, \"lam\": 1e9, \"fisher_batches\": 1000}),\n",
    "    # (\"sa-snn\", {\"attach_to\":\"f6\",\"k\":8,\"tau\":28,\"vt_scale\":1.33,\"p\":2_000_000,\n",
    "    #             \"flatten_spatial\":False,\"assume_binary_spikes\":False,\"reset_counters_each_task\":False}),\n",
    "    # (\"sa-snn\", {\"attach_to\":\"f6\",\"k\":8,\"tau\":32,\"vt_scale\":1.33,\"p\":5_000_000,\n",
    "    #             \"flatten_spatial\":False,\"assume_binary_spikes\":False,\"reset_counters_each_task\":False}),\n",
    "    # (\"sa-snn\", {\"attach_to\":\"f6\",\"k\":9,\"tau\":28,\"vt_scale\":1.33,\"p\":5_000_000,\n",
    "    #             \"flatten_spatial\":False,\"assume_binary_spikes\":False,\"reset_counters_each_task\":False}),\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    # --- Cl√°sicos ---\n",
    "    (\"naive\", {}),  # baseline sin mitigaci√≥n del olvido\n",
    "\n",
    "    # Mejor EWC que ya viste en accurate\n",
    "    (\"ewc\", {\"lam\": 7e8, \"fisher_batches\": 1000}),   # 500 r√°pido; si hay tiempo: 1000\n",
    "\n",
    "    # Rehearsal estable en tus runs\n",
    "    (\"rehearsal\", {\"buffer_size\": 3000, \"replay_ratio\": 0.1}),  # o 0.2 si puedes\n",
    "\n",
    "    # Combo que ya sali√≥ en tu Pareto (accurate)\n",
    "    (\"rehearsal+ewc\", {\"buffer_size\": 3000, \"replay_ratio\": 0.2, \"lam\": 1e9, \"fisher_batches\": 1500}),\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    (\"sa-snn\", {\"attach_to\":\"f6\",\"k\":8,\"tau\":28,\"vt_scale\":1.33,\"p\":2_000_000,\n",
    "                \"th_min\":1.0,\"th_max\":2.0,\"flatten_spatial\":False,\n",
    "                \"assume_binary_spikes\":False,\"reset_counters_each_task\":False}),\n",
    "    (\"sa-snn\", {\"attach_to\":\"f6\",\"k\":8,\"tau\":32,\"vt_scale\":1.33,\"p\":5_000_000,\n",
    "                \"th_min\":1.0,\"th_max\":2.0,\"flatten_spatial\":False,\n",
    "                \"assume_binary_spikes\":False,\"reset_counters_each_task\":False}),\n",
    "    (\"sa-snn\", {\"attach_to\":\"f6\",\"k\":9,\"tau\":28,\"vt_scale\":1.33,\"p\":5_000_000,\n",
    "                \"th_min\":1.0,\"th_max\":2.0,\"flatten_spatial\":False,\n",
    "                \"assume_binary_spikes\":False,\"reset_counters_each_task\":False}),\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    # SA-SNN ‚Äúintermedio‚Äù (entre lo que ya lanzaste):\n",
    "    (\"sa-snn\", {\n",
    "        \"attach_to\":\"f6\",\n",
    "        \"k\": 8,\n",
    "        \"tau\": 30,          # ‚Üê intermedio (tu preset usa 30)\n",
    "        \"vt_scale\": 1.33,   # ‚Üê igual que las variantes que mejoraron olvido\n",
    "        \"p\": 5_000_000,\n",
    "        \"th_min\": 1.0, \"th_max\": 2.0,\n",
    "        \"flatten_spatial\": False,\n",
    "        \"assume_binary_spikes\": False,\n",
    "        \"reset_counters_each_task\": False\n",
    "    }),\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    (\"sca-snn\", {\n",
    "    \"attach_to\":\"f6\",\n",
    "    \"flatten_spatial\": False,\n",
    "    \"num_bins\": 50,\n",
    "    \"bin_lo\": -1.0, \"bin_hi\": 1.0,\n",
    "    \"anchor_batches\": 10,\n",
    "    \"max_per_bin\": 512,\n",
    "    \"beta\": 0.65,         # ‚Üì\n",
    "    \"bias\": 0.05,         # ‚Üì\n",
    "    \"soft_mask_temp\": 1.0,\n",
    "    \"habit_decay\": 0.995,\n",
    "    \"verbose\": True,\n",
    "    \"log_every\": 50\n",
    "}),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1fe026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6) Barrido de m√©todos / variantes para la memoria\n",
    "# =============================================================================\n",
    "from copy import deepcopy\n",
    "from src.runner import run_continual\n",
    "from src.utils import build_task_list_for, build_components_for\n",
    "\n",
    "# Define aqu√≠ SOLO la lista definitiva que quieres lanzar ahora\n",
    "EXPS = [\n",
    "    # baseline naive (sin mitigaci√≥n del olvido)\n",
    "    dict(\n",
    "         method=\"naive\",\n",
    "         params={},\n",
    "         tag=\"grid01\"\n",
    "    ),\n",
    "    # SCA-SNN: mejor tuyo + dos variaciones de beta\n",
    "    dict(\n",
    "        method=\"sca-snn\",\n",
    "        params={\n",
    "            \"attach_to\": \"f6\",\n",
    "            \"flatten_spatial\": False,\n",
    "            \"num_bins\": 50,\n",
    "            \"bin_lo\": -1.0, \"bin_hi\": 1.0,\n",
    "            \"anchor_batches\": 12,\n",
    "            \"max_per_bin\": 512,\n",
    "            \"beta\": 0.65,\n",
    "            \"bias\": 0.0,\n",
    "            \"soft_mask_temp\": 0.75,\n",
    "            \"habit_decay\": 0.995,\n",
    "            \"verbose\": True,\n",
    "            \"log_every\": 8192\n",
    "        },\n",
    "        tag=\"grid06_sca_b065\"\n",
    "    ),\n",
    "    dict(\n",
    "        method=\"sca-snn\",\n",
    "        params={\n",
    "            \"attach_to\": \"f6\",\n",
    "            \"flatten_spatial\": False,\n",
    "            \"num_bins\": 50,\n",
    "            \"bin_lo\": -1.0, \"bin_hi\": 1.0,\n",
    "            \"anchor_batches\": 12,\n",
    "            \"max_per_bin\": 512,\n",
    "            \"beta\": 0.60,\n",
    "            \"bias\": 0.0,\n",
    "            \"soft_mask_temp\": 0.75,\n",
    "            \"habit_decay\": 0.995,\n",
    "            \"verbose\": True,\n",
    "            \"log_every\": 8192\n",
    "        },\n",
    "        tag=\"grid07_sca_b060\"\n",
    "    ),\n",
    "    dict(\n",
    "        method=\"sca-snn\",\n",
    "        params={\n",
    "            \"attach_to\": \"f6\",\n",
    "            \"flatten_spatial\": False,\n",
    "            \"num_bins\": 50,\n",
    "            \"bin_lo\": -1.0, \"bin_hi\": 1.0,\n",
    "            \"anchor_batches\": 12,\n",
    "            \"max_per_bin\": 512,\n",
    "            \"beta\": 0.70,\n",
    "            \"bias\": 0.0,\n",
    "            \"soft_mask_temp\": 0.75,\n",
    "            \"habit_decay\": 0.995,\n",
    "            \"verbose\": True,\n",
    "            \"log_every\": 8192\n",
    "        },\n",
    "        tag=\"grid08_sca_b070\"\n",
    "    ),\n",
    "\n",
    "    # AS-SNN (mejores HPO que ya tienes)\n",
    "    dict(\n",
    "        method=\"as-snn\",\n",
    "        params={\"gamma_ratio\": 0.3, \"lambda_a\": 1.6, \"ema\": 0.9},\n",
    "        tag=\"grid09_as\"\n",
    "    ),\n",
    "\n",
    "    # SA-SNN (tus hiperpar√°metros buenos)\n",
    "    dict(\n",
    "        method=\"sa-snn\",\n",
    "        params={\"k\": 8, \"tau\": 28, \"thresh_lo\": 1.2, \"period\": 200000},\n",
    "        tag=\"grid10_sa\"\n",
    "    ),\n",
    "\n",
    "    # EWC (tu mejor lambda y fisher_batches)\n",
    "    dict(\n",
    "        method=\"ewc\",\n",
    "        params={\"lam\": 7e8, \"fisher_batches\": 1000},\n",
    "        tag=\"grid11_ewc\"\n",
    "    ),\n",
    "    # Rehearsal: barrido de replay_ratio (buffer_size fijo=3000)\n",
    "    dict(\n",
    "        method=\"rehearsal\",\n",
    "        params={\"buffer_size\": 3000, \"replay_ratio\": 0.10},\n",
    "        tag=\"grid02_rr10\"\n",
    "    ),\n",
    "    dict(\n",
    "        method=\"rehearsal\",\n",
    "        params={\"buffer_size\": 3000, \"replay_ratio\": 0.15},\n",
    "        tag=\"grid03_rr15\"\n",
    "    ),\n",
    "    dict(\n",
    "        method=\"rehearsal\",\n",
    "        params={\"buffer_size\": 3000, \"replay_ratio\": 0.20},\n",
    "        tag=\"grid04_rr20\"\n",
    "    ),\n",
    "    dict(\n",
    "        method=\"rehearsal\",\n",
    "        params={\"buffer_size\": 3000, \"replay_ratio\": 0.25},\n",
    "        tag=\"grid05_rr25\"\n",
    "    ),\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "runs_out = []\n",
    "\n",
    "for exp in EXPS:\n",
    "    method_name   = exp[\"method\"]\n",
    "    method_params = exp[\"params\"]\n",
    "\n",
    "    # clonar preset base para no pisar CFG\n",
    "    cfg_i = deepcopy(CFG)\n",
    "    cfg_i[\"continual\"][\"method\"] = method_name\n",
    "    cfg_i[\"continual\"][\"params\"] = method_params\n",
    "\n",
    "    # metadato opcional para que runner etiquete bien la carpeta\n",
    "    # (runner ya mira cfg[\"naming\"][\"tag\"] si existe)\n",
    "    cfg_i.setdefault(\"naming\", {})\n",
    "    cfg_i[\"naming\"][\"tag\"] = exp[\"tag\"]\n",
    "\n",
    "    # peque√±o ajuste de robustez para rehearsal: baja workers persistentes\n",
    "    if \"rehearsal\" in method_name:\n",
    "        cfg_i[\"data\"][\"persistent_workers\"] = False\n",
    "\n",
    "    # construir componentes coherentes con ESTA cfg_i\n",
    "    tfm_i, make_loader_fn_i, make_model_fn_i = build_components_for(cfg_i, ROOT)\n",
    "    task_list_i, tasks_file_i = build_task_list_for(cfg_i, ROOT)\n",
    "\n",
    "    print(\n",
    "        f\"\\n=== RUN: preset={PRESET} | method={method_name} \"\n",
    "        f\"| seed={cfg_i['data']['seed']} | enc={cfg_i['data']['encoder']} \"\n",
    "        f\"| kwargs={method_params} | tag={exp['tag']} ===\"\n",
    "    )\n",
    "\n",
    "    out_dir, _ = run_continual(\n",
    "        task_list=task_list_i,\n",
    "        make_loader_fn=make_loader_fn_i,\n",
    "        make_model_fn=make_model_fn_i,\n",
    "        tfm=tfm_i,\n",
    "        cfg=cfg_i,\n",
    "        preset_name=PRESET,\n",
    "        out_root=OUT,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    runs_out.append(out_dir)\n",
    "\n",
    "print(\"\\nHecho:\", [str(p) for p in runs_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6) Barrido \"mejor por m√©todo\" (accurate, T=30, B=160)\n",
    "# =============================================================================\n",
    "from copy import deepcopy\n",
    "from src.runner import run_continual\n",
    "from src.utils import load_preset, build_task_list_for, build_components_for\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Carga preset ---\n",
    "ROOT = Path.cwd().parents[0] if (Path.cwd().name == \"notebooks\") else Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "PRESET = \"accurate\"\n",
    "CFG = load_preset(ROOT / \"configs\" / \"presets.yaml\", PRESET)\n",
    "\n",
    "# === Lista de mejores por m√©todo (seg√∫n tu tabla) ===\n",
    "# Nombres y par√°metros alineados con las implementaciones actuales\n",
    "EXPS = [\n",
    "  # Rehearsal ganador: buffer=3000, rr=0.10\n",
    "  # dict(\n",
    "  #   method=\"rehearsal\",\n",
    "  #   params={\n",
    "  #     \"buffer_size\": 3000,\n",
    "  #     \"replay_ratio\": 0.10\n",
    "  #   },\n",
    "  #   tag=\"best_reh_buf3000_rr10\"\n",
    "  # ),\n",
    "\n",
    "  # SA-SNN ganador: k=8, tau=28, p=2e6, th 1‚Äì2\n",
    "  # dict(\n",
    "  #   method=\"sa-snn\",\n",
    "  #   params={\n",
    "  #     \"attach_to\": \"f6\",\n",
    "  #     \"k\": 8,\n",
    "  #     \"tau\": 28.0,\n",
    "  #     \"th_min\": 1.0,\n",
    "  #     \"th_max\": 2.0,\n",
    "  #     \"p\": 2000000,\n",
    "  #     \"vt_scale\": 1.0,\n",
    "  #     \"flatten_spatial\": False,\n",
    "  #     \"assume_binary_spikes\": False,\n",
    "  #     \"reset_counters_each_task\": False\n",
    "  #   },\n",
    "  #   tag=\"best_sa_k8_tau28_p2m\"\n",
    "  # ),\n",
    "\n",
    "  # SCA-SNN ganador: bins50, beta=0.60, bias=0.05, temp=0.5, ab=16, flat=0\n",
    "  # dict(\n",
    "  #     method=\"sca-snn\",\n",
    "  #     params={\n",
    "  #         \"attach_to\": \"f6\",\n",
    "  #         \"flatten_spatial\": False,\n",
    "  #         \"num_bins\": 50,\n",
    "  #         \"anchor_batches\": 16,\n",
    "  #         \"beta\": 0.60,\n",
    "  #         \"bias\": 0.05,\n",
    "  #         \"soft_mask_temp\": 0.50,\n",
    "  #         \"verbose\": False,\n",
    "  #         \"log_every\": 65536\n",
    "  #     },\n",
    "  #     tag=\"best_sca_b060_bias005_t050_ab16\"\n",
    "  # ),\n",
    "\n",
    "  # AS-SNN ganador: gamma_ratio=0.3, lambda‚âà1.59168\n",
    "  dict(\n",
    "    method=\"as-snn\",\n",
    "    params={\n",
    "      \"gamma_ratio\": 0.3,\n",
    "      \"lambda_a\": 1.59168,\n",
    "      \"ema\": 0.9\n",
    "    },\n",
    "    tag=\"best_as_gr03_lam1p59168\"\n",
    "  ),\n",
    "\n",
    "  # EWC ganador: lam=7e8, fisher=1000\n",
    "  dict(\n",
    "    method=\"ewc\",\n",
    "    params={\n",
    "      \"lam\": 7e8,\n",
    "      \"fisher_batches\": 1000\n",
    "    },\n",
    "    tag=\"best_ewc_lam7e8_f1000\"\n",
    "  ),\n",
    "\n",
    "  # Baseline naive (sin mitigaci√≥n)\n",
    "  dict(\n",
    "    method=\"naive\",\n",
    "    params={},\n",
    "    tag=\"baseline_naive\"\n",
    "  ),\n",
    "]\n",
    "\n",
    "# --- Ejecutar ---\n",
    "OUT = ROOT / \"outputs\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "runs_out = []\n",
    "for exp in EXPS:\n",
    "    method_name   = exp[\"method\"]\n",
    "    method_params = exp[\"params\"]\n",
    "\n",
    "    cfg_i = deepcopy(CFG)\n",
    "    cfg_i[\"continual\"][\"method\"]  = method_name\n",
    "    cfg_i[\"continual\"][\"params\"]  = method_params\n",
    "    cfg_i.setdefault(\"naming\", {})\n",
    "    cfg_i[\"naming\"][\"tag\"] = exp[\"tag\"]\n",
    "\n",
    "    # Seguridad: rehearsal a veces se lleva mal con workers persistentes\n",
    "    if \"rehearsal\" in method_name:\n",
    "        cfg_i[\"data\"][\"persistent_workers\"] = False\n",
    "\n",
    "    tfm_i, make_loader_fn_i, make_model_fn_i = build_components_for(cfg_i, ROOT)\n",
    "    task_list_i, tasks_file_i = build_task_list_for(cfg_i, ROOT)\n",
    "\n",
    "    print(f\"\\n=== RUN accurate: method={method_name} | seed={cfg_i['data']['seed']} \"\n",
    "          f\"| enc={cfg_i['data']['encoder']} | kwargs={method_params} | tag={exp['tag']} ===\")\n",
    "\n",
    "    out_dir, _ = run_continual(\n",
    "        task_list=task_list_i,\n",
    "        make_loader_fn=make_loader_fn_i,\n",
    "        make_model_fn=make_model_fn_i,\n",
    "        tfm=tfm_i,\n",
    "        cfg=cfg_i,\n",
    "        preset_name=PRESET,\n",
    "        out_root=OUT,\n",
    "        verbose=True,\n",
    "    )\n",
    "    runs_out.append(out_dir)\n",
    "\n",
    "print(\"\\nHecho:\", [str(p) for p in runs_out])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0650365",
   "metadata": {},
   "source": [
    "<a id=\"sec-07\"></a>\n",
    "## 7) Resumen completo: inventario ‚Üí parseo ‚Üí agregados ‚Üí tabla\n",
    "\n",
    "**Objetivo**  \n",
    "Crear un **resumen reproducible** de todos los *runs*:\n",
    "\n",
    "- **Inventario** de carpetas `outputs/continual_*`.\n",
    "- **Parseo** de nombres para extraer `preset`, `m√©todo`, `encoder`, `seed`, `modelo`, y par√°metros relevantes.\n",
    "- C√°lculo de **olvido** (absoluto y relativo) y **agregados** por grupo (media, œÉ, n).\n",
    "- Export a `outputs/summary/continual_summary_agg.csv` y **tabla formateada** para la memoria.\n",
    "\n",
    "> Si no se detectan *runs*, verifica que exista `continual_results.json` dentro de cada carpeta.  \n",
    "\n",
    "[‚Üë Volver al √≠ndice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7) Resumen y gr√°ficas\n",
    "# =============================================================================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.results_io import build_results_table\n",
    "from src.plots import plot_across_runs\n",
    "\n",
    "summary_dir = OUT / \"summary\"\n",
    "summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def canonical_method(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"unknown\"\n",
    "    t = s.lower()\n",
    "    if (\"rehearsal\" in t) and (\"+ewc\" in t or \"_ewc\" in t):\n",
    "        return \"rehearsal+ewc\"\n",
    "    if \"sca-snn\" in t:\n",
    "        return \"sca-snn\"\n",
    "    if re.search(r\"\\bsa[-_]snn\\b\", t):\n",
    "        return \"sa-snn\"\n",
    "    if re.search(r\"\\bas[-_]snn\\b\", t):\n",
    "        return \"as-snn\"\n",
    "    if \"colanet\" in t:\n",
    "        return \"colanet\"\n",
    "    if re.search(r\"\\bewc\\b\", t) or \"ewc_lam\" in t:\n",
    "        return \"ewc\"\n",
    "    if \"rehearsal\" in t:\n",
    "        return \"rehearsal\"\n",
    "    if \"naive\" in t or \"finetune\" in t or \"fine-tune\" in t:\n",
    "        return \"naive\"\n",
    "    return t.split(\"_\")[0]\n",
    "\n",
    "# --- 7.1 Tabla consolidada ---\n",
    "df = build_results_table(OUT)\n",
    "df[\"method_base\"] = df[\"method\"].astype(str).apply(canonical_method)\n",
    "display(df)\n",
    "df.to_csv(summary_dir / \"results_table.csv\", index=False)\n",
    "print(f\"[OK] Tabla guardada en {summary_dir/'results_table.csv'}\")\n",
    "\n",
    "# --- 7.2 Gr√°ficas comparativas (final MAE, olvido, emisiones, trade-off) ---\n",
    "plots_dir = plot_across_runs(df, summary_dir / \"plots\")\n",
    "print(\"[OK] Gr√°ficas comparativas en:\", plots_dir)\n",
    "\n",
    "# --- 7.3 Curvas de loss por tarea (para la memoria) ---\n",
    "def plot_losses_for_run(run_dir: Path, outdir: Path):\n",
    "    \"\"\"Busca en run_dir/task_*/manifest.json y dibuja curvas de train/val loss por tarea.\"\"\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    task_dirs = sorted(run_dir.glob(\"task_*\"))\n",
    "    if not task_dirs:\n",
    "        print(f\"[WARN] No hay carpetas task_* en {run_dir}\")\n",
    "        return\n",
    "\n",
    "    for td in task_dirs:\n",
    "        # manifest.json o metrics.json\n",
    "        man = None\n",
    "        for cand in (\"manifest.json\", \"metrics.json\"):\n",
    "            p = td / cand\n",
    "            if p.exists():\n",
    "                with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                    man = json.load(f)\n",
    "                break\n",
    "        if man is None:\n",
    "            print(f\"[WARN] Sin manifest/metrics en {td.name}\")\n",
    "            continue\n",
    "\n",
    "        hist = (man.get(\"history\") or {})\n",
    "        tr = hist.get(\"train_loss\") or []\n",
    "        va = hist.get(\"val_loss\") or []\n",
    "\n",
    "        if not tr and not va:\n",
    "            print(f\"[WARN] {td.name}: sin 'train_loss'/'val_loss' en history.\")\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(7,4))\n",
    "        if tr:\n",
    "            plt.plot(range(1, len(tr)+1), tr, label=\"train_loss\")\n",
    "        if va:\n",
    "            plt.plot(range(1, len(va)+1), va, label=\"val_loss\")\n",
    "        plt.title(f\"{run_dir.name} ‚Äî {td.name}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MSE loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(outdir / f\"{run_dir.name}__{td.name}_loss.png\", dpi=160)\n",
    "        plt.savefig(outdir / f\"{run_dir.name}__{td.name}_loss.svg\")\n",
    "        plt.show()\n",
    "\n",
    "# Elige el/los runs para curvas de loss (aqu√≠, el √∫ltimo por fecha):\n",
    "runs = sorted(OUT.glob(\"continual_*\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if runs:\n",
    "    loss_plots_dir = summary_dir / \"loss_curves\"\n",
    "    print(\"Generando curvas de loss para:\", runs[0].name)\n",
    "    plot_losses_for_run(runs[0], loss_plots_dir)\n",
    "    print(\"[OK] Curvas de loss en:\", loss_plots_dir)\n",
    "else:\n",
    "    print(\"[INFO] No hay runs en outputs/ todav√≠a.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Selecci√≥n autom√°tica de runs representativos ===\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def norm01(x):\n",
    "    x = x.astype(float)\n",
    "    lo, hi = np.nanmin(x), np.nanmax(x)\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        return np.ones_like(x) * 0.5\n",
    "    return (x - lo) / (hi - lo)\n",
    "\n",
    "# Usa la tabla ya construida en la celda anterior\n",
    "d = df.copy()\n",
    "\n",
    "# Normaliza m√©todo base (por si esta celda se ejecuta sola)\n",
    "if \"method_base\" not in d.columns:\n",
    "    d[\"method_base\"] = d[\"method\"].astype(str).apply(canonical_method)\n",
    "\n",
    "# Filtra preset = accurate\n",
    "d = d[d[\"preset\"] == \"accurate\"].copy()\n",
    "assert not d.empty, \"No hay runs con preset='accurate'.\"\n",
    "\n",
    "# --- Pol√≠tica de tarea principal: √öLTIMA *_final_mae ---\n",
    "# (si prefieres media de tareas, cambia la selecci√≥n abajo)\n",
    "task_cols = [c for c in d.columns if c.endswith(\"_final_mae\")]\n",
    "assert len(task_cols) > 0, \"No encuentro columnas *_final_mae en la tabla.\"\n",
    "\n",
    "def sort_key(col):\n",
    "    name = col.replace(\"_final_mae\", \"\")\n",
    "    m = re.search(r\"(\\d+)$\", name)\n",
    "    base = re.sub(r\"\\d+$\", \"\", name)\n",
    "    idx = int(m.group(1)) if m else 0\n",
    "    return (base, idx)\n",
    "\n",
    "task_cols_sorted = sorted(task_cols, key=sort_key)\n",
    "primary_mae  = task_cols_sorted[-1]    # p.ej. 'circuito2_final_mae'\n",
    "primary_task = primary_mae.replace(\"_final_mae\", \"\")\n",
    "print(\"[INFO] Tareas detectadas:\", task_cols_sorted)\n",
    "print(\"[INFO] Usando como MAE principal:\", primary_mae)\n",
    "\n",
    "# Mant√©n columnas necesarias y normaliza\n",
    "keep = [\"run_dir\",\"preset\",\"method\",\"method_base\",\"encoder\",\"model\",\"seed\",\n",
    "        \"elapsed_sec\",\"emissions_kg\",\"avg_forget_rel\", primary_mae]\n",
    "d = d[keep].copy()\n",
    "for c in [\"emissions_kg\",\"avg_forget_rel\", primary_mae]:\n",
    "    d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "\n",
    "# Relleno conservador\n",
    "if d[\"emissions_kg\"].isna().all():\n",
    "    d[\"emissions_kg\"] = 0.0\n",
    "else:\n",
    "    d[\"emissions_kg\"] = d[\"emissions_kg\"].fillna(d[\"emissions_kg\"].median())\n",
    "d[\"avg_forget_rel\"] = d[\"avg_forget_rel\"].fillna(d[\"avg_forget_rel\"].max())\n",
    "\n",
    "# --- Frente de Pareto (minimizar: MAE, olvido, emisiones) ---\n",
    "M = d[[primary_mae,\"avg_forget_rel\",\"emissions_kg\"]].values\n",
    "is_dominated = np.zeros(len(d), dtype=bool)\n",
    "for i in range(len(d)):\n",
    "    ai = np.nan_to_num(M[i], nan=np.inf)\n",
    "    for j in range(len(d)):\n",
    "        if i == j: continue\n",
    "        aj = np.nan_to_num(M[j], nan=np.inf)\n",
    "        if np.all(aj <= ai) and np.any(aj < ai):\n",
    "            is_dominated[i] = True\n",
    "            break\n",
    "\n",
    "pareto = d.loc[~is_dominated].sort_values([primary_mae, \"avg_forget_rel\", \"emissions_kg\"])\n",
    "print(f\"=== Frente de Pareto (no dominados) ‚Äî MAE final ({primary_task}) ===\")\n",
    "display(pareto)\n",
    "\n",
    "# --- Ranking por puntuaci√≥n compuesta (opcional) ---\n",
    "w_mae, w_forget, w_emiss = 0.5, 0.4, 0.1\n",
    "d[\"_mae_n\"]    = norm01(d[primary_mae].values)\n",
    "d[\"_forget_n\"] = norm01(d[\"avg_forget_rel\"].values)\n",
    "d[\"_emiss_n\"]  = norm01(d[\"emissions_kg\"].values)\n",
    "d[\"score\"]     = w_mae*d[\"_mae_n\"] + w_forget*d[\"_forget_n\"] + w_emiss*d[\"_emiss_n\"]\n",
    "\n",
    "topN = d.sort_values(\"score\").head(6)\n",
    "print(\"=== Top-6 por score compuesto (‚Üì mejor) ===\")\n",
    "display(topN[[\"run_dir\",\"preset\",\"method\",\"method_base\",\"seed\",primary_mae,\"avg_forget_rel\",\"emissions_kg\",\"score\"]])\n",
    "\n",
    "# ---- Si prefieres usar la MEDIA de tareas en lugar de la √∫ltima ----\n",
    "# mean_mae = d[task_cols].mean(axis=1, skipna=True)\n",
    "# d_alt = d.copy()\n",
    "# d_alt[\"mean_mae_final\"] = mean_mae\n",
    "# ... y sustituyes 'primary_mae' por 'mean_mae_final' en el Pareto/ranking.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
