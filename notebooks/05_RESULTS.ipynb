{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2f9063",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# 04 · Resumen y visualización de resultados\n",
    "\n",
    "**Qué hace este notebook**  \n",
    "Inspecciona los **resultados de entrenamientos continual** y presenta:\n",
    "- Un **resumen por experimento** y otro **por tarea** (historiales).\n",
    "- Lectura de **telemetría de emisiones** de *CodeCarbon* (`emissions.csv`) y del log ligero `telemetry.jsonl`.\n",
    "- Un **CSV enriquecido** con rendimiento + emisiones para análisis posterior.\n",
    "- Gráficos rápidos: MAE, olvido relativo y emisiones (kg CO₂e), además de un *scatter* rendimiento vs. emisiones.\n",
    "\n",
    "**Fuentes de datos**\n",
    "- `outputs/continual_*/*/manifest.json` (o `metrics.json`) → historiales por tarea.\n",
    "- `outputs/continual_*/continual_results.json` → métricas a nivel de experimento.\n",
    "- `outputs/continual_*/emissions.csv` (si *CodeCarbon* estaba activo en el run).\n",
    "- `outputs/continual_*/telemetry.jsonl` (eventos del runner).\n",
    "\n",
    "## ✅ Prerrequisitos\n",
    "- Haber ejecutado **03_TRAIN_CONTINUAL.ipynb** (para poblar `outputs/continual_*`).\n",
    "- (Opcional) Tener *CodeCarbon* instalado/activado para disponer de `emissions.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "## Índice\n",
    "\n",
    "- [1) Imports y rutas base](#sec-01)  \n",
    "- [2) Utilidades de parseo y lectura robusta](#sec-02)  \n",
    "- [3) Resumen “continual” (por run)](#sec-03)  \n",
    "- [4) Resumen de entrenamiento por tarea (manifest/metrics)](#sec-04)  \n",
    "- [5) Gráficos rápidos](#sec-05)  \n",
    "- [6) Inspección de un experimento concreto](#sec-06)\n",
    "- [7) Inspección detallada: CodeCarbon + Telemetry (último run)](#sec-07)  \n",
    "- [8) Agregados por preset/método con emisiones](#sec-08)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5d88e",
   "metadata": {},
   "source": [
    "<a id=\"sec-01\"></a>\n",
    "## 1) Imports y rutas base\n",
    "\n",
    "**Objetivo**  \n",
    "Configurar el entorno mínimo de lectura/visualización:\n",
    "- Detectar la **raíz del repo** (`ROOT`) tanto si ejecutas desde `notebooks/` como desde la raíz.  \n",
    "- Definir `OUT = ROOT / \"outputs\"` como carpeta base de resultados.\n",
    "\n",
    "> No modifica ni reescribe archivos de los runs: **solo lee** y visualiza.  \n",
    "\n",
    "[↑ Volver al índice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca50e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 0 — Config (versión dinámica)\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, math, re, os, json\n",
    "from datetime import datetime\n",
    "\n",
    "# -------- Raíz de outputs del proyecto --------\n",
    "OUT = Path(os.environ.get(\"OUT_DIR\", \"/home/cesar/proyectos/TFM_SNN/outputs\")).resolve()\n",
    "\n",
    "# -------- Filtros \"paper set\" (por defecto) --------\n",
    "# Puedes sobreescribirlos con env vars: PRESET_KEEP, ENCODER_KEEP, SEED_KEEP, METHODS_KEEP\n",
    "def _parse_set(env_name, default):\n",
    "    val = os.environ.get(env_name, None)\n",
    "    if val is None:\n",
    "        return default\n",
    "    if val.strip().lower() in {\"none\", \"\", \"all\"}:\n",
    "        return None\n",
    "    return set([v.strip() for v in val.split(\",\") if v.strip()])\n",
    "\n",
    "PRESET_KEEP   = _parse_set(\"PRESET_KEEP\",   {\"std\"})   # None → no filtra por preset\n",
    "ENCODER_KEEP  = _parse_set(\"ENCODER_KEEP\",  {\"rate\"})   # None → no filtra\n",
    "SEED_KEEP     = _parse_set(\"SEED_KEEP\",     None)       # p.ej. \"42,7\"\n",
    "METHODS_KEEP  = _parse_set(\"METHODS_KEEP\",  None)\n",
    "\n",
    "ONLY_NEW_RUNNER = os.environ.get(\"ONLY_NEW_RUNNER\", \"1\") not in {\"0\",\"false\",\"False\"}\n",
    "\n",
    "# Ventana temporal: SINCE=\"2025-11-11\" o SINCE=\"3d\" (últimos 3 días). Si no se da, no filtra por fecha.\n",
    "# SINCE = os.environ.get(\"SINCE\", \"\").strip()\n",
    "SINCE=\"2025-11-11\"\n",
    "\n",
    "def _compute_mtime_from(since_str: str):\n",
    "    if not since_str:\n",
    "        return None\n",
    "    try:\n",
    "        if since_str.lower().endswith(\"d\"):\n",
    "            days = int(since_str[:-1])\n",
    "            return pd.Timestamp.now().normalize() - pd.Timedelta(days=days)\n",
    "        return pd.Timestamp(since_str)\n",
    "    except Exception:\n",
    "        return None\n",
    "MTIME_FROM = _compute_mtime_from(SINCE) or None\n",
    "\n",
    "# -------- Comparabilidad \"dura\" --------\n",
    "STRICT_CFG = os.environ.get(\"STRICT_CFG\", \"1\") not in {\"0\",\"false\",\"False\"}\n",
    "\n",
    "# -------- Métrica compuesta --------\n",
    "ALPHA_COMPOSITE = float(os.environ.get(\"ALPHA_COMPOSITE\", \"0.5\"))\n",
    "\n",
    "# -------- Opciones de informe / narrativa --------\n",
    "# IGNORE_NAIVE_IN_REPORTS = os.environ.get(\"IGNORE_NAIVE\", \"0\") in {\"1\",\"true\",\"True\"}\n",
    "GROUP_BY_FULL_METHOD    = os.environ.get(\"GROUP_BY_FULL_METHOD\", \"0\") in {\"1\",\"true\",\"True\"}\n",
    "# RELATIVE_BASELINE       = os.environ.get(\"RELATIVE_BASELINE\", \"ewc\")  # \"naive\",\"ewc\",\"auto\", o \"\" para desactivar\n",
    "# BASELINE_MATCH_STRICT   = os.environ.get(\"BASELINE_MATCH_STRICT\", \"1\") not in {\"0\",\"false\",\"False\"}\n",
    "\n",
    "IGNORE_NAIVE_IN_REPORTS = True       # excluye naive de tablas/plots de informe\n",
    "RELATIVE_BASELINE = \"ewc\"            # baseline para comparativas\n",
    "BASELINE_MATCH_STRICT = True         # emparejamiento estricto de runs comparables\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "# -------- Carpeta SUMMARY dinámica --------\n",
    "def _token(val):\n",
    "    if val is None: return \"all\"\n",
    "    if isinstance(val, (set, list, tuple)):\n",
    "        return \"-\".join(sorted(map(str, val)))\n",
    "    return str(val)\n",
    "\n",
    "now = pd.Timestamp.now(tz=\"Europe/Madrid\")\n",
    "auto_label = f\"paperset_{_token(PRESET_KEEP)}_{_token(ENCODER_KEEP)}_{now.strftime('%Y-%m-%d_%H%M')}\"\n",
    "SUMMARY_LABEL = os.environ.get(\"SUMMARY_LABEL\", \"\").strip() or auto_label\n",
    "\n",
    "SUMMARY = (OUT / \"summary\" / SUMMARY_LABEL).resolve()\n",
    "SUMMARY.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[SUMMARY] {SUMMARY}\")\n",
    "\n",
    "# Symlink \"latest\" → esta ejecución (si el FS lo permite)\n",
    "LATEST = OUT / \"summary\" / \"latest\"\n",
    "try:\n",
    "    if LATEST.exists() or LATEST.is_symlink():\n",
    "        LATEST.unlink()\n",
    "    LATEST.symlink_to(SUMMARY, target_is_directory=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Guardar manifest de filtros/ajustes para trazabilidad\n",
    "manifest = {\n",
    "    \"summary_label\": SUMMARY_LABEL,\n",
    "    \"created_at\": now.isoformat(),\n",
    "    \"OUT\": str(OUT),\n",
    "    \"filters\": {\n",
    "        \"PRESET_KEEP\": sorted(list(PRESET_KEEP)) if PRESET_KEEP else None,\n",
    "        \"ENCODER_KEEP\": sorted(list(ENCODER_KEEP)) if ENCODER_KEEP else None,\n",
    "        \"SEED_KEEP\": sorted(list(SEED_KEEP)) if SEED_KEEP else None,\n",
    "        \"METHODS_KEEP\": sorted(list(METHODS_KEEP)) if METHODS_KEEP else None,\n",
    "        \"ONLY_NEW_RUNNER\": ONLY_NEW_RUNNER,\n",
    "        \"MTIME_FROM\": str(MTIME_FROM) if MTIME_FROM is not None else None,\n",
    "    },\n",
    "    \"comparability\": {\"STRICT_CFG\": STRICT_CFG},\n",
    "    \"composite\": {\"ALPHA_COMPOSITE\": ALPHA_COMPOSITE},\n",
    "    \"report_opts\": {\n",
    "        \"IGNORE_NAIVE_IN_REPORTS\": IGNORE_NAIVE_IN_REPORTS,\n",
    "        \"GROUP_BY_FULL_METHOD\": GROUP_BY_FULL_METHOD,\n",
    "        \"RELATIVE_BASELINE\": RELATIVE_BASELINE,\n",
    "        \"BASELINE_MATCH_STRICT\": BASELINE_MATCH_STRICT,\n",
    "    },\n",
    "}\n",
    "(SUMMARY / \"summary_manifest.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# --- Manifest \"último\" para reutilizar rutas en otras celdas/notebooks\n",
    "LAST_PATH = OUT / \"summary\" / \"_last.json\"\n",
    "\n",
    "def _last_update(**kwargs):\n",
    "    # Carga el estado actual (si existe)\n",
    "    try:\n",
    "        cur = json.loads(LAST_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        cur = {}\n",
    "\n",
    "    # Campos fijos útiles\n",
    "    cur.update({\n",
    "        \"summary_label\": SUMMARY_LABEL,\n",
    "        \"summary_dir\": str(SUMMARY.resolve()),\n",
    "        \"created_at\": now.isoformat(),\n",
    "    })\n",
    "\n",
    "    # Normaliza y añade las nuevas rutas/etiquetas\n",
    "    for k, v in list(kwargs.items()):\n",
    "        if isinstance(v, (str, Path)):\n",
    "            kwargs[k] = str(Path(v).resolve())\n",
    "    cur.update(kwargs)\n",
    "\n",
    "    LAST_PATH.write_text(json.dumps(cur, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"[LAST] actualizado → {LAST_PATH}\")\n",
    "\n",
    "# Crea/actualiza _last.json al inicio con la info básica de esta ejecución\n",
    "_last_update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53940c08",
   "metadata": {},
   "source": [
    "<a id=\"sec-02\"></a>\n",
    "## 2) Utilidades de parseo y lectura robusta\n",
    "\n",
    "**Objetivo**  \n",
    "Apoyarse en utilidades **canónicas** del proyecto (`src.utils_exp`) para:\n",
    "- `safe_read_json`: lectura tolerante a errores (devuelve `{}` si no existe o está corrupto).  \n",
    "- `parse_exp_name`: extraer metadatos desde el nombre de la carpeta `continual_*` (preset, método, encoder, seed, modelo…).  \n",
    "- `build_runs_df` y `aggregate_and_show`: construir un **DataFrame** consolidado por experimento y, opcionalmente, generar **agregados** y guardarlos.\n",
    "\n",
    "Se añaden *helpers* locales para:\n",
    "- Encontrar el primer JSON disponible por **tarea** (`manifest.json` o `metrics.json`).  \n",
    "- Leer **CodeCarbon** (`emissions.csv`) de forma **tolerante a versiones** (columnas pueden variar según versión).  \n",
    "- Leer el último evento de `telemetry.jsonl` para capturar `elapsed_sec` y `emissions_kg` si el runner los registró.\n",
    "\n",
    "[↑ Volver al índice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1 — Utils\n",
    "\n",
    "def _safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        if x is None: return default\n",
    "        if isinstance(x, (int, float)): return float(x)\n",
    "        s = str(x).strip()\n",
    "        if s.lower() in {\"nan\",\"none\",\"null\",\"\"}: return default\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _read_json(p: Path):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _read_csv_df(p: Path):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _abs_run_dir(rel: str|Path) -> Path:\n",
    "    rel = str(rel)\n",
    "    return (OUT / rel) if not rel.startswith(str(OUT)) else Path(rel)\n",
    "\n",
    "def run_mtime(rel: str|Path) -> float:\n",
    "    rd = _abs_run_dir(rel)\n",
    "    mt = 0.0\n",
    "    for root, _, files in os.walk(rd):\n",
    "        for f in files:\n",
    "            try:\n",
    "                mt = max(mt, (Path(root)/f).stat().st_mtime)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return mt\n",
    "\n",
    "def canonical_method(m: str) -> str:\n",
    "    if not m: return \"none\"\n",
    "    m = m.lower()\n",
    "    if m.startswith(\"ewc\"):        return \"ewc\"\n",
    "    if m.startswith(\"as-snn\"):     return \"as-snn\"\n",
    "    if m.startswith(\"sa-snn\"):     return \"sa-snn\"\n",
    "    if m.startswith(\"sca-snn\"):    return \"sca-snn\"\n",
    "    if m.startswith(\"rehearsal\"):  return \"rehearsal\"\n",
    "    if m.startswith(\"naive\"):      return \"naive\"\n",
    "    return m\n",
    "\n",
    "def _read_forgetting(run_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Lee primero forgetting_summary.json (preferente).\n",
    "    Si no existe, lee forgetting.json (anidado por tareas) y calcula la media.\n",
    "    Devuelve claves normalizadas: circuito1_forget_abs/rel, circuito2_forget_abs/rel, avg_forget_rel.\n",
    "    \"\"\"\n",
    "    def _nanmean2(a, b):\n",
    "        vals = [x for x in [a, b] if x is not None and not math.isnan(_safe_float(x))]\n",
    "        return float(np.mean([_safe_float(x) for x in vals])) if vals else np.nan\n",
    "\n",
    "    # 1) summary (preferido)\n",
    "    summ = _read_json(run_dir / \"forgetting_summary.json\")\n",
    "    if isinstance(summ, dict):\n",
    "        c1_abs = _safe_float(summ.get(\"circuito1_forget_abs\", summ.get(\"task_1_forget_abs\")))\n",
    "        c1_rel = _safe_float(summ.get(\"circuito1_forget_rel\", summ.get(\"task_1_forget_rel\")))\n",
    "        c2_abs = _safe_float(summ.get(\"circuito2_forget_abs\", summ.get(\"task_2_forget_abs\")))\n",
    "        c2_rel = _safe_float(summ.get(\"circuito2_forget_rel\", summ.get(\"task_2_forget_rel\")))\n",
    "        avg_rel = _safe_float(summ.get(\"avg_forget_rel\", summ.get(\"avg_forgetting_rel\")))\n",
    "        if math.isnan(avg_rel):\n",
    "            avg_rel = _nanmean2(c1_rel, c2_rel)\n",
    "        return {\n",
    "            \"circuito1_forget_abs\": c1_abs,\n",
    "            \"circuito1_forget_rel\": c1_rel,\n",
    "            \"circuito2_forget_abs\": c2_abs,\n",
    "            \"circuito2_forget_rel\": c2_rel,\n",
    "            \"avg_forget_rel\":       avg_rel,\n",
    "        }\n",
    "\n",
    "    # 2) forgetting.json (anidado por tareas) o plano\n",
    "    js = _read_json(run_dir / \"forgetting.json\") or {}\n",
    "    if not isinstance(js, dict):\n",
    "        return {}\n",
    "\n",
    "    def pick(d, *keys):\n",
    "        for k in keys:\n",
    "            if k in d: return d[k]\n",
    "        return None\n",
    "\n",
    "    c1_abs = _safe_float(pick(js, \"circuito1_forget_abs\",\"task_1_circuito1_forget_abs\",\"c1_forget_abs\",\"task_1_forget_abs\"))\n",
    "    c1_rel = _safe_float(pick(js, \"circuito1_forget_rel\",\"task_1_circuito1_forget_rel\",\"c1_forget_rel\",\"task_1_forget_rel\"))\n",
    "    c2_abs = _safe_float(pick(js, \"circuito2_forget_abs\",\"task_2_circuito2_forget_abs\",\"c2_forget_abs\",\"task_2_forget_abs\"))\n",
    "    c2_rel = _safe_float(pick(js, \"circuito2_forget_rel\",\"task_2_circuito2_forget_rel\",\"c2_forget_rel\",\"task_2_forget_rel\"))\n",
    "    avg_rel = _safe_float(pick(js, \"avg_forget_rel\",\"avg_forgetting_rel\",\"mean_forget_rel\",\"forget_rel_avg\"))\n",
    "\n",
    "    # Si sigue faltando, leer anidado por tarea\n",
    "    if (\"circuito1\" in js) and isinstance(js[\"circuito1\"], dict):\n",
    "        if math.isnan(c1_abs): c1_abs = _safe_float(js[\"circuito1\"].get(\"forget_abs\"))\n",
    "        if math.isnan(c1_rel): c1_rel = _safe_float(js[\"circuito1\"].get(\"forget_rel\"))\n",
    "    if (\"circuito2\" in js) and isinstance(js[\"circuito2\"], dict):\n",
    "        if math.isnan(c2_abs): c2_abs = _safe_float(js[\"circuito2\"].get(\"forget_abs\"))\n",
    "        if math.isnan(c2_rel): c2_rel = _safe_float(js[\"circuito2\"].get(\"forget_rel\"))\n",
    "\n",
    "    if math.isnan(avg_rel):\n",
    "        avg_rel = _nanmean2(c1_rel, c2_rel)\n",
    "\n",
    "    return {\n",
    "        \"circuito1_forget_abs\": c1_abs,\n",
    "        \"circuito1_forget_rel\": c1_rel,\n",
    "        \"circuito2_forget_abs\": c2_abs,\n",
    "        \"circuito2_forget_rel\": c2_rel,\n",
    "        \"avg_forget_rel\":       avg_rel,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57533e55",
   "metadata": {},
   "source": [
    "<a id=\"sec-03\"></a>\n",
    "## 3) Resumen “continual” + emisiones (por run)\n",
    "\n",
    "**Objetivo**  \n",
    "Construir un **resumen a nivel de experimento** a partir de `continual_results.json` y **enriquecerlo** con:\n",
    "- **Emisiones** de *CodeCarbon* (`emissions.csv`): `emissions_kg`, `energy_kwh`, `cpu_kwh`, `gpu_kwh`, `ram_kwh`, `duration_s`.  \n",
    "- **Telemetría del runner** (`telemetry.jsonl`): `telemetry_elapsed_sec`, `telemetry_emissions_kg` (si existen).\n",
    "\n",
    "**Salida adicional**\n",
    "- Se guarda `outputs/summary/runs_with_emissions.csv` (rendimiento + emisiones) listo para graficar o compartir.\n",
    "\n",
    "> Si no hay `emissions.csv` (no activaste CodeCarbon), las columnas de emisiones quedarán **vacías** para esos runs.\n",
    "\n",
    "[↑ Volver al índice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2 — Reconstrucción 100% desde ficheros (robusta a formatos)\n",
    "\n",
    "import os, re, json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ——— Fallback por si _read_forgetting no quedó definido en la Celda 1 ———\n",
    "try:\n",
    "    _read_forgetting\n",
    "except NameError:\n",
    "    def _read_forgetting(run_dir: Path):\n",
    "        def _nanmean2(a, b):\n",
    "            vals = [x for x in [a, b] if x is not None and not math.isnan(_safe_float(x))]\n",
    "            return (float(np.mean([_safe_float(x) for x in vals])) if vals else np.nan)\n",
    "        summ = _read_json(run_dir / \"forgetting_summary.json\")\n",
    "        if isinstance(summ, dict):\n",
    "            c1_abs = _safe_float(summ.get(\"circuito1_forget_abs\", summ.get(\"task_1_forget_abs\")))\n",
    "            c1_rel = _safe_float(summ.get(\"circuito1_forget_rel\", summ.get(\"task_1_forget_rel\")))\n",
    "            c2_abs = _safe_float(summ.get(\"circuito2_forget_abs\", summ.get(\"task_2_forget_abs\")))\n",
    "            c2_rel = _safe_float(summ.get(\"circuito2_forget_rel\", summ.get(\"task_2_forget_rel\")))\n",
    "            avg_rel = _safe_float(summ.get(\"avg_forget_rel\", summ.get(\"avg_forgetting_rel\")))\n",
    "            if math.isnan(avg_rel): avg_rel = _nanmean2(c1_rel, c2_rel)\n",
    "            return {\n",
    "                \"circuito1_forget_abs\": c1_abs, \"circuito1_forget_rel\": c1_rel,\n",
    "                \"circuito2_forget_abs\": c2_abs, \"circuito2_forget_rel\": c2_rel,\n",
    "                \"avg_forget_rel\": avg_rel,\n",
    "            }\n",
    "        js = _read_json(run_dir / \"forgetting.json\") or {}\n",
    "        if not isinstance(js, dict): return {}\n",
    "        def pick(d, *keys):\n",
    "            for k in keys:\n",
    "                if k in d: return d[k]\n",
    "            return None\n",
    "        c1_abs = _safe_float(pick(js, \"circuito1_forget_abs\",\"task_1_circuito1_forget_abs\",\"c1_forget_abs\",\"task_1_forget_abs\"))\n",
    "        c1_rel = _safe_float(pick(js, \"circuito1_forget_rel\",\"task_1_circuito1_forget_rel\",\"c1_forget_rel\",\"task_1_forget_rel\"))\n",
    "        c2_abs = _safe_float(pick(js, \"circuito2_forget_abs\",\"task_2_circuito2_forget_abs\",\"c2_forget_abs\",\"task_2_forget_abs\"))\n",
    "        c2_rel = _safe_float(pick(js, \"circuito2_forget_rel\",\"task_2_circuito2_forget_rel\",\"c2_forget_rel\",\"task_2_forget_rel\"))\n",
    "        avg_rel = _safe_float(pick(js, \"avg_forget_rel\",\"avg_forgetting_rel\",\"mean_forget_rel\",\"forget_rel_avg\"))\n",
    "        if (\"circuito1\" in js) and isinstance(js[\"circuito1\"], dict):\n",
    "            if math.isnan(c1_abs): c1_abs = _safe_float(js[\"circuito1\"].get(\"forget_abs\"))\n",
    "            if math.isnan(c1_rel): c1_rel = _safe_float(js[\"circuito1\"].get(\"forget_rel\"))\n",
    "        if (\"circuito2\" in js) and isinstance(js[\"circuito2\"], dict):\n",
    "            if math.isnan(c2_abs): c2_abs = _safe_float(js[\"circuito2\"].get(\"forget_abs\"))\n",
    "            if math.isnan(c2_rel): c2_rel = _safe_float(js[\"circuito2\"].get(\"forget_rel\"))\n",
    "        if math.isnan(avg_rel):\n",
    "            vals = [x for x in [c1_rel, c2_rel] if not math.isnan(_safe_float(x))]\n",
    "            avg_rel = float(np.mean([_safe_float(x) for x in vals])) if vals else np.nan\n",
    "        return {\n",
    "            \"circuito1_forget_abs\": c1_abs, \"circuito1_forget_rel\": c1_rel,\n",
    "            \"circuito2_forget_abs\": c2_abs, \"circuito2_forget_rel\": c2_rel,\n",
    "            \"avg_forget_rel\": avg_rel,\n",
    "        }\n",
    "\n",
    "# ——— Helpers internos ———\n",
    "\n",
    "def _parse_basic_meta(run_dir: Path) -> dict:\n",
    "    \"\"\"Extrae preset, method, encoder, model, seed, T, amp, batch_size desde los artefactos.\n",
    "       Prioridad: run_row.json → manifest de tarea → heurística de nombre de carpeta.\n",
    "    \"\"\"\n",
    "    jrow = _read_json(run_dir / \"run_row.json\") or {}\n",
    "\n",
    "    def gj(*ks, default=None):\n",
    "        obj = jrow\n",
    "        for k in ks:\n",
    "            if not isinstance(obj, dict) or (k not in obj):\n",
    "                return default\n",
    "            obj = obj[k]\n",
    "        return obj\n",
    "\n",
    "    # Campos directos de run_row.json (top-level o dentro de meta/data/training)\n",
    "    preset   = jrow.get(\"preset\") or gj(\"meta\",\"preset\")\n",
    "    method   = jrow.get(\"method\") or gj(\"meta\",\"method\")\n",
    "    encoder  = jrow.get(\"encoder\") or gj(\"meta\",\"encoder\")\n",
    "    model    = jrow.get(\"model\")   or jrow.get(\"model_name\") or gj(\"meta\",\"model\") or gj(\"meta\",\"model_name\")\n",
    "    seed     = jrow.get(\"seed\")    or gj(\"meta\",\"seed\")\n",
    "    T        = jrow.get(\"T\")       or gj(\"data\",\"T\") or gj(\"meta\",\"data\",\"T\") or gj(\"meta\",\"T\")\n",
    "    amp      = jrow.get(\"amp\")     or gj(\"training\",\"amp\") or gj(\"meta\",\"amp\")\n",
    "    batch_sz = jrow.get(\"batch_size\") or gj(\"meta\",\"batch_size\")\n",
    "\n",
    "    # Manifest de la primera tarea (por si faltan cosas)\n",
    "    man1 = _read_json(run_dir / \"task_1_circuito1\" / \"manifest.json\")\n",
    "    if isinstance(man1, dict):\n",
    "        meta1 = man1.get(\"meta\", {}) if isinstance(man1.get(\"meta\", {}), dict) else {}\n",
    "        if not model:        model     = meta1.get(\"model\") or man1.get(\"model\") or man1.get(\"model_name\")\n",
    "        if batch_sz is None: batch_sz  = meta1.get(\"batch_size\", batch_sz)\n",
    "        # Claves típicas en tu formato:\n",
    "        if T is None:        T         = meta1.get(\"T\", T)        # T vive en meta\n",
    "        if amp is None:      amp       = meta1.get(\"amp\", amp)    # amp vive en meta\n",
    "        if not encoder:      encoder   = meta1.get(\"encoder\", encoder)\n",
    "        if not preset:       preset    = meta1.get(\"preset\", preset)\n",
    "        if not method:       method    = meta1.get(\"method\", method)\n",
    "\n",
    "    # Heurística de nombre de carpeta si faltan preset/method/encoder\n",
    "    if not preset or not method or not encoder:\n",
    "        name = run_dir.name\n",
    "        m = re.match(r\"continual_([^_]+)_([^_].*?)_(rate|latency|raw|image).*\", name)\n",
    "        if m:\n",
    "            preset  = preset  or m.group(1)\n",
    "            method  = method  or m.group(2)\n",
    "            encoder = encoder or m.group(3)\n",
    "\n",
    "    # Tipados/normalizaciones\n",
    "    seed    = _safe_float(seed)\n",
    "    T       = _safe_float(T)\n",
    "    if isinstance(amp, str):\n",
    "        amp = amp.strip().lower() in {\"true\",\"1\",\"yes\",\"y\"}\n",
    "    elif isinstance(amp, (int, float)):\n",
    "        amp = bool(amp)\n",
    "    elif amp is not None and not isinstance(amp, bool):\n",
    "        amp = None\n",
    "    batch_sz = _safe_float(batch_sz)\n",
    "\n",
    "    return dict(\n",
    "        preset=preset, method=method, encoder=encoder, model=model,\n",
    "        seed=seed, T=T, amp=amp, batch_size=batch_sz\n",
    "    )\n",
    "\n",
    "def _read_per_task_perf(run_dir: Path) -> dict:\n",
    "    \"\"\"Devuelve dict por tarea con {'best_mae','final_mae'} normalizadas.\n",
    "       Soporta per_task_perf.json (lista/dict) y per_task_perf.csv.\n",
    "       Normaliza nombres de tarea (lower) y fusiona JSON+CSV para rellenar huecos.\n",
    "    \"\"\"\n",
    "    def _norm_row_dictlike(d):\n",
    "        tname = (str(d.get(\"task_name\") or d.get(\"task\") or d.get(\"name\") or \"\")).strip().lower()\n",
    "        # añadimos 'test_mae' como candidato de 'best'\n",
    "        best_candidates  = [\"best_mae\",\"val_best_mae\",\"best\",\"mae_best\",\"min_mae\",\"test_mae\"]\n",
    "        final_candidates = [\"final_mae\",\"val_final_mae\",\"val_last_mae\",\"last_mae\",\"mae_last\",\"mae_final\"]\n",
    "        best  = next((d.get(k) for k in best_candidates  if k in d), None)\n",
    "        final = next((d.get(k) for k in final_candidates if k in d), None)\n",
    "        return tname, {\"best_mae\": _safe_float(best), \"final_mae\": _safe_float(final)}\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    # 1) JSON\n",
    "    js = _read_json(run_dir / \"per_task_perf.json\")\n",
    "    if js is not None:\n",
    "        if isinstance(js, list):\n",
    "            for row in js:\n",
    "                if isinstance(row, dict):\n",
    "                    t, val = _norm_row_dictlike(row)\n",
    "                    if t:\n",
    "                        out[t] = val\n",
    "        elif isinstance(js, dict):\n",
    "            if all(isinstance(v, dict) for v in js.values()):\n",
    "                for k, v in js.items():\n",
    "                    t, val = _norm_row_dictlike({\"task_name\": k, **v})\n",
    "                    if t:\n",
    "                        out[t] = val\n",
    "            else:\n",
    "                try:\n",
    "                    df = pd.DataFrame(js)\n",
    "                    for _, row in df.iterrows():\n",
    "                        t, val = _norm_row_dictlike(row.to_dict())\n",
    "                        if t:\n",
    "                            out[t] = val\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # 2) CSV (fusiona y rellena NaNs si los hay)\n",
    "    df = _read_csv_df(run_dir / \"per_task_perf.csv\")\n",
    "    if df is not None and not df.empty:\n",
    "        for _, row in df.iterrows():\n",
    "            tn = (str(row.get(\"task_name\") or row.get(\"task\") or row.get(\"name\") or \"\")).strip().lower()\n",
    "            if not tn:\n",
    "                continue\n",
    "            cur = out.get(tn, {\"best_mae\": np.nan, \"final_mae\": np.nan})\n",
    "            if math.isnan(_safe_float(cur.get(\"best_mae\"))):\n",
    "                cur[\"best_mae\"] = _safe_float(row.get(\"val_best_mae\", row.get(\"best_mae\", row.get(\"test_mae\"))))\n",
    "            if math.isnan(_safe_float(cur.get(\"final_mae\"))):\n",
    "                cur[\"final_mae\"] = _safe_float(row.get(\"val_last_mae\", row.get(\"val_final_mae\", row.get(\"final_mae\"))))\n",
    "            out[tn] = cur\n",
    "\n",
    "    return out\n",
    "\n",
    "def _read_efficiency(run_dir: Path):\n",
    "    \"\"\"Lee emisiones/tiempo desde efficiency_summary.json; si falta, intenta emissions.csv y run_row.json.\"\"\"\n",
    "    j = _read_json(run_dir / \"efficiency_summary.json\") or {}\n",
    "    emissions = _safe_float(j.get(\"emissions_kg\"), default=np.nan)\n",
    "    elapsed   = _safe_float(j.get(\"elapsed_sec\"),   default=np.nan)\n",
    "\n",
    "    if math.isnan(emissions):\n",
    "        df = _read_csv_df(run_dir / \"emissions.csv\")\n",
    "        if df is not None:\n",
    "            col = \"co2e_kg\" if \"co2e_kg\" in df.columns else (\"emissions_kg\" if \"emissions_kg\" in df.columns else None)\n",
    "            if col:\n",
    "                emissions = float(df[col].sum())\n",
    "\n",
    "    if math.isnan(elapsed) or math.isnan(emissions):\n",
    "        rj = _read_json(run_dir / \"run_row.json\") or {}\n",
    "        if math.isnan(elapsed):\n",
    "            elapsed = _safe_float(rj.get(\"elapsed_sec\"), default=elapsed)\n",
    "        if math.isnan(emissions):\n",
    "            emissions = _safe_float(rj.get(\"emissions_kg\"), default=emissions)\n",
    "\n",
    "    return emissions, elapsed\n",
    "\n",
    "def _read_eval_matrix(run_dir: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"Carga eval_matrix (csv preferente) o reconstruye desde json {'tasks':[], 'mae_matrix':[[]]}.\"\"\"\n",
    "    p_csv = run_dir / \"eval_matrix.csv\"\n",
    "    if p_csv.exists():\n",
    "        try:\n",
    "            return pd.read_csv(p_csv)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    js = _read_json(run_dir / \"eval_matrix.json\")\n",
    "    if isinstance(js, dict) and (\"tasks\" in js) and (\"mae_matrix\" in js):\n",
    "        tasks = list(js.get(\"tasks\") or [])\n",
    "        mat   = js.get(\"mae_matrix\") or []\n",
    "        # Intentamos formato de columnas como en CSV: 'task', 'after_circuito1', 'after_circuito2',...\n",
    "        try:\n",
    "            mat = np.array(mat, dtype=float)\n",
    "            cols = [\"task\"] + [f\"after_{t}\" for t in tasks]\n",
    "            data = {\"task\": tasks}\n",
    "            for j, cname in enumerate(cols[1:]):\n",
    "                colvals = [row[j] if j < len(row) else np.nan for row in mat]\n",
    "                data[cname] = colvals\n",
    "            return pd.DataFrame(data)\n",
    "        except Exception:\n",
    "            # fallback genérico\n",
    "            try:\n",
    "                return pd.DataFrame(js)\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def _compute_best_final_from_eval(eval_df: pd.DataFrame, task_token: str):\n",
    "    \"\"\"Obtiene BEST/FINAL mirando primero por FILAS (col 'task') y,\n",
    "    si no existe, hace fallback a columnas que contengan el token.\"\"\"\n",
    "    if eval_df is None or eval_df.empty:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # 1) Preferir formato fila: 'task' == task_token\n",
    "    tcol = None\n",
    "    for c in eval_df.columns:\n",
    "        if str(c).strip().lower() == \"task\":\n",
    "            tcol = c\n",
    "            break\n",
    "    if tcol is not None:\n",
    "        mask = eval_df[tcol].astype(str).str.lower() == str(task_token).lower()\n",
    "        if mask.any():\n",
    "            row = eval_df.loc[mask].iloc[0]\n",
    "            data_cols = [c for c in eval_df.columns if c != tcol]\n",
    "\n",
    "            vals = pd.to_numeric(row[data_cols], errors=\"coerce\").values.astype(float)\n",
    "            finite = np.isfinite(vals)\n",
    "            best = float(np.nanmin(vals)) if finite.any() else np.nan\n",
    "\n",
    "            # FINAL = valor en la última columna temporal (p.ej., after_última_tarea)\n",
    "            final = _safe_float(row[data_cols[-1]])\n",
    "            return (best, final)\n",
    "\n",
    "    # 2) Fallback: columnas que contengan el token (formatos antiguos)\n",
    "    cols = [c for c in eval_df.columns if str(task_token).lower() in str(c).lower()]\n",
    "    if cols:\n",
    "        dfc = eval_df[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        vals = dfc.values.astype(float)\n",
    "        finite = np.isfinite(vals)\n",
    "        best = float(np.nanmin(vals)) if finite.any() else np.nan\n",
    "        final = _safe_float(dfc.iloc[-1, -1])\n",
    "        return (best, final)\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "def _compute_forgetting_from_eval_matrix(eval_df: pd.DataFrame, per_task: dict):\n",
    "    \"\"\"Olvido para c1 tras aprender c2. C2 no olvida (0).\"\"\"\n",
    "    out = {}\n",
    "    t1_keys = [k for k in (per_task or {}).keys() if \"circuito1\" in str(k).lower()]\n",
    "    best_t1 = None\n",
    "    if t1_keys:\n",
    "        best_t1 = _safe_float((per_task.get(t1_keys[0]) or {}).get(\"best_mae\"))\n",
    "    if best_t1 is None or math.isnan(best_t1):\n",
    "        best_t1, _ = _compute_best_final_from_eval(eval_df, \"circuito1\")\n",
    "\n",
    "    _, final_t1_after_last = _compute_best_final_from_eval(eval_df, \"circuito1\")\n",
    "    if best_t1 is None or math.isnan(best_t1) or final_t1_after_last is None or math.isnan(final_t1_after_last):\n",
    "        return out\n",
    "\n",
    "    forget_abs = max(0.0, final_t1_after_last - best_t1)\n",
    "    forget_rel = forget_abs / max(1e-9, best_t1)\n",
    "    out[\"circuito1_forget_abs\"] = forget_abs\n",
    "    out[\"circuito1_forget_rel\"] = forget_rel\n",
    "    out[\"circuito2_forget_abs\"] = 0.0\n",
    "    out[\"circuito2_forget_rel\"] = 0.0\n",
    "    out[\"avg_forget_rel\"] = forget_rel\n",
    "    return out\n",
    "\n",
    "def _read_continual_results(run_dir: Path) -> dict:\n",
    "    \"\"\"Fallback final para MAEs desde continual_results.json.\"\"\"\n",
    "    j = _read_json(run_dir / \"continual_results.json\")\n",
    "    if not isinstance(j, dict):\n",
    "        return {}\n",
    "    c1 = j.get(\"circuito1\", {}) or {}\n",
    "    c2 = j.get(\"circuito2\", {}) or {}\n",
    "    return {\n",
    "        \"c1_best\":  _safe_float(c1.get(\"test_mae\")),\n",
    "        \"c1_final\": _safe_float(c1.get(\"after_circuito2_mae\")),\n",
    "        \"c2_best\":  _safe_float(c2.get(\"test_mae\")),\n",
    "        \"c2_final\": _safe_float(c2.get(\"test_mae\")),\n",
    "    }\n",
    "\n",
    "def build_results_table_from_disk(base_out: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    run_dirs = [p for p in base_out.glob(\"continual_*\") if p.is_dir()]\n",
    "    print(f\"[INFO] Escaneando {len(run_dirs)} runs en {base_out}\")\n",
    "\n",
    "    for rd in run_dirs:\n",
    "        meta      = _parse_basic_meta(rd)\n",
    "        per_task  = _read_per_task_perf(rd)\n",
    "        eff_kg, elapsed = _read_efficiency(rd)\n",
    "        forget_js = _read_forgetting(rd) or {}\n",
    "        eval_df   = _read_eval_matrix(rd)\n",
    "\n",
    "        # MAEs por tarea con fallback a eval_matrix y, si hace falta, continual_results.json\n",
    "        c1_best = c1_final = c2_best = c2_final = np.nan\n",
    "\n",
    "        t1_keys = [k for k in per_task.keys() if \"circuito1\" in str(k).lower()]\n",
    "        if t1_keys:\n",
    "            c1_best  = _safe_float(per_task[t1_keys[0]].get(\"best_mae\"),  default=np.nan)\n",
    "            c1_final = _safe_float(per_task[t1_keys[0]].get(\"final_mae\"), default=np.nan)\n",
    "        if math.isnan(c1_best) or math.isnan(c1_final):\n",
    "            b, f = _compute_best_final_from_eval(eval_df, \"circuito1\")\n",
    "            if math.isnan(c1_best):  c1_best  = b\n",
    "            if math.isnan(c1_final): c1_final = f\n",
    "\n",
    "        t2_keys = [k for k in per_task.keys() if \"circuito2\" in str(k).lower()]\n",
    "        if t2_keys:\n",
    "            c2_best  = _safe_float(per_task[t2_keys[0]].get(\"best_mae\"),  default=np.nan)\n",
    "            c2_final = _safe_float(per_task[t2_keys[0]].get(\"final_mae\"), default=np.nan)\n",
    "        if math.isnan(c2_best) or math.isnan(c2_final):\n",
    "            b, f = _compute_best_final_from_eval(eval_df, \"circuito2\")\n",
    "            if math.isnan(c2_best):  c2_best  = b\n",
    "            if math.isnan(c2_final): c2_final = f\n",
    "\n",
    "        # Fallback definitivo: continual_results.json\n",
    "        if any(math.isnan(x) for x in [c1_best, c1_final, c2_best, c2_final]):\n",
    "            cr = _read_continual_results(rd)\n",
    "            if math.isnan(c1_best):  c1_best  = _safe_float(cr.get(\"c1_best\"),  default=c1_best)\n",
    "            if math.isnan(c1_final): c1_final = _safe_float(cr.get(\"c1_final\"), default=c1_final)\n",
    "            if math.isnan(c2_best):  c2_best  = _safe_float(cr.get(\"c2_best\"),  default=c2_best)\n",
    "            if math.isnan(c2_final): c2_final = _safe_float(cr.get(\"c2_final\"), default=c2_final)\n",
    "\n",
    "        # Olvido (summary/json) o cálculo desde eval_matrix\n",
    "        f_c1_abs = _safe_float(forget_js.get(\"circuito1_forget_abs\"))\n",
    "        f_c1_rel = _safe_float(forget_js.get(\"circuito1_forget_rel\"))\n",
    "        f_c2_abs = _safe_float(forget_js.get(\"circuito2_forget_abs\"))\n",
    "        f_c2_rel = _safe_float(forget_js.get(\"circuito2_forget_rel\"))\n",
    "        avg_f_rel = _safe_float(forget_js.get(\"avg_forget_rel\"))\n",
    "        if all(math.isnan(x) for x in [f_c1_abs, f_c1_rel, f_c2_abs, f_c2_rel, avg_f_rel]):\n",
    "            comp = _compute_forgetting_from_eval_matrix(eval_df, per_task)\n",
    "            if comp:\n",
    "                f_c1_abs = comp.get(\"circuito1_forget_abs\", f_c1_abs)\n",
    "                f_c1_rel = comp.get(\"circuito1_forget_rel\", f_c1_rel)\n",
    "                f_c2_abs = comp.get(\"circuito2_forget_abs\", f_c2_abs)\n",
    "                f_c2_rel = comp.get(\"circuito2_forget_rel\", f_c2_rel)\n",
    "                avg_f_rel = comp.get(\"avg_forget_rel\", avg_f_rel)\n",
    "\n",
    "        row = dict(\n",
    "            run_dir=str(rd.relative_to(base_out)),\n",
    "            preset=meta[\"preset\"],\n",
    "            method=meta[\"method\"],\n",
    "            encoder=meta[\"encoder\"],\n",
    "            model=meta[\"model\"],\n",
    "            seed=meta[\"seed\"],\n",
    "            T=meta[\"T\"],\n",
    "            batch_size=meta[\"batch_size\"],\n",
    "            amp=meta[\"amp\"],\n",
    "            emissions_kg=eff_kg,\n",
    "            elapsed_sec=elapsed,\n",
    "            circuito1_best_mae=c1_best,\n",
    "            circuito1_final_mae=c1_final,\n",
    "            circuito2_best_mae=c2_best,\n",
    "            circuito2_final_mae=c2_final,\n",
    "            circuito1_forget_abs=f_c1_abs,\n",
    "            circuito1_forget_rel=f_c1_rel,\n",
    "            circuito2_forget_abs=f_c2_abs,\n",
    "            circuito2_forget_rel=f_c2_rel,\n",
    "            avg_forget_rel=avg_f_rel,\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Flags extra\n",
    "    df[\"is_new_runner\"] = df[\"run_dir\"].apply(lambda rd: (_abs_run_dir(rd) / \"run_row.json\").exists() or (_abs_run_dir(rd) / \"run_row.csv\").exists())\n",
    "    df[\"mtime\"] = df[\"run_dir\"].apply(run_mtime)\n",
    "    df[\"mtime_dt\"] = pd.to_datetime(df[\"mtime\"], unit=\"s\")\n",
    "    df[\"method_base\"] = df[\"method\"].astype(str).apply(canonical_method)\n",
    "\n",
    "    # Tipado numérico: EXCLUYE 'amp' (lo normalizamos como boolean más abajo)\n",
    "    numeric_cols = [\n",
    "        \"seed\",\"T\",\"batch_size\",\"emissions_kg\",\"elapsed_sec\",\n",
    "        \"circuito1_best_mae\",\"circuito1_final_mae\",\"circuito2_best_mae\",\"circuito2_final_mae\",\n",
    "        \"circuito1_forget_abs\",\"circuito1_forget_rel\",\"circuito2_forget_abs\",\"circuito2_forget_rel\",\"avg_forget_rel\"\n",
    "    ]\n",
    "    for c in numeric_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # 'amp' como boolean (permite NA)\n",
    "    if \"amp\" in df.columns:\n",
    "        if df[\"amp\"].dtype == bool:\n",
    "            df[\"amp\"] = df[\"amp\"].astype(\"boolean\")\n",
    "        else:\n",
    "            df[\"amp\"] = df[\"amp\"].map(\n",
    "                lambda v: bool(v) if isinstance(v, (bool,int,float))\n",
    "                else (str(v).strip().lower() in {\"true\",\"1\",\"yes\",\"y\"} if isinstance(v, str) else pd.NA)\n",
    "            ).astype(\"boolean\")\n",
    "\n",
    "    out_csv = SUMMARY / \"results_table_fromdisk.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] results_table_fromdisk → {out_csv} | filas:\", len(df))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777fd96",
   "metadata": {},
   "source": [
    "<a id=\"sec-04\"></a>\n",
    "## 4) Resumen de entrenamiento por tarea (manifest/metrics)\n",
    "\n",
    "**Objetivo**  \n",
    "Leer, para cada carpeta `outputs/continual_*/*task_*`, el archivo `manifest.json` (o `metrics.json`) y extraer:\n",
    "- **Hiperparámetros efectivos** por tarea: `epochs`, `batch_size`, `lr`, `amp`, `seed`.  \n",
    "- **Historial de pérdidas**: últimas `train_loss` y `val_loss`.\n",
    "\n",
    "Notas:\n",
    "- Si estaba activo **Early Stopping**, el número real de épocas útiles puede ser **menor** que el configurado.  \n",
    "- Si una carpeta de tarea no contiene `manifest.json` ni `metrics.json`, simplemente se ignora en este resumen.  \n",
    "- Este bloque **no recalcula** métricas; solo muestra lo **registrado** durante el entrenamiento.\n",
    "\n",
    "[↑ Volver al índice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed5a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3 — Construcción + diagnóstico NaNs\n",
    "\n",
    "df_all = build_results_table_from_disk(OUT)\n",
    "display(df_all.head(3))\n",
    "\n",
    "nan_cols = [\"circuito1_best_mae\",\"circuito1_final_mae\",\"circuito2_best_mae\",\"circuito2_final_mae\",\"avg_forget_rel\"]\n",
    "print(\"[DEBUG] NaNs globales:\", {c:int(df_all[c].isna().sum()) for c in nan_cols if c in df_all.columns})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f6ecf",
   "metadata": {},
   "source": [
    "<a id=\"sec-05\"></a>\n",
    "## 5) Gráficos rápidos (MAE, olvido, emisiones)\n",
    "\n",
    "**Objetivo**  \n",
    "Ofrecer una vista rápida y comparativa:\n",
    "- Barras de **`c1_mae`** por experimento (calidad en la primera tarea).  \n",
    "- Barras de **olvido relativo** (`c1_forgetting_mae_rel_%`) si la columna existe y hay ≥2 tareas.  \n",
    "- **NUEVO**: Barras de **emisiones totales (kg CO₂e)** por experimento y *scatter* **MAE vs emisiones** (trade-off rendimiento/sostenibilidad), usando el CSV enriquecido.\n",
    "\n",
    "> Gráficos intencionadamente simples para inspección rápida. Para informes finales, exporta el CSV y construye figuras personalizadas.\n",
    "\n",
    "[↑ Volver al índice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735adae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4 — Selección y comparabilidad\n",
    "def _filter_paperset(df: pd.DataFrame, require_both_tasks: bool = True) -> pd.DataFrame:\n",
    "    m = df.copy()\n",
    "    if PRESET_KEEP:\n",
    "        m = m[m[\"preset\"].isin(PRESET_KEEP)]\n",
    "    if ENCODER_KEEP:\n",
    "        m = m[m[\"encoder\"].isin(ENCODER_KEEP)]\n",
    "    if SEED_KEEP:\n",
    "        m = m[m[\"seed\"].isin(SEED_KEEP)]\n",
    "    if METHODS_KEEP:\n",
    "        m = m[m[\"method_base\"].isin(METHODS_KEEP)]\n",
    "    if ONLY_NEW_RUNNER:\n",
    "        m = m[m[\"is_new_runner\"] == True]\n",
    "    if MTIME_FROM is not None:\n",
    "        m = m[m[\"mtime_dt\"] >= MTIME_FROM]\n",
    "\n",
    "    # Runs con ambas tareas evaluadas (mejor y final). Evita arrastrar runs truncados.\n",
    "    if require_both_tasks:\n",
    "        must_cols = [\"circuito1_best_mae\",\"circuito1_final_mae\",\"circuito2_best_mae\",\"circuito2_final_mae\"]\n",
    "        for c in must_cols:\n",
    "            if c in m.columns:\n",
    "                m = m[~m[c].isna()]\n",
    "\n",
    "    print(f\"[DEBUG] filtros duros → inicio → {len(df)} runs | post → {len(m)} runs\")\n",
    "    return m\n",
    "\n",
    "def _comparability_slice(df: pd.DataFrame, strict: bool = True):\n",
    "    if df.empty:\n",
    "        return df, {\"estrategia\":\"empty\", \"kept\":0, \"dropped\":0}\n",
    "    # Siempre mismo modelo y mismo T\n",
    "    modes_model = df[\"model\"].dropna().unique().tolist()\n",
    "    modes_T     = df[\"T\"].dropna().unique().tolist()\n",
    "    if len(modes_model) > 1:\n",
    "        top_model = df[\"model\"].value_counts().idxmax()\n",
    "        df = df[df[\"model\"] == top_model]\n",
    "    if len(modes_T) > 1:\n",
    "        top_T = df[\"T\"].value_counts().idxmax()\n",
    "        df = df[df[\"T\"] == top_T]\n",
    "\n",
    "    kept_before = len(df)\n",
    "    if strict:\n",
    "        # AMP mayoritario (si hay), manteniendo NaN\n",
    "        if df[\"amp\"].notna().any():\n",
    "            top_amp = df[\"amp\"].value_counts(dropna=True).idxmax()\n",
    "            df = df[(df[\"amp\"].isna()) | (df[\"amp\"] == top_amp)]\n",
    "        # batch_size mayoritario (si hay), manteniendo NaN\n",
    "        if df[\"batch_size\"].notna().any():\n",
    "            top_bs = df[\"batch_size\"].value_counts(dropna=True).idxmax()\n",
    "            df = df[(df[\"batch_size\"].isna()) | (df[\"batch_size\"] == top_bs)]\n",
    "        strategy = \"strict:model+T(+amp+batch)\"\n",
    "    else:\n",
    "        strategy = \"relaxed:model+T\"\n",
    "\n",
    "    kept_after = len(df)\n",
    "    return df, {\"estrategia\": strategy, \"kept\": kept_after, \"dropped\": kept_before - kept_after}\n",
    "\n",
    "df_sel0 = _filter_paperset(df_all, require_both_tasks=True)\n",
    "df_sel, stats = _comparability_slice(df_sel0, strict=STRICT_CFG)\n",
    "if df_sel.empty and STRICT_CFG:\n",
    "    print(\"[WARN] Comparabilidad dejó 0 runs. Relajando AMP y batch_size…\")\n",
    "    df_sel, stats = _comparability_slice(df_sel0, strict=False)\n",
    "\n",
    "# Copia para informes (se puede filtrar naive aquí si se desea solo de cara a tablas/plots)\n",
    "df_sel[\"batch_size_filled\"] = df_sel[\"batch_size\"].copy()\n",
    "df_report = df_sel.copy()\n",
    "if IGNORE_NAIVE_IN_REPORTS and \"method_base\" in df_report.columns:\n",
    "    df_report = df_report[df_report[\"method_base\"] != \"naive\"]\n",
    "\n",
    "display(df_sel.head(10))\n",
    "print(f\"[OK] Selección final: {len(df_sel)} runs | estrategia: {stats}\")\n",
    "\n",
    "out_csv = SUMMARY / \"selection_table.csv\"\n",
    "df_sel.to_csv(out_csv, index=False)\n",
    "print(\"[OK] Selección →\", out_csv)\n",
    "\n",
    "# --- NUEVO: guarda la ruta en _last.json\n",
    "_last_update(selection=out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbe622",
   "metadata": {},
   "source": [
    "<a id=\"sec-06\"></a>\n",
    "## 6) Inspección de un experimento concreto\n",
    "\n",
    "**Objetivo**  \n",
    "Abrir el `continual_results.json` del **experimento más reciente** (o el que indiques manualmente) y mostrar su contenido completo (diccionario).\n",
    "- Útil para revisar **todas** las claves guardadas por el *runner* y verificar cálculos.  \n",
    "- Si no existen carpetas `continual_*`, se informa en consola y no se imprime nada.\n",
    "\n",
    "> Pista: revisa las secciones por **tarea** y los bloques `after_*` que reflejan el rendimiento **tras** aprender nuevas tareas (base del cálculo de olvido).\n",
    "\n",
    "[↑ Volver al índice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5 — Ranking compuesto + winners por método (versión robusta)\n",
    "def _ensure_mae_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Crea alias entre c2_final_mae y circuito2_final_mae si falta alguno.\"\"\"\n",
    "    df = df.copy()\n",
    "    has_c2    = \"c2_final_mae\" in df.columns\n",
    "    has_circ2 = \"circuito2_final_mae\" in df.columns\n",
    "    if has_c2 and not has_circ2:\n",
    "        df[\"circuito2_final_mae\"] = df[\"c2_final_mae\"]\n",
    "    elif has_circ2 and not has_c2:\n",
    "        df[\"c2_final_mae\"] = df[\"circuito2_final_mae\"]\n",
    "    return df\n",
    "\n",
    "def _minmax_norm(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if s.size == 0:\n",
    "        return s\n",
    "    vals = s.values.astype(float)\n",
    "    finite = np.isfinite(vals)\n",
    "    if finite.sum() == 0:\n",
    "        return pd.Series(np.full(len(s), np.nan), index=s.index)\n",
    "    lo, hi = float(np.nanmin(vals)), float(np.nanmax(vals))\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        out = np.zeros(len(s), dtype=float)\n",
    "        out[~finite] = np.nan\n",
    "        return pd.Series(out, index=s.index)\n",
    "    return (s - lo) / (hi - lo)\n",
    "\n",
    "# 1) Copia y compatibilidad de columnas\n",
    "df_rank = _ensure_mae_columns(df_report)\n",
    "\n",
    "# Columnas clave\n",
    "MAE_COL  = \"circuito2_final_mae\"\n",
    "FORG_COL = \"avg_forget_rel\"\n",
    "\n",
    "# 2) Si está vacío, escribe CSV vacío y sal con aviso\n",
    "if df_rank.empty:\n",
    "    winners = df_rank.copy()\n",
    "    out_csv = SUMMARY / \"winners_per_method.csv\"\n",
    "    winners.to_csv(out_csv, index=False)\n",
    "    print(\"[WARN] df_rank está vacío tras filtros. Winners vacío →\", out_csv)\n",
    "    display(winners)  # DF vacío\n",
    "else:\n",
    "    mae_norm  = _minmax_norm(df_rank[MAE_COL]  if MAE_COL  in df_rank.columns else pd.Series([], dtype=float))\n",
    "    forg_norm = _minmax_norm(df_rank[FORG_COL] if FORG_COL in df_rank.columns else pd.Series([], dtype=float))\n",
    "\n",
    "    score_comp = ALPHA_COMPOSITE * mae_norm + (1 - ALPHA_COMPOSITE) * forg_norm\n",
    "    score_eff  = score_comp.copy()\n",
    "    if len(score_eff) != 0:\n",
    "        score_eff[forg_norm.isna()] = mae_norm[forg_norm.isna()]  # fallback\n",
    "\n",
    "    df_rank[\"score_comp\"] = score_comp\n",
    "    df_rank[\"score_eff\"]  = score_eff\n",
    "\n",
    "    # Agrupación: por método base o por método completo (separa composites)\n",
    "    group_col = \"method\" if GROUP_BY_FULL_METHOD else \"method_base\"\n",
    "    if group_col not in df_rank.columns:\n",
    "        df_rank[group_col] = df_rank[\"method\"] if \"method\" in df_rank.columns else \"unknown\"\n",
    "\n",
    "    # Winners\n",
    "    if df_rank[\"score_eff\"].notna().any():\n",
    "        winners = (\n",
    "            df_rank\n",
    "            .sort_values([group_col, \"score_eff\", MAE_COL], ascending=[True, True, True])\n",
    "            .groupby(group_col, as_index=False)\n",
    "            .head(1)\n",
    "            .sort_values(\"score_eff\", ascending=True)\n",
    "        )\n",
    "    else:\n",
    "        winners = (\n",
    "            df_rank\n",
    "            .sort_values([group_col, MAE_COL], ascending=[True, True])\n",
    "            .groupby(group_col, as_index=False)\n",
    "            .head(1)\n",
    "        )\n",
    "\n",
    "    # Export\n",
    "    out_csv = SUMMARY / f\"winners_per_{'fullmethod' if GROUP_BY_FULL_METHOD else 'methodbase'}.csv\"\n",
    "    winners.to_csv(out_csv, index=False)\n",
    "\n",
    "    n_rows = len(df_rank)\n",
    "    n_groups = df_rank[group_col].nunique()\n",
    "    print(f\"[OK] Winners → {out_csv} | candidatos={n_rows} | grupos={n_groups}\")\n",
    "    cols_show = [c for c in [\"run_dir\", group_col, \"seed\", MAE_COL, FORG_COL, \"emissions_kg\", \"score_eff\"] if c in winners.columns]\n",
    "    display(winners[cols_show] if cols_show else winners)\n",
    "\n",
    "# --- NUEVO: guarda la ruta de winners\n",
    "_last_update(winners=out_csv)\n",
    "# Mantén \"winners\" en el entorno por si otras celdas lo usan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b8c801",
   "metadata": {},
   "source": [
    "<a id=\"sec-07\"></a>\n",
    "## 7) Inspección detallada: CodeCarbon + Telemetry (último run)\n",
    "\n",
    "**Objetivo**  \n",
    "Mostrar los **últimos registros** de `emissions.csv` (CodeCarbon) y la **última entrada** de `telemetry.jsonl` para el run más reciente:\n",
    "- Tabla con las **últimas filas** de `emissions.csv` (para ver el cierre del tracker).  \n",
    "- Resumen rápido: `emissions_kg`, `energy_kwh`, `duration_s`.  \n",
    "- Último evento de `telemetry.jsonl` (suele incluir `elapsed_sec` y, si estaba disponible, `emissions_kg`).\n",
    "\n",
    "> Útil para validar que el tracker **cerró bien** y que los tiempos/emisiones concuerdan con lo registrado por el runner.\n",
    "\n",
    "[↑ Volver al índice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5.1 — Deltas relativos vs baseline (para narrativa TFM)\n",
    "def _row_key_for_match(r):\n",
    "    \"\"\"Clave de emparejamiento para baseline estricto.\"\"\"\n",
    "    return (\n",
    "        r.get(\"preset\"), r.get(\"encoder\"), r.get(\"model\"), r.get(\"T\"),\n",
    "        r.get(\"amp\"), r.get(\"batch_size\"), r.get(\"seed\")\n",
    "    )\n",
    "\n",
    "def compute_relative_to_baseline(df_in: pd.DataFrame, baseline=\"naive\", strict=True):\n",
    "    if not baseline:\n",
    "        return None\n",
    "    if isinstance(baseline, str) and baseline.lower() == \"auto\":\n",
    "        bases = df_in[\"method_base\"].dropna().str.lower().unique().tolist()\n",
    "        baseline = \"ewc\" if \"ewc\" in bases else (\"naive\" if \"naive\" in bases else None)\n",
    "        if not baseline:\n",
    "            return None\n",
    "    dfb = df_in.copy()\n",
    "    dfb[\"__key__\"] = dfb.apply(\n",
    "        lambda r: (r.get(\"preset\"), r.get(\"encoder\"), r.get(\"model\"), r.get(\"T\"),\n",
    "                   r.get(\"amp\"), r.get(\"batch_size\"), r.get(\"seed\")), axis=1)\n",
    "\n",
    "    # Para cada clave, escoge el run baseline con menor MAE y usa sus dos métricas\n",
    "    base_rows = []\n",
    "    for key, g in dfb[dfb[\"method_base\"].astype(str).str.lower() == baseline.lower()].groupby(\"__key__\"):\n",
    "        if g.empty:             continue\n",
    "        idx = g[\"circuito2_final_mae\"].astype(float).idxmin()\n",
    "        row = g.loc[idx]\n",
    "        base_rows.append({\n",
    "            \"__key__\": key,\n",
    "            \"baseline_mae\": float(row[\"circuito2_final_mae\"]),\n",
    "            \"baseline_forget\": float(row[\"avg_forget_rel\"])\n",
    "        })\n",
    "    base_tbl = pd.DataFrame(base_rows)\n",
    "\n",
    "    out = dfb.merge(base_tbl, on=\"__key__\", how=\"left\")\n",
    "    out[\"delta_mae_vs_base\"]    = out[\"circuito2_final_mae\"] - out[\"baseline_mae\"]\n",
    "    out[\"delta_forget_vs_base\"] = out[\"avg_forget_rel\"]      - out[\"baseline_forget\"]\n",
    "    return out\n",
    "\n",
    "if RELATIVE_BASELINE:\n",
    "    df_rel = compute_relative_to_baseline(df_sel, baseline=RELATIVE_BASELINE, strict=BASELINE_MATCH_STRICT)\n",
    "    if df_rel is not None:\n",
    "        out_csv = SUMMARY / f\"relative_to_{RELATIVE_BASELINE}.csv\"\n",
    "        df_rel.to_csv(out_csv, index=False)\n",
    "        print(f\"[OK] Relative-to-baseline → {out_csv}\")\n",
    "\n",
    "        # --- NUEVO: guarda la ruta en _last.json\n",
    "        _last_update(relative_baseline=RELATIVE_BASELINE, relative_csv=out_csv)\n",
    "\n",
    "        # Top-10 mejoras vs baseline (orden: delta_mae luego delta_forget)\n",
    "        mask_has_base = (~df_rel[\"baseline_mae\"].isna()) & (~df_rel[\"baseline_forget\"].isna())\n",
    "        top_improve = (df_rel[mask_has_base]\n",
    "                       .sort_values([\"delta_mae_vs_base\",\"delta_forget_vs_base\"], ascending=[True, True])\n",
    "                       .head(10))\n",
    "        cols_show = [\"run_dir\",\"method\",\"method_base\",\"circuito2_final_mae\",\"avg_forget_rel\",\n",
    "                     \"baseline_mae\",\"baseline_forget\",\"delta_mae_vs_base\",\"delta_forget_vs_base\"]\n",
    "        display(top_improve[cols_show])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d8366",
   "metadata": {},
   "source": [
    "<a id=\"sec-08\"></a>\n",
    "## 8) Agregados por preset/método con emisiones\n",
    "\n",
    "**Objetivo**  \n",
    "Calcular y mostrar agregados **por `preset`** y **por `method`** a partir de `runs_with_emissions.csv`:\n",
    "- `runs` (conteo), `mean_c1_mae`, `sum_emissions_kg`, `mean_emissions_kg`.\n",
    "\n",
    "> Te da una visión global del **trade-off** por configuración: calidad media y huella total/media.  \n",
    "> Si no existe el CSV enriquecido, ejecuta antes la **Sección 3**.\n",
    "\n",
    "[↑ Volver al índice](#toc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd488599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5.2 — Porcentajes vs EWC y scorecards por método (sin dependencias extra)\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Asegura que tenemos df_rel (de la Celda 5.1)\n",
    "try:\n",
    "    df_rel\n",
    "except NameError:\n",
    "    df_rel = compute_relative_to_baseline(df_sel, baseline=\"ewc\", strict=True)\n",
    "\n",
    "# 1) % mejora en MAE y cambio de olvido en puntos porcentuales (pp)\n",
    "rel = df_rel.copy()\n",
    "rel[\"mae_improv_pct\"] = 100.0 * (rel[\"baseline_mae\"] - rel[\"circuito2_final_mae\"]) / rel[\"baseline_mae\"]\n",
    "rel[\"forget_delta_pp\"] = 100.0 * (rel[\"avg_forget_rel\"] - rel[\"baseline_forget\"])\n",
    "\n",
    "# 2) “Winners” por método ya calculado en Celda 5; si no existe, lo recomputamos rápido\n",
    "try:\n",
    "    winners\n",
    "except NameError:\n",
    "    tmp = df_rel.copy()\n",
    "    # Normaliza columnas si faltan\n",
    "    if \"c2_final_mae\" not in tmp.columns and \"circuito2_final_mae\" in tmp.columns:\n",
    "        tmp[\"c2_final_mae\"] = tmp[\"circuito2_final_mae\"]\n",
    "    # Score simple si no hay score_eff\n",
    "    if \"score_eff\" not in tmp.columns:\n",
    "        tmp[\"score_eff\"] = np.nan\n",
    "    group_col = \"method_base\" if \"method_base\" in tmp.columns else \"method\"\n",
    "    winners = (tmp.sort_values([group_col, \"c2_final_mae\"], ascending=[True, True])\n",
    "                   .groupby(group_col, as_index=False).head(1))\n",
    "\n",
    "# 3) “Scorecard” por método: junta winners con porcentajes vs baseline\n",
    "key_cols = [\"run_dir\",\"method\",\"method_base\",\"preset\",\"encoder\",\"model\",\"seed\",\"T\",\"batch_size\",\"amp\"]\n",
    "cols_metrics = [\"circuito2_final_mae\",\"avg_forget_rel\",\"baseline_mae\",\"baseline_forget\",\n",
    "                \"mae_improv_pct\",\"forget_delta_pp\",\"emissions_kg\",\"elapsed_sec\"]\n",
    "scorecards = winners[key_cols].merge(rel[key_cols + cols_metrics], on=key_cols, how=\"left\")\n",
    "\n",
    "# 4) Exporta CSV legible para la memoria\n",
    "out_dir = SUMMARY\n",
    "scorecards_csv = out_dir / \"scorecards_por_metodo_vs_ewc.csv\"\n",
    "scorecards.to_csv(scorecards_csv, index=False)\n",
    "print(\"[OK] Scorecards →\", scorecards_csv)\n",
    "\n",
    "# 5) (Opcional) si tus ángulos están normalizados [-1,1], dar MAE en grados (±25° típico)\n",
    "STEER_REP = os.environ.get(\"STEER_REP\", \"unknown\")  # \"norm1\" si [-1,1], \"deg\" si ya son grados\n",
    "DEG_RANGE = float(os.environ.get(\"DEG_RANGE\", \"25\"))  # cambia si tu rango real es ±X grados\n",
    "\n",
    "def _mae_to_deg(mae):\n",
    "    try:\n",
    "        v = float(mae)\n",
    "        if STEER_REP.lower() == \"norm1\":\n",
    "            return v * DEG_RANGE\n",
    "        return v  # ya en grados o desconocido\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "scorecards[\"c2_final_mae_deg\"] = scorecards[\"circuito2_final_mae\"].apply(_mae_to_deg)\n",
    "scorecards.to_csv(scorecards_csv, index=False)  # re-escribe con la columna extra\n",
    "print(\"[OK] Añadido c2_final_mae_deg (si procede) →\", scorecards_csv)\n",
    "\n",
    "# 6) Vista rápida ordenada por “mejor” (↑% mejora MAE, ↓Δ olvido)\n",
    "show_cols = [\"method_base\",\"method\",\"circuito2_final_mae\",\"c2_final_mae_deg\",\"avg_forget_rel\",\n",
    "             \"baseline_mae\",\"baseline_forget\",\"mae_improv_pct\",\"forget_delta_pp\",\"emissions_kg\"]\n",
    "disp = (scorecards.sort_values([\"mae_improv_pct\",\"forget_delta_pp\"], ascending=[False, True]))[show_cols]\n",
    "display(disp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4a121",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6 — Top-N global y Pareto (versión robusta)\n",
    "# helpers (por si no vienes de la Celda 5)\n",
    "try:\n",
    "    _ = _ensure_mae_columns\n",
    "except NameError:\n",
    "    def _ensure_mae_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        has_c2 = \"c2_final_mae\" in df.columns\n",
    "        has_circ2 = \"circuito2_final_mae\" in df.columns\n",
    "        if has_c2 and not has_circ2:\n",
    "            df[\"circuito2_final_mae\"] = df[\"c2_final_mae\"]\n",
    "        elif has_circ2 and not has_c2:\n",
    "            df[\"c2_final_mae\"] = df[\"circuito2_final_mae\"]\n",
    "        return df\n",
    "\n",
    "df_rank = _ensure_mae_columns(df_report if \"df_report\" in globals() else df_sel.copy())\n",
    "if \"score_eff\" not in df_rank.columns:\n",
    "    df_rank = df_rank.copy()\n",
    "    df_rank[\"score_eff\"] = np.nan\n",
    "if \"method_base\" not in df_rank.columns:\n",
    "    df_rank[\"method_base\"] = df_rank[\"method\"] if \"method\" in df_rank.columns else \"unknown\"\n",
    "\n",
    "MAE_COL = \"circuito2_final_mae\" if \"circuito2_final_mae\" in df_rank.columns else (\"c2_final_mae\" if \"c2_final_mae\" in df_rank.columns else \"circuito2_final_mae\")\n",
    "\n",
    "# Top-N compuesto\n",
    "TOPN = 6\n",
    "if df_rank.empty:\n",
    "    topn = df_rank.copy()\n",
    "    print(\"[WARN] df_rank está vacío; Top-N vacío.\")\n",
    "else:\n",
    "    cols_sort = [c for c in [\"score_eff\", MAE_COL] if c in df_rank.columns]\n",
    "    if not cols_sort:\n",
    "        topn = df_rank.head(TOPN)\n",
    "    else:\n",
    "        topn = df_rank.sort_values(cols_sort, ascending=[True] * len(cols_sort)).head(TOPN)\n",
    "\n",
    "out_csv = SUMMARY / \"top6_composite.csv\"\n",
    "topn.to_csv(out_csv, index=False)\n",
    "print(\"[OK] Top-6 →\", out_csv)\n",
    "\n",
    "# --- NUEVO: guarda la ruta\n",
    "_last_update(top6=out_csv)\n",
    "\n",
    "cols_show = [c for c in [\"run_dir\",\"method_base\",\"seed\",MAE_COL,\"avg_forget_rel\",\"emissions_kg\",\"score_eff\"] if c in topn.columns]\n",
    "display(topn[cols_show] if cols_show else topn)\n",
    "\n",
    "# Pareto (minimiza MAE y olvido); NaN -> +inf para no dominar\n",
    "tmp = _ensure_mae_columns(df_report.copy())\n",
    "if \"method_base\" not in tmp.columns:\n",
    "    tmp[\"method_base\"] = tmp[\"method\"] if \"method\" in tmp.columns else \"unknown\"\n",
    "if MAE_COL not in tmp.columns:\n",
    "    tmp[MAE_COL] = np.nan\n",
    "\n",
    "tmp[\"_olvido_for_pareto\"] = pd.to_numeric(tmp.get(\"avg_forget_rel\", np.nan), errors=\"coerce\")\n",
    "tmp.loc[tmp[\"_olvido_for_pareto\"].isna(), \"_olvido_for_pareto\"] = np.inf\n",
    "tmp[\"_mae_for_pareto\"] = pd.to_numeric(tmp.get(MAE_COL, np.nan), errors=\"coerce\")\n",
    "tmp.loc[tmp[\"_mae_for_pareto\"].isna(), \"_mae_for_pareto\"] = np.inf\n",
    "\n",
    "pareto_idx = []\n",
    "vals = tmp[[\"_mae_for_pareto\", \"_olvido_for_pareto\"]].values\n",
    "for i in range(len(tmp)):\n",
    "    mae_i, forg_i = vals[i]\n",
    "    dominated = False\n",
    "    for j in range(len(tmp)):\n",
    "        if j == i: continue\n",
    "        mae_j, forg_j = vals[j]\n",
    "        if (mae_j <= mae_i) and (forg_j <= forg_i) and ((mae_j < mae_i) or (forg_j < forg_i)):\n",
    "            dominated = True\n",
    "            break\n",
    "    if not dominated:\n",
    "        pareto_idx.append(i)\n",
    "\n",
    "df_pareto = tmp.iloc[pareto_idx].drop(columns=[\"_olvido_for_pareto\", \"_mae_for_pareto\"])\n",
    "out_csv = SUMMARY / \"pareto.csv\"\n",
    "df_pareto.to_csv(out_csv, index=False)\n",
    "print(\"[OK] Pareto →\", out_csv)\n",
    "\n",
    "# --- NUEVO: guarda la ruta\n",
    "_last_update(pareto=out_csv)\n",
    "\n",
    "cols_show = [c for c in [\"run_dir\",\"method_base\",MAE_COL,\"avg_forget_rel\"] if c in df_pareto.columns]\n",
    "display(df_pareto[cols_show] if cols_show else df_pareto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e0ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7 — Scatter MAE vs Olvido\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _scatter(df, title, out_png: Path):\n",
    "    x = pd.to_numeric(df[\"avg_forget_rel\"], errors=\"coerce\")\n",
    "    y = pd.to_numeric(df[\"circuito2_final_mae\"], errors=\"coerce\")\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(x, y)  # sin estilos ni colores específicos\n",
    "    plt.xlabel(\"avg_forget_rel (↓ mejor)\")\n",
    "    plt.ylabel(\"circuito2_final_mae (↓ mejor)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "_scatter(df_sel, \"Todos\", SUMMARY / \"scatter_all.png\")\n",
    "_scatter(winners if \"winners\" in globals() else df_sel.head(0), \"Winners por método\", SUMMARY / \"scatter_winners.png\")\n",
    "print(\"[OK] Scatter →\", SUMMARY / \"scatter_all.png\")\n",
    "print(\"[OK] Scatter →\", SUMMARY / \"scatter_winners.png\")\n",
    "\n",
    "# --- NUEVO: guarda las rutas\n",
    "_last_update(scatter_all=SUMMARY / \"scatter_all.png\",\n",
    "             scatter_winners=SUMMARY / \"scatter_winners.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8 — Trazabilidad por run\n",
    "\n",
    "def explain_sources(run_row):\n",
    "    rd = _abs_run_dir(run_row[\"run_dir\"])\n",
    "    exists = {\n",
    "        \"per_task_perf.json\": (rd / \"per_task_perf.json\").exists(),\n",
    "        \"per_task_perf.csv\":  (rd / \"per_task_perf.csv\").exists(),\n",
    "        \"forgetting.json\":    (rd / \"forgetting.json\").exists(),\n",
    "        \"eval_matrix.csv\":    (rd / \"eval_matrix.csv\").exists(),\n",
    "        \"eval_matrix.json\":   (rd / \"eval_matrix.json\").exists(),\n",
    "        \"efficiency_summary.json\": (rd / \"efficiency_summary.json\").exists(),\n",
    "        \"emissions.csv\":      (rd / \"emissions.csv\").exists(),\n",
    "        \"run_row.json\":       (rd / \"run_row.json\").exists(),\n",
    "        \"task_1_circuito1/manifest.json\": (rd / \"task_1_circuito1/manifest.json\").exists(),\n",
    "    }\n",
    "    return exists\n",
    "\n",
    "print(\"## NaN en selección ##\")\n",
    "cols_check = [\"circuito1_best_mae\",\"circuito1_final_mae\",\"circuito2_final_mae\",\"avg_forget_rel\"]\n",
    "mask_nans = df_sel[cols_check].isna().any(axis=1)\n",
    "df_nans = df_sel[mask_nans].copy()\n",
    "print(f\"[INFO] {len(df_nans)} runs con NaN en {cols_check}:\")\n",
    "display(df_nans[[\"run_dir\",\"method_base\",\"preset\",\"encoder\",\"seed\",\"T\",\"amp\",\"batch_size\"] + cols_check] if len(df_nans) else df_nans)\n",
    "\n",
    "for _, r in df_sel.iterrows():\n",
    "    ex = explain_sources(r)\n",
    "    ok_flags = \" | \".join([f\"{k} → {'OK' if v else 'NO'}\" for k,v in ex.items()])\n",
    "    print(f\"— {r['run_dir']}\\n    {ok_flags}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9 — Sanidad naive vs CL\n",
    "\n",
    "DF = df_sel.copy()\n",
    "agg = (\n",
    "    DF.groupby(\"method_base\", dropna=False)\n",
    "      .agg(forget_median=(\"avg_forget_rel\",\"median\"),\n",
    "           forget_mean=(\"avg_forget_rel\",\"mean\"),\n",
    "           mae_mean=(\"circuito2_final_mae\",\"mean\"),\n",
    "           runs=(\"run_dir\",\"count\"))\n",
    "      .reset_index()\n",
    ")\n",
    "display(agg)\n",
    "\n",
    "naive_med = agg.loc[agg[\"method_base\"]==\"naive\",\"forget_median\"].values\n",
    "cl_med    = agg.loc[agg[\"method_base\"]!=\"naive\",\"forget_median\"].median()\n",
    "print(f\"[CHECK] naive_median_forget={naive_med[0] if len(naive_med) else np.nan} vs CL_median={cl_med}\")\n",
    "if len(naive_med) and (np.isnan(naive_med[0]) or (not np.isnan(cl_med) and naive_med[0] <= cl_med)):\n",
    "    print(\"[WARN] Naive NO olvida más que CL. Revisa eval_matrix/forgetting.json o implementación.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9b — Sanity-check EWC (parse logs en carpetas de run)\n",
    "\n",
    "import glob\n",
    "\n",
    "def _find_log_files(run_dir: Path):\n",
    "    pats = [\"*.log\", \"stdout*.txt\", \"train*.log\"]\n",
    "    files = []\n",
    "    for p in pats:\n",
    "        files += list(run_dir.glob(p))\n",
    "        files += list((run_dir / \"task_1_circuito1\").glob(p))\n",
    "        files += list((run_dir / \"task_2_circuito2\").glob(p))\n",
    "    return files\n",
    "\n",
    "def _parse_ewc_lines(text: str):\n",
    "    # Busca líneas del estilo: [EWC] base=... | pen=... | pen/base=...\n",
    "    bases, pens, ratios = [], [], []\n",
    "    for line in text.splitlines():\n",
    "        if \"[EWC]\" in line:\n",
    "            # intenta extraer números\n",
    "            try:\n",
    "                # formato flexible\n",
    "                # ej: [EWC] base=0.007436 | pen=0 | pen/base=0.000\n",
    "                m_base = re.search(r\"base\\s*=\\s*([0-9.eE+-]+)\", line)\n",
    "                m_pen  = re.search(r\"pen\\s*=\\s*([0-9.eE+-]+)\", line)\n",
    "                m_rat  = re.search(r\"pen/base\\s*=\\s*([0-9.eE+-]+)\", line)\n",
    "                if m_base: bases.append(float(m_base.group(1)))\n",
    "                if m_pen:  pens.append(float(m_pen.group(1)))\n",
    "                if m_rat:  ratios.append(float(m_rat.group(1)))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return bases, pens, ratios\n",
    "\n",
    "rows = []\n",
    "for _, r in df_sel.iterrows():\n",
    "    if not str(r.get(\"method\",\"\")).lower().startswith(\"ewc\") and \"ewc\" not in str(r.get(\"method\",\"\")).lower():\n",
    "        continue\n",
    "    rd = _abs_run_dir(r[\"run_dir\"])\n",
    "    logs = _find_log_files(rd)\n",
    "    all_bases, all_pens, all_ratios = [], [], []\n",
    "    for lf in logs:\n",
    "        try:\n",
    "            txt = lf.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            b, p, q = _parse_ewc_lines(txt)\n",
    "            all_bases += b; all_pens += p; all_ratios += q\n",
    "        except Exception:\n",
    "            pass\n",
    "    rows.append({\n",
    "        \"run_dir\": r[\"run_dir\"],\n",
    "        \"has_logs\": len(logs) > 0,\n",
    "        \"pen_gt0_count\": int(sum(1 for x in all_pens if (isinstance(x,float) and x > 0))),\n",
    "        \"pen_entries\": len(all_pens),\n",
    "        \"ratio_gt0_count\": int(sum(1 for x in all_ratios if (isinstance(x,float) and x > 0))),\n",
    "        \"ratio_entries\": len(all_ratios),\n",
    "    })\n",
    "\n",
    "df_ewc_sanity = pd.DataFrame(rows)\n",
    "out_csv = SUMMARY / \"ewc_sanity.csv\"\n",
    "df_ewc_sanity.to_csv(out_csv, index=False)\n",
    "print(\"[OK] EWC sanity →\", out_csv)\n",
    "display(df_ewc_sanity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 10 — Export final\n",
    "print(\"[OK] Artifacts en:\", SUMMARY)\n",
    "for p in sorted(SUMMARY.glob(\"*.csv\")) + sorted(SUMMARY.glob(\"*.png\")):\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "# --- NUEVO: apunta dónde quedó el manifest de “último”\n",
    "print(\"[LAST] Manifiesto final:\", OUT / \"summary\" / \"_last.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70518b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda X — Top-5 por método + parámetros (lee rutas desde _last.json y exporta en SUMMARY)\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Fallbacks por si no vienes de la Celda 0\n",
    "try:\n",
    "    OUT\n",
    "except NameError:\n",
    "    OUT = Path(os.environ.get(\"OUT_DIR\", \"/home/cesar/proyectos/TFM_SNN/outputs\")).resolve()\n",
    "\n",
    "try:\n",
    "    _last_update\n",
    "except NameError:\n",
    "    def _last_update(**kwargs):  # no-op si no está definida\n",
    "        pass\n",
    "\n",
    "LAST_PATH = OUT / \"summary\" / \"_last.json\"\n",
    "LATEST = OUT / \"summary\" / \"latest\"\n",
    "\n",
    "def _pick_latest_selection():\n",
    "    # 1) _last.json\n",
    "    try:\n",
    "        last = json.loads(LAST_PATH.read_text(encoding=\"utf-8\"))\n",
    "        p = Path(last.get(\"selection\", \"\"))\n",
    "        if p.exists():\n",
    "            return p\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) SUMMARY actual\n",
    "    try:\n",
    "        if \"SUMMARY\" in globals():\n",
    "            p = Path(SUMMARY) / \"selection_table.csv\"\n",
    "            if p.exists():\n",
    "                return p\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3) symlink latest\n",
    "    try:\n",
    "        p = LATEST / \"selection_table.csv\"\n",
    "        if p.exists():\n",
    "            return p\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 4) glob: el más reciente\n",
    "    cands = list(OUT.glob(\"summary/*/selection_table.csv\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"No se encontró ningún selection_table.csv en outputs/summary/**/\")\n",
    "    cands.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "# 1) Carga selección\n",
    "sel_csv = _pick_latest_selection()\n",
    "df = pd.read_csv(sel_csv)\n",
    "print(f\"[LOAD] selection_table.csv → {sel_csv}\")\n",
    "\n",
    "# 2) Normaliza columnas clave\n",
    "def _canonical_method(m: str) -> str:\n",
    "    s = (m or \"\").lower()\n",
    "    if s.startswith(\"ewc\"): return \"ewc\"\n",
    "    if s.startswith(\"as-snn\"): return \"as-snn\"\n",
    "    if s.startswith(\"sa-snn\"): return \"sa-snn\"\n",
    "    if s.startswith(\"sca-snn\"): return \"sca-snn\"\n",
    "    if s.startswith(\"rehearsal\"): return \"rehearsal\"\n",
    "    if s.startswith(\"naive\"): return \"naive\"\n",
    "    return s or \"unknown\"\n",
    "\n",
    "if \"method_base\" not in df.columns or df[\"method_base\"].isna().all():\n",
    "    df[\"method_base\"] = df[\"method\"].astype(str).apply(_canonical_method)\n",
    "else:\n",
    "    # rellena NA con prefijo del method o con canonical\n",
    "    mask_na = df[\"method_base\"].isna()\n",
    "    if mask_na.any():\n",
    "        df.loc[mask_na, \"method_base\"] = (\n",
    "            df.loc[mask_na, \"method\"].astype(str).apply(_canonical_method)\n",
    "        )\n",
    "\n",
    "# 3) Parser de hiperparámetros desde 'method' (robusto a variaciones de nombres)\n",
    "_float = lambda s: (None if s is None else float(str(s).replace(\"_\", \"\").replace(\"p\", \".\")))\n",
    "\n",
    "def parse_params(m):\n",
    "    s = str(m or \"\").lower()\n",
    "    out = {}\n",
    "\n",
    "    # -------- AS-SNN --------\n",
    "    # as-snn_gr_<float>_lam_<float>_att_f<d>   + flags: scale_on, ema\\d+, _l1/_l2\n",
    "    m_as = re.search(r\"as-snn(?:(?:_gr[_-]?([\\d._e+-]+)))?(?:_lam[_-]?([\\d._e+-]+))?(?:_att[_-]?f(\\d))?\", s)\n",
    "    if m_as:\n",
    "        gr, lam, att = m_as.groups()\n",
    "        if gr:  out[\"gr\"] = _float(gr)\n",
    "        if lam: out[\"lam\"] = lam  # mantenemos texto por si es 3e+08\n",
    "        if att: out[\"attach\"] = f\"f{att}\"\n",
    "    if \"scale_on\" in s: out[\"scale_on\"] = 1\n",
    "    m_ema = re.search(r\"ema0?(\\d+)\", s)\n",
    "    if m_ema: out[\"ema\"] = int(m_ema.group(1))\n",
    "    if \"_l1\" in s: out[\"penalty\"] = (\"l1\" if \"penalty\" not in out else out[\"penalty\"] + \"/l1\")\n",
    "    if \"_l2\" in s: out[\"penalty\"] = (\"l2\" if \"penalty\" not in out else out[\"penalty\"] + \"/l2\")\n",
    "\n",
    "    # -------- SA-SNN --------\n",
    "    # sa-snn_k<d>_tau<d>_vt<1p33|1.33>_p<nnn|2m> + reset1 + flat[01]\n",
    "    m_sa = re.search(r\"sa(?:-snn)?(?:.*?_k(\\d+))?(?:_tau(\\d+))?(?:_vt([0-9p\\._+-]+))?(?:_p([\\d_]+|(\\d+)m))?\", s)\n",
    "    if m_sa:\n",
    "        k, tau, vt, p_raw, p_m = m_sa.groups()\n",
    "        if k:   out[\"k\"] = int(k)\n",
    "        if tau: out[\"tau\"] = int(tau)\n",
    "        if vt:  out[\"vt\"] = float(str(vt).replace(\"_\",\"\").replace(\"p\",\".\"))\n",
    "        if p_m: out[\"p\"] = int(p_m) * 1_000_000\n",
    "        elif p_raw:\n",
    "            out[\"p\"] = int(str(p_raw).replace(\"_\",\"\"))\n",
    "    if \"reset1\" in s: out[\"reset\"] = 1\n",
    "    if \"flat1\"  in s: out[\"flat\"]  = 1\n",
    "    elif \"flat0\" in s: out[\"flat\"] = 0\n",
    "\n",
    "    # -------- SCA-SNN --------\n",
    "    # sca-snn_bins<d>_beta<flt>_bias<flt>_temp<flt>_ab<d+>\n",
    "    m_sca = re.search(r\"sca(?:-snn)?_bins(\\d+).*?beta([-\\d\\.e+]+).*?bias([-\\d\\.e+]+).*?temp([-\\d\\.e+]+).*?(ab\\d+)\", s)\n",
    "    if m_sca:\n",
    "        bins, beta, bias, temp, ab = m_sca.groups()\n",
    "        out.update({\n",
    "            \"bins\": int(bins),\n",
    "            \"beta\": float(beta),\n",
    "            \"bias\": float(bias),\n",
    "            \"temp\": float(temp),\n",
    "            \"attach_block\": ab\n",
    "        })\n",
    "\n",
    "    # -------- Rehearsal --------\n",
    "    # rehearsal_buf_<int>_rr_<0-100 | 0.x>\n",
    "    m_reh = re.search(r\"rehearsal.*?_buf[_-]?(\\d+).*?_rr[_-]?([0-9\\.]+)\", s)\n",
    "    if m_reh:\n",
    "        buf, rr = m_reh.groups()\n",
    "        rr_f = float(rr)\n",
    "        rr_f = rr_f/100.0 if rr_f > 1.0 else rr_f\n",
    "        out.update({\"buffer\": int(buf), \"rr\": rr_f})\n",
    "\n",
    "    # -------- EWC --------\n",
    "    # ewc_*_lam_<3e+08>  + fisher batches: fb\\d+ o 'f\\d' (evita confundir con attach)\n",
    "    m_ewc = re.search(r\"\\bewc\\b.*?_lam[_-]?([\\de\\+\\-\\.]+)\", s)\n",
    "    if m_ewc:\n",
    "        out[\"lambda\"] = m_ewc.group(1)\n",
    "    m_fb = re.search(r\"\\bfb(\\d+)\\b\", s) or re.search(r\"\\bf(\\d+)\\b\", s)\n",
    "    if \"ewc\" in s and m_fb:\n",
    "        out[\"fisher_batches\"] = int(m_fb.group(1))\n",
    "\n",
    "    # -------- genérico: attach fX si aparece suelto y no viene de AS-SNN\n",
    "    if \"attach\" not in out:\n",
    "        m_att = re.search(r\"\\bf(\\d)\\b\", s)\n",
    "        if m_att:\n",
    "            out[\"attach\"] = f\"f{m_att.group(1)}\"\n",
    "\n",
    "    return out\n",
    "\n",
    "params = df[\"method\"].apply(parse_params).apply(pd.Series)\n",
    "dfp = pd.concat([df, params], axis=1)\n",
    "\n",
    "# 4) Métrica objetivo y orden\n",
    "for col_src, col_dst in [(\"circuito2_final_mae\",\"c2_final_mae\"),\n",
    "                         (\"avg_forget_rel\",\"forget\"),\n",
    "                         (\"emissions_kg\",\"co2\")]:\n",
    "    if col_src in dfp.columns:\n",
    "        dfp[col_dst] = pd.to_numeric(dfp[col_src], errors=\"coerce\")\n",
    "    else:\n",
    "        dfp[col_dst] = np.nan\n",
    "\n",
    "# 5) Top-5 por method_base\n",
    "tops = []\n",
    "for mb, g in dfp.groupby(\"method_base\", dropna=False):\n",
    "    g = g.sort_values([\"c2_final_mae\",\"forget\",\"co2\"], ascending=[True, True, True]).head(5)\n",
    "    # columnas de hiperparámetros presentes dinámicamente\n",
    "    param_cols = [c for c in [\n",
    "        \"gr\",\"lam\",\"attach\",\"k\",\"tau\",\"vt\",\"p\",\"bins\",\"beta\",\"bias\",\"temp\",\n",
    "        \"attach_block\",\"flat\",\"buffer\",\"rr\",\"lambda\",\"fisher_batches\",\"scale_on\",\"ema\",\"penalty\",\"reset\"\n",
    "    ] if c in g.columns]\n",
    "    keep_cols = [\"method_base\",\"method\",\"c2_final_mae\",\"forget\",\"co2\"] + param_cols\n",
    "    tops.append(g[keep_cols])\n",
    "\n",
    "if tops:\n",
    "    top5 = pd.concat(tops, ignore_index=True)\n",
    "else:\n",
    "    top5 = dfp.head(0)\n",
    "\n",
    "# 6) Exporta en el SUMMARY correspondiente a la selección\n",
    "export_dir = sel_csv.parent   # mismo SUMMARY donde está selection_table.csv\n",
    "out_csv = export_dir / \"top5_por_metodo.csv\"\n",
    "out_tex = export_dir / \"top5_por_metodo.tex\"\n",
    "\n",
    "top5.to_csv(out_csv, index=False)\n",
    "\n",
    "with open(out_tex, \"w\", encoding=\"utf-8\") as f:\n",
    "    for mb, g in top5.groupby(\"method_base\", dropna=False):\n",
    "        # ordena columnas: primero métricas, luego params\n",
    "        base_cols = [\"method\",\"c2_final_mae\",\"forget\",\"co2\"]\n",
    "        other_cols = [c for c in g.columns if c not in ([\"method_base\"] + base_cols)]\n",
    "        gg = g[base_cols + other_cols]\n",
    "        f.write(gg.to_latex(index=False, float_format=\"%.6f\"))\n",
    "\n",
    "print(f\"[OK] → {out_csv.name} / {out_tex.name} en {export_dir}\")\n",
    "\n",
    "# registra rutas para reutilización\n",
    "_last_update(top5_csv=out_csv, top5_tex=out_tex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda A — Cargar y mostrar el Top-5 por método (y ruta de dónde salió)\n",
    "\n",
    "from pathlib import Path\n",
    "import json, os\n",
    "import pandas as pd\n",
    "\n",
    "OUT = Path(os.environ.get(\"OUT_DIR\", \"/home/cesar/proyectos/TFM_SNN/outputs\")).resolve()\n",
    "LAST = OUT / \"summary\" / \"_last.json\"\n",
    "\n",
    "def _pick_top5_path():\n",
    "    # 1) _last.json si existe\n",
    "    try:\n",
    "        j = json.loads(LAST.read_text(encoding=\"utf-8\"))\n",
    "        p = Path(j.get(\"top5_csv\",\"\"))\n",
    "        if p.exists(): \n",
    "            return p\n",
    "        # fallback a la carpeta del selection si solo tenemos eso\n",
    "        p2 = Path(j.get(\"selection\",\"\")).parent / \"top5_por_metodo.csv\"\n",
    "        if p2.exists():\n",
    "            return p2\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) symlink 'latest'\n",
    "    p = OUT / \"summary\" / \"latest\" / \"top5_por_metodo.csv\"\n",
    "    if p.exists(): \n",
    "        return p\n",
    "    # 3) glob más reciente\n",
    "    cands = sorted(OUT.glob(\"summary/*/top5_por_metodo.csv\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"No encuentro top5_por_metodo.csv; ejecuta antes la celda de Top-5.\")\n",
    "    return cands[0]\n",
    "\n",
    "top5_csv = _pick_top5_path()\n",
    "top5 = pd.read_csv(top5_csv)\n",
    "print(f\"[OK] Cargado Top-5 → {top5_csv}\")\n",
    "display(top5)\n",
    "\n",
    "# Vista separada por método (para inspección rápida en el notebook)\n",
    "for mb, g in top5.groupby(\"method_base\", dropna=False):\n",
    "    print(f\"\\n### {mb} — {len(g)} configs\")\n",
    "    display(g)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
